{"pages":[],"posts":[{"title":"","text":"前言上一篇讲解了反射的知识[]，作为反射的入门级，然后这一篇主要也是讲解动态代理的实现机制。 动态代理包括jdk的动态代理和cglib 的动态代理，两者实现相同的功能，但是实现方式却是有明显的区别。 下面我们就通过代码的方式层层的深入这两种动态代理，了解他们的性能以、底层的实现原理及应用场景。 代理模式在详细介绍动态代理之前，先来说说Java中的代理模式。代理模式分为两种： 静态代理：也就是23种设计模式中的代理模式，由程序员自己编写源代码并进行编译，在程序运行之前已经编译好了.class文件。 动态代理：包括jdk的动态代理和cglib的动态代理，运行时通过反射动态创建。 代理模式定义：我的个人理解就是给某一个对象提供一个代理对象，在代理对象中拥有被代理对象的引用，并在代理对象中调用被代理对象的方法之前和之后进行方法的增强。 我这里画了一张代理模式的类图，设计模式中的代理模式比较简单，代理类和委托类有公共的接口，最后由代理类去执行委托类的方法：代理模式就好像生活中的中介，去帮你做事，而不用比自己去做事。举个例子，比如你要买车，但是买车之前你要到处找车源，找到车源给钱了还要办理一堆手续。 （1）下面我们以买车这个案例进行代理模式的代码编写，首先要有一个公共的接口Person，Person接口里面定义公共的方法： 123public interface Person{ void buyCar();} （2）然后定义一个委托类，也就是我本人Myself，并实现Person接口，具体代码如下： 1234567public class Myself implements Person { @Override public void buyCar() { System.out.println(&quot;我要买车了&quot;); }} （3）最后就是创建代理类CarProxy，同样也是实现Person接口，具体实现代码如下： 123456789101112131415public class CarProxy implements Person{ private Myself myself ; public CarProxy(final Myself myself ) { this.myself = myself ; } @Override public void buyCar() { System.out.println(&quot;买车前去找车源&quot;); myself .buyCar(); System.out.println(&quot;买车后办理手续&quot;); }} 这个代理的demo很简单，如上面的类图所示，代理类和委托类都实现公共的接口Person，在委托类中进行方法的具体业务逻辑的实现，而代理类中再次对这个方法进行增强。 代理模式的优点就是能够对目标对象进行功能的扩展，缺点是每一个业务类都要创建一个代理类，这样会使我们系统内的类的规模变得很大，不利于维护。 于是就出现了动态代理，仔细思考静态代理的缺点，就是一个委托类就会对象一个代理类，那么是否可以将代理类做成一个通用的呢？ 我们仔细来看一下下面的这个图：我们把静态代理所有的执行过程都可以抽象成这张图的执行过程，Proxy角色无非是在调用委托类处理业务的方法之前或者之后做一些额外的操作。 那么为了做一个通用性的处理，就把调用委托类的method的动作抽出来，看成一个通用性的处理类，于是就有了InvocationHandler角色，抽象成一个处理类。 这样在Proxy和委托类之间就多了一个InvocationHandler处理类的角色，这个角色主要是将之前代理类调用委托类的方法的动作进行统一的调用，都由InvocationHandler来处理。 于是之前上面的类图就有了这样的改变，在Proxy和委托类之间加入了InvocationHandler，具体的实现图如下：看完上面的图似乎有那么一点点的理解，下面我们就来详细的深入动态代理。 jdk动态代理上面讲解到动态代理是在运行时环境动态加载class文件，并创建对应的class对象，那么动态代理着静态代理的执行时机是在哪里呢？ 我这边又画了一张原理图，感觉我为画图操碎了心，每一个点都会画一个想截图，是不是很暖。这个是静态代理的运行原理图，静态代理在程序运行时就已经创建好了class文件，在程序启动后的某一个时机（用到class文件）就会加载class文件到内存中。 当在运行时期动态生成class文件并加载class文件的运行原理图如下：在JVM运行期时遵循JVM字节码的结构和规范生成二进制文件，并加载到内存中生成对应的Class对象。这样，就完成了动态创建class文件和Class对象的功能了。 在jdk的动态代理中的Proxy类和委托类要求实现相同的功能，这里的相同是指他们都可以调用统一的逻辑业务方法。要实现这样的设计有以下三种方法： 实现同一个接口：接口里面定义公共的方法。 继承：Proxy继承委托类，这样Proxy就有了和委托类一样的功能，或者两者都继承同一个类，把公共实现业务逻辑的方法放在父类中，这样也能实现。 两者内部都有同一个类的引用：这个和继承有异曲同工之妙，都可以统一的调用统一的业务逻辑方法。 在jdk的动态代理中，是采用第一种方法进行实现，必须有公共的接口，下面我们还是通过静态代理的案例使用动态代理来实现。 （1）首先创建一个公共接口Person： 123public interface Person{ void buyCar();} （2）然后创建接口的实现类Myself： 1234567public class Myself implements Person { @Override public void buyCar() { System.out.println(&quot;我要买车了&quot;); }} （3）这一步就是比较关键的，要创建一个类并实现InvocationHandler： 12345678910111213141516public class InvocationHandlerImpl implements InvocationHandler { private Person person; public InvocationHandlerImpl(Person person){ this.person=person; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(&quot;买车前开始找车源。。。。&quot;); method.invoke(person, args); System.out.println(&quot;买车后办理手续。。。。&quot;); return null; } } （4）最后一步就是进行测试： 12345678910public class Test { public static void main(String[] args) { Myself myself= new Myself(); // 创建代理对象，这里有三个参数，第一个是类的ClassLoader，第二个是该类的接口集合，第三个就是InvocationHandler Object o = Proxy.newProxyInstance(myself.getClass().getClassLoader(), myself.getClass().getInterfaces(), new InvocationHandlerImpl(myself)); Person person= (Person) o; person.buyCar(); } } 整体来说jdk动态代理的应用过程还是比较简单的，重要的实现理解他的底层实现过程，它的重要实现步骤就是InvocationHandler中 的invoke方法处理。 invoke方法才是实现方法的调用者，根据上面的参数最后才会创建代理对象newProxyInstance(ClassLoader loader,Class&lt;?&gt;[] interfaces,InvocationHandler h)。 那么在实现jdk动态代理的过程都做了哪些工作呢？具体有以下6个步骤： 获取委托类也就是Myself上的所有接口。 生成代理，生成的代理的名称也是有规律的，一般是在com.sun.proxy.$ProxyXXX。 动态创建代理类的字节码信息，也就是class文件。 根据class文件创建Class对象。 创建自己的InvocationHandler并实现InvocationHandler重写invoke方法，实现对委托类方法的调用和增强。 最后是代理对象的创建，并调用方法，实现代理的功能。 我们可以通过反编译工具来看看生成的代理类的源码是怎么样的，我这里使用的反编译工具是jd-gui，推荐给大家。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172 public final class MyselfProxy extends Proxy implements Person { private static Method m1; private static Method m3; private static Method m0; private static Method m2; public MyselfProxy(InvocationHandler paramInvocationHandler) throws { super(paramInvocationHandler); } public final boolean equals(Object paramObject) throws { try { // InvocationHandler 实现equals的调用 return ((Boolean)this.h.invoke(this, m1, new Object[] { paramObject })).booleanValue(); } catch (Error|RuntimeException localError) { throw localError; } catch (Throwable localThrowable) { throw new UndeclaredThrowableException(localThrowable); } } public final void buyCar() throws { try { // InvocationHandler实现buyCar的调用 this.h.invoke(this, m3, null); return; } catch (Error|RuntimeException localError) { throw localError; } catch (Throwable localThrowable) { throw new UndeclaredThrowableException(localThrowable); } } public final int hashCode() throws { try { // InvocationHandler实现hashCode方法的调用 return ((Integer)this.h.invoke(this, m0, null)).intValue(); } catch (Error|RuntimeException localError) { throw localError; } catch (Throwable localThrowable) { throw new UndeclaredThrowableException(localThrowable); } } public final String toString() throws { try { // InvocationHandler实现toString的调用 return (String)this.h.invoke(this, m2, null); } catch (Error|RuntimeException localError) { throw localError; } catch (Throwable localThrowable) { throw new UndeclaredThrowableException(localThrowable); } } static { try { //在静态块中通过反射初始化函数 m1 = Class.forName(&quot;java.lang.Object&quot;).getMethod(&quot;equals&quot;, new Class[] { Class.forName(&quot;java.lang.Object&quot;) }); m3 = Class.forName(&quot;com.ldc.org.Person&quot;).getMethod(&quot;buyCar&quot;, new Class[0]); m0 = Class.forName(&quot;java.lang.Object&quot;).getMethod(&quot;hashCode&quot;, new Class[0]); m2 = Class.forName(&quot;java.lang.Object&quot;).getMethod(&quot;toString&quot;, new Class[0]); return; } catch (NoSuchMethodException localNoSuchMethodException) { throw new NoSuchMethodError(localNoSuchMethodException.getMessage()); } catch (ClassNotFoundException localClassNotFoundException) { throw new NoClassDefFoundError(localClassNotFoundException.getMessage()); } } } 从上面反编译的源码中可以可以看出，在静态块中直接通过反射的方式来生成Method对象，对方法的调用则是通过InvocationHandler对象来进行调用。 仔细的总结可以看出上面反编译出来的代理类有以下特征： 继承 java.lang.reflect.Proxy类，并实现统一的接口Person。 所有的方法都是final修饰的。 在代理类中都是通过InvocationHandler对象执行invoke方法的调用统一调用函数，invoke方法通过Method参数来区分是什么方法，进而相应的处理。 到这里我想大家应该对jdk的动态代理有一个清晰的认识了，包括他的底层实现的原理，下面我们就来详细的了解cglib动态代理的是实现方式。 cglib动态代理在实现jdk的动态代理的实现会发现，jdk动态代理必须实现一个接口，并且代理类也只能代理接口中实现的方法，要是实现类中有自己私有的方法，而接口中没有的话，该方法不能进行代理调用。 基于这种情况cglib便出现了，他也可以在运行期扩展Java类和Java接口。 cglib底层是采用字节码技术，其原理是通过字节码技术生成一个子类，并在子类中拦截父类的方法的调用，织入业务逻辑。 因为原理是采用继承的方式，所以被代理的类不能被final修饰，在Spring Aop中底层的实现是以这两种动态代理作为基础进行实现。 当使用cglib动态代理一个类demo时，JVM又做了哪些工作呢？ 首先找到demo类中的所有非final的公共方法。 然后将这些方法转化为字节码。 通过这些字节码转化为Class对象。 最后由MethodInterceptor实现代理类中所有方法的调用。 （1）那么我们通过代码也来实现cglib动态代理，还是创建Myself类，但是此时不需要实现接口： 1234567public class Myself { @Override public void buyCar() { System.out.println(&quot;I'm going to buy a house&quot;); }} （2）然后是创建MyMethodInterceptor类实现MethodInterceptor接口，这个和动态代理实现InvocationHandler方式一样，实现统一方法的调用。 12345678910public class MyMethodInterceptor implements MethodInterceptor { @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable { System.out.println(&quot;买车前开始找车源。。。。&quot;); proxy.invokeSuper(obj, args); System.out.println(&quot;买车后办理手续。。。。&quot;); return null; } } （3）最后是进行测试 123456789101112131415public class Test { public static void main(String[] args) { Myself myself= new Myself(); MyMethodInterceptor myMethodInterceptor = new MyMethodInterceptor (); //cglib 中加强器，用来创建动态代理 Enhancer enhancer = new Enhancer(); //设置要创建的代理类 enhancer.setSuperclass(myself.getClass()); // 设置回调，这里相当于是对于代理类上所有方法的调用 enhancer.setCallback(myMethodInterceptor ); // 创建代理类 Programmer proxy =(Myself)enhancer.create(); proxy.buyCar(); } } 总结来说cglib是一个强大的、高性能的Code生产类库，在Spring中就是通过cglib方式继承要被代理的类，重写父类的方法，实现Aop编程。 cglib创建动态代理对象的性能时机要比jdk动态代理的方式高很多，但是创建对象所花的时间却要比jdk动态代理方式多很多。 在应用方面单例模式更适合用cglib，无需频繁的创建对象，相反，则使用jdk动态代理的方式更加合适。","link":"/2020/10/14/Java%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E5%92%8CCglib%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E6%9C%80%E5%BC%BA%E7%8E%8B%E8%80%85%E9%98%B5%E5%AE%B9/"},{"title":"","text":"概述对于mysql的优化是一个综合性的技术，sql的优化只是其中的一种，其中主要包括 ： 表的设计合理化(符合3大范式)。 添加索引(index) [普通索引、主键索引、唯一索引unique、全文索引]。 分表技术(水平分割、垂直分割)。 读写[写: update/delete/add]分离。 合理设计表（三大范式）在表的设计中一定条件下要满足三范式，表的范式，是首先符合第一范式, 才能满足第二范式 , 进一步满足第三范式。 第一范式: 即表的列的具有原子性,不可再分解，即列的信息，不能分解, 只有数据库是关系型数据库(mysql/oracle/db2/sql server)，就自动的满足第一范式。 第二范式: 表中的记录是唯一的, 就满足第二范式, 通常我们设计一个主键来实现。 第三范式: 即表中不要有冗余数据, 就是说，表的信息，如果能够被推导出来，就不应该单独的设计一个字段来存放. 比如下面的设计就是不满足第三范式：表1存在冗余表2的数据，正常的设计都会设计成如下：注意： 反第三范式: 但是没有冗余的数据库未必是最好的数据库，有时为了提高运行效率，就必须降低范式标准，适当保留冗余数据。具体做法是： 在概念数据模型设计时遵守第三范式，降低范式标准的工作放到物理数据模型设计时考虑。降低范式就是增加字段，允许冗余。 在1对N的情况下，为了提高查询的效率，是允许部分字段冗余的。 Sql优化Sql的优化中，主要是对字段添加索引，主要包含有这四种索引(主键索引/唯一索引/全文索引/普通索引) 主键索引添加当一张表，把某个列设为主键的时候，则该列就是主键索引,下面的id 列就是主键索引 123create table user(id int unsigned primary key auto_increment ,name varchar(32) not null defaul ‘’); 如果你创建表时，没有指定主键索引，也可以在创建表后，在添加, 指令: 123alter table 表名 add primary key (列名);//举例alter table user add primary key (id); 普通索引一般来说，普通索引的创建，是先创建表，然后在创建普通索引比如: 123456create table user(id int unsigned,name varchar(32))create index 索引名 on 表 (列1,列名2); 创建全文索引全文索引，主要是针对对文件，文本的检索, 比如文章, 全文索引针对MyISAM有用。创建如下： 123456CREATE TABLE articles ( id INT UNSIGNED AUTO_INCREMENT NOT NULL PRIMARY KEY, title VARCHAR(200), body TEXT, FULLTEXT (title,body) )engine=myisam charset utf8; 如何使用全文索引: 1234567select * from articles where body like ‘%非科班%’; //不会使用到全文索引// 查看是否使用索引:explain select * from articles where body like ‘%非科班%’// 正确的用法是:select * from articles where match(title,body) against(‘非科班’); 说明: 在mysql中fulltext 索引只针对 myisam生效 mysql自己提供的fulltext针对英文生效-&gt;sphinx (coreseek) 技术处理中文 使用方法是match(字段名..) against(‘关键字’) 全文索引一个 叫 停止词, 因为在一个文本中，创建索引是一个无穷大的数，因此，对一些常用词和字符，就不会创建，这些词，称为停止词. 唯一索引当表的某列被指定为unique约束时，这列就是一个唯一索引 12// 创建create table user(id int primary key auto_increment , name varchar(32) unique); 这时, name 列就是一个唯一索引，unique字段可以为NULL,并可以有多NULL, 但是如果是具体内容，则不能重复，主键字段，不能为NULL,也不能重复。 创建唯一索引 123create table user(id int primary key auto_increment, name varchar(32));create unique index 索引名 on 表名 (列表..); 查询索引 123desc 表名 //不能够显示索引名show index(es) from 表名show keys from 表名 删除索引 1234alter table 表名 drop index 索引名; //如果删除主键索引。alter table 表名 drop primary key 索引使用的注意事项由于索引本身很大，占用磁盘空间，对dml操作有影响，变慢，满足以下条件的字段，才应该创建索引。 肯定在where条经常使用 该字段的内容不是唯一的几个值 字段内容不是频繁变化 explain 可以帮助我们在不真正执行某个sql语句时，就执行mysql怎样执行，这样利用我们去分析sql指令。 id：查询的序列号。 select_type：查询类型。 table：查询表名。 type：扫描方式，all表示全表扫描。 possible_keys：可是使用到的索引。 key：实际使用到的索引。 rows：该sql扫面了多少行。 Extra：sql语句额外的信息，比如排序方式 sql语句的小技巧 在使用group by 分组查询是，默认分组后，还会排序，可能会降低速度，在group by 后面增加 order by null 就可以防止排序。如下图所示 有些情况下，可以使用连接来替代子查询。因为使用join，MySQL不需要在内存中创建临时表。 123select * from dept, emp where dept.deptno=emp.deptno; // 替换成select * from dept left join emp on dept.deptno=emp.deptno; 正确的选择mysql的存储引擎Myisam : 如果表对事务要求不高，同时是以查询和添加为主的，我们考虑使用myisam存储引擎. ,比如 bbs 中的 发帖表，回复表。 INNODB : 对事务要求高，保存的数据都是重要数据，我们建议使用INNODB,比如订单表，账号表。如果你的数据库的存储引擎是myisam,请一定记住要定时进行碎片整理 分表技术 为什么要分表？（1） 如果一个表的每条记录的内容很大，那么就需要更多的IO操作，如果字段值比较大，而使用频率相对比较低，可以将大字段移到另一张表中，当查询不查大字段的时候，这样就减少了I/O操作（2）如果表的数据量非常非常大，那么查询就变的比较慢；也就是表的数据量影响这查询的性能。 （3）表中的数据本来就有独立性，例如分别记录各个地区的数据或者不同时期的数据，特别是有些数据常用，而另外一些数据不常用。 （4） 分表技术有(水平分割和垂直分割) 垂直分割垂直分割是指数据表列的拆分，把一张列比较多的表拆分为多张表。 垂直分割一般用于拆分大字段和访问频率低的字段，分离冷热数据。 垂直分割比较常见：例如博客系统中的文章表，比如文章tbl_articles(id, titile, summary, content, user_id, create_time)，因为文章中的内容content会比较长，放在tbl_articles中会严重影响表的查询速度，所以将内容放到tbl_articles_detail(article_id, content)，像文章列表只需要查询tbl_articles中的字段即可。 垂直拆分的优点： 可以使得行数据变小，在查询时减少读取的Block数，减少I/O次数。此外，垂直分区可以简化表的结构，易于维护。 垂直拆分的缺点： 主键会出现冗余，需要管理冗余列，并会引起Join操作，可以通过在应用层进行Join来解决。此外，垂直分区会让事务变得更加复杂。 水平分割水平拆分是指数据表行数据的拆分，表的行数超过500万行或者单表容量超过10GB时，查询就会变慢，这时可以把一张的表的数据拆成多张表来存放。水平分表尽可能使每张表的数据量相当，比较均匀。 水平拆分会给应用增加复杂度，它通常在查询是需要多个表名，查询所有数据需要union操作。在许多数据库应用中，这种复杂性会超过它带来的优点。 因为只要索引关键字不大，则在索引用于查询时，表中增加2-3倍数据量，查询时也就增加读一个索引层的磁盘次数，所以水平拆分要考虑数据量的增长速度，根据实际情况决定是否需要对表进行水平拆分。 水平分割最重要的是找到分割的标准，不同的表应根据业务找出不同的标准 用户表可以根据用户的手机号段进行分割如user183、user150、user153、user189等，每个号段就是一张表 用户表也可以根据用户的id进行分割，加入分3张表user0,user1,user2，如果用户的id%3=0就查询user0表，如果用户的id%3=1就查询user1表 对于订单表可以按照订单的时间进行分表 读写分离实现MySQL读写分离的前提是我们已经将MySQL主从复制配置完毕，读写分离实现方式：（1）配置多数据源。（2）使用mysql的proxy中间件代理工具。 主从复制的原理MySQL的主从复制和读写分离两者有着紧密的联系，首先要部署主从复制，只有主从复制完成了才能在此基础上进行数据的读写分离。 读写分离的原理读写分离就是只在主服务器上写，只在从服务器上读。基本原理是让主数据库处理事务性查询，而从服务器处理select查询。数据库复制被用来把事务性查询导致的变更同步到从数据库中。","link":"/2020/10/14/%E4%BD%A0%E8%A6%81%E7%9A%84Mysql%E4%BC%98%E5%8C%96%E6%8F%90%E9%AB%98%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86%EF%BC%8C%E6%9D%A5%E8%87%AA%E4%BA%8E%E4%B8%80%E4%BD%8D%E5%A4%A7%E4%BD%AC%E7%9A%84%E7%AC%94%E8%AE%B0/"},{"title":"","text":"概述Redis的文章，我之前写过一篇关于Redis的缓存的三大问题，累计阅读也快800了，对于还只有3k左右的粉丝量，能够达到这个阅读量，已经是比较难了。 这说明那篇文章写的还过得去，收到很多人的阅读肯定，感兴趣的看一下[]。 三大缓存问题只是Redis的其中的一小部分的知识点，想要深入学习Redis还要学习比较多的知识点。 那么今天就带来了一个面试常问的一个问题：假如你的Redis内存满了怎么办？ 长期的把Redis作为缓存使用，总有一天会存满的时候对吧。 这个面试题不慌呀，在Redis中有配置参数maxmemory可以设置Redis内存的大小。 在Redis的配置文件redis.conf文件中，配置maxmemory的大小参数如下所示：实际生产中肯定不是100mb的大小哈，不要给误导了，这里我只是让大家认识这个参数，一般小的公司都是设置为3G左右的大小。 除了在配置文件中配置生效外，还可以通过命令行参数的形式，进行配置，具体的配置命令行如下所示： 1234//获取maxmemory配置参数的大小127.0.0.1:6379&gt; config get maxmemory//设置maxmemory参数为100mb127.0.0.1:6379&gt; config set maxmemory 100mb 倘若实际的存储中超出了Redis的配置参数的大小时，Redis中有淘汰策略，把需要淘汰的key给淘汰掉，整理出干净的一块内存给新的key值使用。 接下来我们就详细的聊一聊Redis中的淘汰策略，并且深入的理解每个淘汰策略的原理和应用的场景。 淘汰策略Redis提供了6种的淘汰策略，其中默认的是noeviction，这6中淘汰策略如下： noeviction(默认策略)：若是内存的大小达到阀值的时候，所有申请内存的指令都会报错。 allkeys-lru：所有key都是使用LRU算法进行淘汰。 volatile-lru：所有设置了过期时间的key使用LRU算法进行淘汰。 allkeys-random：所有的key使用随机淘汰的方式进行淘汰。 volatile-random：所有设置了过期时间的key使用随机淘汰的方式进行淘汰。 volatile-ttl：所有设置了过期时间的key根据过期时间进行淘汰，越早过期就越快被淘汰。 假如在Redis中的数据有一部分是热点数据，而剩下的数据是冷门数据，或者我们不太清楚我们应用的缓存访问分布状况，这时可以使用allkeys-lru。 假如所有的数据访问的频率大概一样，就可以使用allkeys-random的淘汰策略。 假如要配置具体的淘汰策略，可以在redis.conf配置文件中配置，具体配置如下所示：这只需要把注释给打开就可以，并且配置指定的策略方式，另一种的配置方式就是命令的方式进行配置，具体的执行命令如下所示： 1234// 获取maxmemory-policy配置127.0.0.1:6379&gt; config get maxmemory-policy// 设置maxmemory-policy配置为allkeys-lru127.0.0.1:6379&gt; config set maxmemory-policy allkeys-lru 在介绍6种的淘汰策略方式的时候，说到了LRU算法，那么什么是LRU算法呢？ LRU算法LRU(Least Recently Used)即表示最近最少使用，也就是在最近的时间内最少被访问的key，算法根据数据的历史访问记录来进行淘汰数据。 它的核心的思想就是：假如一个key值在最近很少被使用到，那么在将来也很少会被访问。 实际上Redis实现的LRU并不是真正的LRU算法，也就是名义上我们使用LRU算法淘汰键，但是实际上被淘汰的键并不一定是真正的最久没用的。 Redis使用的是近似的LRU算法，通过随机采集法淘汰key，每次都会随机选出5个key，然后淘汰里面最近最少使用的key。 这里的5个key只是默认的个数，具体的个数也可以在配置文件中进行配置，在配置文件中的配置如下图所示：当近似LRU算法取值越大的时候就会越接近真实的LRU算法，可以这样理解，因为取值越大那么获取的数据就越全，淘汰中的数据的就越接近最近最少使用的数据。 那么为了实现根据时间实现LRU算法，Redis必须为每个key中额外的增加一个内存空间用于存储每个key的时间，大小是3字节。 在Redis 3.0中对近似的LRU算法做了一些优化，Redis中会维护大小是16的一个候选池的内存。 当第一次随机选取的采样数据，数据都会被放进候选池中，并且候选池中的数据会根据时间进行排序。 当第二次以后选取的数据，只有小于候选池内的最小时间的才会被放进候选池中。 当某一时刻候选池的数据满了，那么时间最大的key就会被挤出候选池。当执行淘汰时，直接从候选池中选取最近访问时间最小的key进行淘汰。 这样做的目的就是选取出最近似符合最近最少被访问的key值，能够正确的淘汰key值，因为随机选取的样本中的最小时间可能不是真正意义上的最小时间。 但是LRU算法有一个弊端：就是假如一个key值在以前都没有被访问到，然而最近一次被访问到了，那么就会认为它是热点数据，不会被淘汰。 然而有些数据以前经常被访问到，只是最近的时间内没有被访问到，这样就导致这些数据很可能被淘汰掉，这样一来就会出现误判而淘汰热点数据。 于是在Redis 4.0的时候除了LRU算法，新加了一种LFU算法，那么什么是LFU算法算法呢？ LFU算法LFU(Least Frequently Used)即表示最近频繁被使用，也就是最近的时间段内，频繁被访问的key，它以最近的时间段的被访问次数的频率作为一种判断标准。 它的核心思想就是：根据key最近被访问的频率进行淘汰，比较少被访问的key优先淘汰，反之则优先保留。 LFU算法反映了一个key的热度情况，不会因为LRU算法的偶尔一次被访问被认为是热点数据。 在LFU算法中支持volatile-lfu策略和allkeys-lfu策略。 以上介绍了Redis的6种淘汰策略，这6种淘汰策略旨在告诉我们怎么做，但是什么时候做？这个还没说，下面我们就来详细的了解Redis什么时候执行淘汰策略。 删除过期键策略在Redis种有三种删除的操作此策略，分别是： 定时删除：创建一个定时器，定时的执行对key的删除操作。 惰性删除：每次只有再访问key的时候，才会检查key的过期时间，若是已经过期了就执行删除。 定期删除：每隔一段时间，就会检查删除掉过期的key。 定时删除对于内存来说是友好的，定时清理出干净的空间，但是对于cpu来说并不是友好的，程序需要维护一个定时器，这就会占用cpu资源。 惰性的删除对于cpu来说是友好的，cpu不需要维护其它额外的操作，但是对于内存来说是不友好的，因为要是有些key一直没有被访问到，就会一直占用着内存。 定期删除是上面两种方案的折中方案，每隔一段时间删除过期的key，也就是根据具体的业务，合理的取一个时间定期的删除key。 通过最合理控制删除的时间间隔来删除key，减少对cpu的资源的占用消耗，使删除操作合理化。 RDB和AOF 的淘汰处理在Redis中持久化的方式有两种RDB和AOF，具体这两种详细的持久化介绍，可以参考这一篇文章[]。 在RDB中是以快照的形式获取内存中某一时间点的数据副本，在创建RDB文件的时候可以通过save和bgsave命令执行创建RDB文件。 这两个命令都不会把过期的key保存到RDB文件中，这样也能达到删除过期key的效果。 当在启动Redis载入RDB文件的时候，Master不会把过期的key载入，而Slave会把过期的key载入。 在AOF模式下，Redis提供了Rewite的优化措施，执行的命令分别是REWRITEAOF和BGREWRITEAOF，这两个命令都不会把过期的key写入到AOF文件中，也能删除过期key。","link":"/2020/10/14/%E5%88%AB%E5%86%8D%E9%97%AE%E6%88%91Redis%E5%86%85%E5%AD%98%E6%BB%A1%E4%BA%86%E8%AF%A5%E6%80%8E%E4%B9%88%E5%8A%9E%E4%BA%86/"},{"title":"","text":"简介在我们实际的项目中，经常会将服务器进行划分，比如：专门运行我们程序的应用服务器、专门运行我们数据库的数据库服务器、还有负责文件存储的文件服务器、负责视频存储的服务器。 这些服务器组成我们的所有服务、各司其职，这样划分的目的相比大家也知道，减轻一台服务器服务器的压力。 当用户请求我们应用应用服务器的时候，由我们的应用再分别访问我们数据库服务器、文件存储服务器等，最后将请求的资源返回给我们的用户，这样构成整个应用的请求过程： 我们今天要讲的就是图片存储，图片储存的方案有很多，比如：可以使用Fastdfs或者HDFS、使用nginx搭建图片存储服务器。 现在比较流行的就是云存储，使用阿里云、七牛云等。这一节我们使用七牛云做了实例讲解。 七牛云入门要是用七牛云的服务，首先注册七牛云账号：https://www.qiniu.com/，这个是七牛云的首页地址： 注册完后直接登陆，登陆后右上角有一个管理控制平台，点击管理控制平台的产品首页，还要进行实名认证才能创建对象存储存储空间，实名认证后点击对象存储的立即添加： 点击立即添加后来到对象存储空间的创建页面，点击新建空间 创建存储空间后可以看见自己创建的空间的详细信息，并且可以创建多个存储空间，每一个存储空间相互独立，不会互相干扰。 七牛云提供了多种的方式操作对象存储服务，我们这里使用的是Java SDK方式，具体的使用可以在开发者中心查看： 在Java SDK开发文档里面有详细的操作的API说明，以及要引入的依赖坐标。 代码实战首先在自己的项目中引入七牛云的依赖坐标： 12345&lt;dependency&gt; &lt;groupId&gt;com.qiniu&lt;/groupId&gt; &lt;artifactId&gt;qiniu-java-sdk&lt;/artifactId&gt; &lt;version&gt;7.2.0&lt;/version&gt;&lt;/dependency&gt; 从Java SDK的操作文档中，简单操作文档的上传的示例代码如下： 1234567891011121314151617181920212223242526272829//构造一个带指定 Region 对象的配置类Configuration cfg = new Configuration(Region.region0());//...其他参数参考类注释UploadManager uploadManager = new UploadManager(cfg);//...生成上传凭证，然后准备上传，填写AK和SK以及buket nameString accessKey = &quot;your access key&quot;;String secretKey = &quot;your secret key&quot;;String bucket = &quot;your bucket name&quot;;//文件路径，如果是Windows情况下，格式是 D:\\\\qiniu\\\\test.pngString localFilePath = &quot;/home/qiniu/test.png&quot;;//默认不指定key的情况下，以文件内容的hash值作为文件名String key = null;Auth auth = Auth.create(accessKey, secretKey);String upToken = auth.uploadToken(bucket);try { Response response = uploadManager.put(localFilePath, key, upToken); //解析上传成功的结果 DefaultPutRet putRet = new Gson().fromJson(response.bodyString(), DefaultPutRet.class); System.out.println(putRet.key); System.out.println(putRet.hash);} catch (QiniuException ex) { Response r = ex.response; System.err.println(r.toString()); try { System.err.println(r.bodyString()); } catch (QiniuException ex2) { //ignore }} 上面的是根据文件路径来上传文件，还支持字节数组上传到空间中： 1234567891011121314151617181920212223242526272829303132//构造一个带指定 Region 对象的配置类Configuration cfg = new Configuration(Region.region0());//...其他参数参考类注释UploadManager uploadManager = new UploadManager(cfg);//...生成上传凭证，然后准备上传String accessKey = &quot;your access key&quot;;String secretKey = &quot;your secret key&quot;;String bucket = &quot;your bucket name&quot;;//默认不指定key的情况下，以文件内容的hash值作为文件名String key = null;try { byte[] uploadBytes = &quot;hello qiniu cloud&quot;.getBytes(&quot;utf-8&quot;); Auth auth = Auth.create(accessKey, secretKey); String upToken = auth.uploadToken(bucket); try { Response response = uploadManager.put(uploadBytes, key, upToken); //解析上传成功的结果 DefaultPutRet putRet = new Gson().fromJson(response.bodyString(), DefaultPutRet.class); System.out.println(putRet.key); System.out.println(putRet.hash); } catch (QiniuException ex) { Response r = ex.response; System.err.println(r.toString()); try { System.err.println(r.bodyString()); } catch (QiniuException ex2) { //ignore } }} catch (UnsupportedEncodingException ex) { //ignore} 以及删除上传的文件： 12345678910111213141516//构造一个带指定 Region 对象的配置类Configuration cfg = new Configuration(Region.region0());//...其他参数参考类注释String accessKey = &quot;your access key&quot;;String secretKey = &quot;your secret key&quot;;String bucket = &quot;your bucket name&quot;;String key = &quot;your file key&quot;;Auth auth = Auth.create(accessKey, secretKey);BucketManager bucketManager = new BucketManager(auth, cfg);try { bucketManager.delete(bucket, key);} catch (QiniuException ex) { //如果遇到异常，说明删除失败 System.err.println(ex.code()); System.err.println(ex.response.toString());} 上面有提到比较关键的信息，因为使用七牛云服务要进行认证，上面有三个数据AK、SK和bucket name，AK和SK也就是AccessKey/SecretKey。 AK和SK这个信息中可以在如下的页面获取，bucket name也就是你在创建存储空间的时候填信息的名称： 有了这些信息，我们可以将上面的代码封装成自己的工具类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public class QiniuUtils { // 填写你的accessKey值 public static String accessKey = &quot;your accessKey&quot;; // 填写你的secretKey 值 public static String secretKey = &quot;your secretKey &quot;; // 填写你的 bucket name值 public static String bucket = &quot;your bucket name&quot;; /** * 上传文件 * @param filePath 文件路径 * @param fileName 文件名 */ public static void uploadFile(String filePath,String fileName){ Configuration cfg = new Configuration(Zone.zone0()); UploadManager uploadManager = new UploadManager(cfg); Auth auth = Auth.create(accessKey, secretKey); String upToken = auth.uploadToken(bucket); try { Response response = uploadManager.put(filePath, fileName, upToken); DefaultPutRet putRet = new Gson().fromJson(response.bodyString(), DefaultPutRet.class); } catch (QiniuException ex) { Response r = ex.response; try { System.err.println(r.bodyString()); } catch (QiniuException ex2) { //ignore } } } /** * 上传文件 * @param bytes 文件字节数组 * @param fileName 文件名 */ public static void uploadFile(byte[] bytes, String fileName){ Configuration cfg = new Configuration(Zone.zone0()); UploadManager uploadManager = new UploadManager(cfg); String key = fileName; Auth auth = Auth.create(accessKey, secretKey); String upToken = auth.uploadToken(bucket); try { Response response = uploadManager.put(bytes, key, upToken); DefaultPutRet putRet = new Gson().fromJson(response.bodyString(), DefaultPutRet.class); } catch (QiniuException ex) { Response r = ex.response; try { System.err.println(r.bodyString()); } catch (QiniuException ex2) { //ignore } } } /** * 删除文件 * @param fileName 文件名 */ public static void deleteFile(String fileName){ Configuration cfg = new Configuration(Zone.zone0()); String key = fileName; Auth auth = Auth.create(accessKey, secretKey); BucketManager bucketManager = new BucketManager(auth, cfg); try { bucketManager.delete(bucket, key); } catch (QiniuException ex) { System.err.println(ex.code()); System.err.println(ex.response.toString()); } }} 然后写一个接口作为文件上传的测试类： 12345678910111213141516171819202122@Autowired private JedisPool jedisPool; //图片上传 @RequestMapping(&quot;/uploadImage&quot;) public ResponseResult upload(@RequestParam(&quot;imgFile&quot;)MultipartFile imgFile){ try{ //获取原始文件名 String originalFilename = imgFile.getOriginalFilename(); //获取图片名后缀 int lastIndexOf = originalFilename.lastIndexOf(&quot;.&quot;); String suffix = originalFilename.substring(lastIndexOf ‐ 1); //使用UUID生成图片名，防止名称一样 String fileName = UUID.randomUUID().toString() + suffix; QiniuUtils.uploadFile(imgFile.getBytes(),fileName); //图片上传成功 ResponseResult result = new ResponseResult (MessageConstant.UPLOAD_SUCCESS,&quot;上传成功&quot;); result.setData(fileName); //将上传图片名称存入Redis，基于Redis的Set集合存储 jedisPool.getResource().sadd(ImageConstant.ALl_PIC_RESOURCES,fileName ); return result; }catch (Exception e){ e.printStackTrace(); //图片上传失败 return new ResponseResult (ResponseConstant.UPLOAD_FAIL,&quot;文件上传失败&quot;); } } 这里在作为图片上传的时候使用Redis的set集合进行缓存，这样做的目的就是，用户自己点击上传文件后，填写完信息，但是后面就有可能包填写的消息取消掉。 这样这些图片就会成为我们的垃圾图片，若是没有存储这些图片的信息，从此也找不到这些图片，这样这些垃圾图片就会占用空间。 我们的解决方案就是，先把所有的这些图片还存在Redis的一个Set集合中，叫做ALl_PIC_RESOURCES。 然后把上传成功的图片还存在Redis的另一个Set集合中叫做L：SUCCESS_PIC_RESOURCES。 最后系统起一个定时任务，定期清理掉ALl_PIC_RESOURCES和L：SUCCESS_PIC_RESOURCES差值的图片数据，清理时间可以设置在晚上凌晨，这样对系统的性能影响就不会那么大。 定时任务可以选用Quartz，我相信这个定时任务框架应该很多人都用过，在项目中直接引入依赖坐标： 12345678&lt;dependency&gt; &lt;groupId&gt;org.quartz-scheduler&lt;/groupId&gt; &lt;artifactId&gt;quartz&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.quartz-scheduler&lt;/groupId&gt; &lt;artifactId&gt;quartz-jobs&lt;/artifactId&gt; &lt;/dependency&gt; 具体的定时任务的配置可以自行百度一下，这里只给出，定时任务清理图片的方法： 12345678910public void deleteImage(){ //比较两个Set集合得到差集，得到无效的图片名称集合 Set&lt;String&gt; imageSet = redisTemplate.opsForSet().difference(&quot;ALl_PIC_RESOURCES&quot;, &quot;L：SUCCESS_PIC_RESOURCES&quot;); for (String imageName : imageSet ) { //删除图片 QiniuUtils.deleteFileFrom(imageName ); //删除缓存中无用的图片 redisTemplate.boundSetOps(&quot;ALl_PIC_RESOURCES&quot;).remove(imageName ); } } 好了，今天的实例讲解就到这里了，觉得有帮助的求个三连，我们下期再见。","link":"/2020/10/14/%E5%9B%BE%E7%89%87%E5%AD%98%E5%82%A8%E6%96%B9%E6%A1%88%E5%AE%9E%E6%88%98%E6%95%99%E7%A8%8B-%E4%B8%83%E7%89%9B%E4%BA%91%E5%AD%98%E5%82%A8/"},{"title":"","text":"前言不知不觉写文章已经快半年了，本来之前写文章只是为了自己总结知识，不知不觉中关注的朋友越来越多了。 现在写文章不单单只是为了考虑自己能看懂，还要考虑各位读者大大是否能看懂，考虑输出文章的质量。 现在的每一次写作就好像在搞一次艺术品，细细雕琢，进行每一次的加工。文章的逻辑性，易懂性，还有文章的排版的美观度，都要细细斟酌。 写在前面先来一碗鸡汤：世界上并没有什么救世主，假如有那便是你自己；世界上也没有什么奇迹，假如有那只是努力的另一个名字罢了。 想想自己毕业差不多一年来走过来的路，看看现在的自己，一切都值得，往后还会不断的努力，看到越来越强的自己。 话不多说下面就直接上干货了，今天来深入的了解CAS和AQS，文章采用层次式、图文并茂的方式一层一层的进行剖析，让各位读者大大能够深入理解。 AQS简介AQS（AbstractQueuedSynchronizer）为抽象队列同步器，简单的说AQS就是一个抽象类，抽象类就是AbstractQueuedSynchronizer，没有实现任何的接口，仅仅定义了同步状态（state）的获取和释放的方法。 它提供了一个FIFO队列，多线程竞争资源的时候，没有竞争到的线程就会进入队列中进行等待，并且定义了一套多线程访问共享资源的同步框架。 在AQS中的锁类型有两种：分别是Exclusive(独占锁)**和Share(共享锁)**。 独占锁就是每次都只有一个线程运行，例如ReentrantLock。关于ReentrantLock之前写过一片详细的源码文章，喜欢的可以看一看[]。 共享锁就是同时可以多个线程运行，如Semaphore、CountDownLatch、ReentrantReadWriteLock。 AQS源码分析在AQS的源码可以看到对于state共享变量，使用volatile关键字进行修饰，从而保证了可见性，若是对于volatile关键字不熟悉的可以参考这一篇[]。从上面的源码中可以看出，对于state的修改操作提供了setState和compareAndSetState，那么为什么要提供这两个对state的修改呢？ 因为compareAndSetState方法通常使用在获取到锁之前，当前线程不是锁持有者，对于state的修改可能存在线程安全问题，所以需要保证对state修改的原子性操作。 而setState方法通常用于当前正持有锁的线程对state共享变量进行修改，因为不存在竞争，是线程安全的，所以没必要使用CAS操作。 分析了AQS的源码的实现，接下来我们看看AQS的实现的原理。这里AQS的实现源码和理论都会比较简单，因为还没有涉及到具体的实现类。 AQS实现原理上面说到AQS中维护了一个FIFO队列，并且该队列式一个双向链表，链表中的每一个节点为Node节点，Node类是AbstractQueuedSynchronizer中的一个内部类。 让我们来看看AQS中Node内部类的源码，这样有助于我们能够对AQS的内部的实现更加的清晰： 123456789101112131415161718192021222324252627282930313233343536373839static final class Node { static final Node SHARED = new Node(); static final Node EXCLUSIVE = null; static final int CANCELLED = 1; static final int SIGNAL = -1; static final int CONDITION = -2; static final int PROPAGATE = -3; volatile int waitStatus; volatile Node prev; volatile Node next; volatile Thread thread; Node nextWaiter; final boolean isShared() { return nextWaiter == SHARED; } final Node predecessor() throws NullPointerException { Node p = prev; if (p == null) throw new NullPointerException(); else return p; } Node() { // Used to establish initial head or SHARED marker } Node(Thread thread, Node mode) { // Used by addWaiter this.nextWaiter = mode; this.thread = thread; } Node(Thread thread, int waitStatus) { // Used by Condition this.waitStatus = waitStatus; this.thread = thread; } } 可以看到上面的Node类比较简单，只是对于每个Node节点拥有的属性进行维护，在Node内部类中最重要的基本构成就是这几个： 123volatile Node prev;volatile Node next;volatile Thread thread; 根据源码的分析和线程的竞争共享资源的原理，关于AQS的实现原理，我这里画了一张图：在FIFO队列中，头节点占有锁，也就是头节点才是锁的持有者，尾指针指向队列的最后一个等待线程节点，除了头节点和尾节点，节点之间都有前驱指针和后继指针 在AQS中维护了一个共享变量state，标识当前的资源是否被线程持有，多线程竞争的时候，会去判断state是否为0，尝试的去把state修改为1 分析了AQS的源码的实现和原理实现，但是AQS里面具体是没有做同步的具体实现，如果要什么了解AQS的具体的实现原理，要需要看AQS的具体实现类，这边就以ReentrantLock为例。 ReentrantLock实现原理如果多线程在竞争共享资源时，竞争失败的线程就会添加入FIFO队列的尾部。 在ReentrantLock的的具体实现中，这边以在ReentrantLock的非公平锁的实现为例，因为公平锁的实现，之前已经写过一篇文章分析过了。 我们来看看新添加节点的源码写的实现逻辑：当竞争锁资源的线程失败后直接进入acquire(1)方法，来来看看acquire(1)的具体实现：从源码中可以看出，acquire(1)的实现主要有这三步的逻辑： tryAcquire(arg)：尝试再次获取锁。 addWaiter(Node.EXCLUSIVE)：若是获取锁失败，就会将当前线程组装成一个Node节点，进行如对操作。 acquireQueued(addWaiter(Node.EXCLUSIVE), arg))：acquireQueued方法以addWaiter返回的头节点作为参数，内部实现进行锁自旋，以及判断是否应该执行线程挂起。 下面我们再来看看tryAcquire(arg)的源码，从上面的看一看出arg的值为1，具体的实现源码如下： 从源码的分析中可以看出，tryAcquire(arg)的实现也就是判断state的值是否已经被释放，若释放则当前线程就会CAS操作将state设置为1，若是没有释放，就会判断是否可以进行锁的重入。 分析完tryAcquire(arg)的实现，来看看addWaiter，入队操作的实现源码如下：从上面的源码分析，可以看出对于新加入的线程添加到双向链表中使用尾插法，具体的实现原理图如下所示。从上图分析，当线程假如队列中，主要进行这几步操作新加入的节点prev指针指向原来的tail节点，原来的tail节点的next指针指向新加入的节点，这个也就是常见的双向列表尾插法的操作。 最后把tail指向新加入的节点，如此一来就完成了新加入节点的入队操作，接下来我们接着分析源码。 当然这里的前提是队列中不为空，若是为空的话，不会走上面的逻辑，而是走enq(node)，进行初始化节点，我们来看看enq(node)操作，源码如下：执行完上面的入对操作后，接着执行acquireQueued方法，来看看它的具体实现源码：从上上面的源码中可以看出，涉及到头节点head的出队操作，并且将当前线程的node节点晋升为head节点。 因为只有头节点才是锁的持有者，所以对于head节点的出队操作，head的指向会随时改变，我这里画了一张原理图如下所示：具体实现如上图所示，会把原来的头节点进行出队操作，也就是把原来的头节点next指针指向null，原来第二节点的prev指针指向null。 最后把head指针指向第二节点，当然thread2同时还会修改共享状态变量state的值，如此一来就完成了锁的释放。 当释放完锁之后，就会执行shouldParkAfterFailedAcquire(p, node) &amp;&amp;parkAndCheckInterrupt()判断当前的线程是否应该被挂起，我们来看看它的源码实现：在shouldParkAfterFailedAcquire中的实现，当前驱节点的状态量waitStatus为SIGNAL的时候，就会挂起。通过上面的分析对于AQS的实现基本有比较清晰的认识，主要是对实现类ReentrantLock的实现原理进行深入的分析，并且是基于非公平锁和独占锁的实现。 在AQS的底层维护了一个FIFO队列，多线程竞争共享资源的时候，失败的线程会被添加入队列中，非公平锁实现中，新加入的线程节点会自旋尝试的获取锁。 分析完AQS我们来分析CAS，那么什么是CAS呢？ CAS简介在分析ReentrantLock的具体实现的源码中，可以看出所有涉及设置共享变量的操作，都会指向CAS操作，保证原子性操作。 CAS(compare and swap)原语理解就是比较并交换的意思，CAS是一种乐观锁的实现。 在CAS的算法实现中有三个值：更新的变量、旧的值、新值。在修改共享资源时候，会与原值进行比较，若是等于原值，就修改为新值。 于是在这里的算法实现下，即使不加锁，也能保证数据的可见性，即使的发现数据是否被更改，若是数据已经被更新则写操作失败。 但是CAS也会引发ABA的问题，什么是ABA问题呢？ 不慌请听我详细道来 ABA问题ABA问题就是假如有两个线程，同一时间读取一个共享变量state=1，此时两个线程都已经将state的副本赋值到自己的工作内存中。 当线程一对state修改state=state+1，并且写入到主存中，然后线程一又对state=state-1写入到主存，此时主存的state是变化了两次，只不过又变回了原来的值。 那么此时线程二修改state的时候就会修改成功，这就是ABA问题。对于ABA问题的解决方案就是加版本号（version），每次进行比较的时候，也会比较版本号。 因为版本版是只增不减，比如以时间作为版本号，每一时刻的时间都不一样，这样就能避免ABA的问题。 CAS性能分析相对于synchronized的阻塞算法的实现，CAS采用的是乐观锁的非阻塞算法的实现，一般CPU在进行线程的上下文切换的时间比执行CPU的指令集的时间长，所以CAS操作在性能上也有了很大的提升。 但是所有的算法都是没有最完美的，在执行CAS的操作中，没有更新成功的就会自旋，这样也会消耗CPU的资源，对于CPU来说是不友好的。","link":"/2020/10/14/%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90AQS%E5%92%8CCAS%EF%BC%8C%E7%9C%8B%E4%BA%86%E9%83%BD%E8%AF%B4%E5%A5%BD/"},{"title":"","text":"本文脑图 运行时数据区模型在java虚拟机中把内存分为若干个不同的数据区域。这些区域有各自的用途，有些区域随着虚拟机进程启动而存在，有些区域则依赖用户线程的启动和结束而建立和销毁。在JVM中主要分为以下几个区域： 程序计数器 方法区 虚拟机栈 本地方法栈 java堆 程序计数器程序计数器是内存中较小的一部分区域，是当前线程执行的字节码的行号指示器。在字节码解释器工作时通过计数器的值来选取下一条指令。 为什么需要计数器？多线程情况下，一条线程中有多个指令，为了使线程切换可以恢复到正确执行位置，每个线程都具有各自独立的程序计数器，所以该区域是非线程共享的内存区域。 如果执行的是Java方法，计数器记录的是正在执行的字节码指令的地址；若执行的是Native方法，计数器存储为空。这块内存区域是虚拟机规范中唯一没有OutOfMemoryError的区域。 我们可以得出程序计数器主要有两个作用： 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。 在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。 方法区方法区也称”永久代“，它用于存储虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据，是各个线程共享的内存区域。 在JDK8之前的HotSpot JVM，存放这些”永久的”的区域叫做“永久代(permanent generation)”。永久代是一片连续的堆空间，在JVM启动之前通过在命令行设置参数-XX:MaxPermSize来设定永久代最大可分配的内存空间，默认大小是64M（64位JVM默认是85M）。 方法区或永生代相关设置 -XX:PermSize=64MB 最小尺寸，初始分配 -XX:MaxPermSize=256MB 最大允许分配尺寸， 按需分配XX:+CMSClassUnloadingEnabled -XX:+CMSPermGenSweepingEnabled 设置垃圾不回收 -server选项下默认MaxPermSize为64m，-client选项下默认MaxPermSize为32m java虚拟机规范堆方法去区限制非常的宽松，可以不选择垃圾回收，以及不需要连续的内存和可扩展的大小。这个区域主要是针对于常量池的回收以及对类型的卸载，当方法区无法分配到足够的内存的时候也会抛出OOM。 虚拟机栈虚拟机栈是每个java方法的内存模型：每个方法被执行的时候都会创建一个”栈帧”,用于存储局部变量表(包括参数)、操作栈、方法出口等信息。每个方法被调用到执行完的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。 平时说的栈一般指局部变量表部分。栈帧对应的结构图，如下图所示：局部变量表所需要的空间在编译期完成分配，当执行一个方法时，该方法需要在栈帧中分配多大的局部变量表的空间完全是可以确定的，因此在方法运行的期间不会改变局部变量表的大小。 初级程序员可能笼统的将Java内存分为堆内存和栈内存时，该区域就是常说的栈内存，该区域的局部变量表存放基本类型、对象的引用类型，在对象的引用类型中存储的是指向对象的地址。 该区域会出现两种异常 当线程请求的栈深度大于虚拟机所允许的深度，就会抛出StackOverflowError异常。 一般虚拟机的内存都是动态扩展的，但是有可能动态的扩展还是配不到足够的内存，就会抛出OOM异常。 本地方法栈本地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务。 本地方法栈为虚拟机使用到的native方法服务，可能底层调用的c或者c++,我们打开jdk安装目录可以看到也有很多用c编写的文件，可能就是native方法所调用的c代码。 Java堆Java 堆（Java Heap）是Java 虚拟机所管理的内存中最大的一块。Java 堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。 此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。堆内存是所有线程共有的，可以分为两个部分：年轻代和老年代。 注意:它是所有线程共享的，它的目的是存放对象实例。同时它也是GC所管理的主要区域，因此常被称为GC堆。根据虚拟机规范，Java堆可以存在物理上不连续的内存空间，就像磁盘空间只要逻辑是连续的即可。它的内存大小可以设为固定大小，也可以扩展。当前主流的虚拟机如HotPot都能按扩展实现(通过设置 -Xmx和-Xms)，如果堆中没有内存内存完成实例分配，而且堆无法扩展将报OOM错误(OutOfMemoryError) 新生代又分为：Eden空间、From Survivor、To Survivor空间。进一步划分的目的是更好地回收内存，或者更快地分配内存。 分代回收的原因：对象的生命周期不同，所以针对不同生命周期的对象可以采取不同的回收方式，以便提高回收效率。从内存分配的角度来看，线程共享的java堆中可能会划分出多个线程私有的分配缓冲区。 垃圾回收算法标记-清除标记-清除是最基本的回收算法，后面的算法的设计的思想都是基于此算法进行设计。标记-清除算法一共分为两个阶段标记和清除阶段，标记阶段是将不可达的对象（即为不存活的对象）进行标记，接着清除阶段将这些标记的对象进行清除。标记-清除算法主要有两个问题：一个是效率的问题，标记和清除两个过程效率都不高；另一个问题就是该算法会产生很多的内存碎片，大量的不连续内存空间，当程序需要申请较大的内存空间存储大对象的时候，有可能无法申请到足够的内存空间而不得不再一次触发一次垃圾回收动作。 复制算法复制算法将内存划分为两个区间，在任意时间点，所有动态分配的对象都只能分配在其中一个区间（称为活动区间），而另外一个区间（称为空闲区间）则是空闲的。 当有效内存空间耗尽时，JVM将暂停程序运行，开启复制算法GC线程。接下来GC线程会将活动区间内的存活对象，全部复制到空闲区间，且严格按照内存地址依次排列，与此同时，GC线程将更新存活对象的内存引用地址指向新的内存地址。 此时，空闲区间已经与活动区间交换，而垃圾对象现在已经全部留在了原来的活动区间，也就是现在的空闲区间。事实上，在活动区间转换为空间区间的同时，垃圾对象已经被一次性全部回收。 新生代中因为对象都是”朝生夕死的”，深入理解JVM虚拟机上说98%的对象存活率很低，适用于复制算法，复制算法比较适合用于存活率低的内存区域。它优化了标记/清除算法的效率和内存碎片问题。 JVM不是平分内存，新生代中由于存活率低，不需要复制保留那么大的区域造成空间上的浪费，因此不需要按1:1划分内存区域，而是将内存分为一块Eden空间和From Survivor、To Survivor【保留空间】。 （1）新生代：大多数对象在新生代中被创建，其中很多对象的生命周期很短。每次新生代的垃圾回收（又称Minor GC）后只有少量对象存活，所以选用复制算法，只需要少量的复制成本就可以完成回收。 在新生代内存中每次只是用Eden和其中的一个S区。当Eden区满时，还存活的对象将被复制到两个Survivor区中的一个。当这个Survivor区满时，此区的存活且不满足“晋升”条件的对象将被复制到另外一个Survivor区。 对象每经历一次Minor GC，年龄加1，达到“晋升年龄阈值”后，被放到老年代，这个过程也称为“晋升”。显然，“晋升年龄阈值”的大小直接影响着对象在新生代中的停留时间，在Serial和ParNew GC两种回收器中，“晋升年龄阈值”通过参数MaxTenuringThreshold设定，默认值为15。 （2）老年代：在新生代中经历了N次垃圾回收后仍然存活的对象，就会被放到年老代，该区域中对象存活率高。老年代的垃圾回收（又称Major GC）通常使用标记-清理或标记-整理算法。整堆包括新生代和老年代的垃圾回收称为Full GC。 （3）永久代：主要存放元数据，例如Class、Method的元信息，与垃圾回收要回收的Java对象关系不大。相对于新生代和年老代来说，该区域的划分对垃圾回收影响比较小。在 JDK 1.8中移除整个永久代，取而代之的是一个叫元空间的区域。 默认的Eden : from : to = 8 : 1 : 1 ( 可以通过参数 –XX:SurvivorRatio 来设定 )，即： Eden = 8/10 的新生代空间大小，from = to = 1/10 的新生代空间大小。 在新生代中，并不是每次存活的对象都少于10%，有时候若是存活的对象大于10%，就会想老年代进行空间分配担保。 标记-整理标记-整理算法也分为两步，首先标记不可达的对象，然后存活的对象往一端移动，然后直接清理掉端边界以外的内存。 老年代中存活率比较高，要是使用复制算法，会大量浪费时间在复制对象上，因此复制算法不适合用在存活率比较高的场景。标记的存活对象将会被整理，按照内存地址依次排列，而未被标记的内存会被清理掉。如此一来，当我们需要给新对象分配内存时，JVM只需要持有一个内存的起始地址即可，这比维护一个空闲列表显然少了许多开销。 不难看出，标记/整理算法不仅可以弥补标记/清除算法当中，内存区域分散的缺点，也消除了复制算法当中，内存减半的高额代价。 不过任何算法都会有其缺点，标记/整理算法唯一的缺点就是效率也不高，不仅要标记所有存活对象，还要整理所有存活对象的引用地址。从效率上来说，标记/整理算法要低于复制算法。 关于JVM深入研究以及JVM调优，后面的文章会继续深入，这篇文章先介绍JVM的内存模型，以及回收算法。","link":"/2020/10/14/%E8%BF%98%E5%9C%A8%E5%AD%A6JVM%EF%BC%9F%E6%88%91%E9%83%BD%E5%B8%AE%E4%BD%A0%E6%80%BB%E7%BB%93%E5%A5%BD%E4%BA%86%EF%BC%88%E9%99%84%E8%84%91%E5%9B%BE%EF%BC%89/"},{"title":"","text":"当希望Mysql能够高效的执行的时候，最好的办法就是清楚的了解Mysql是如何执行查询的，只有更加全面的了解SQL执行的每一个过程，才能更好的进行SQl的优化。 当执行一条查询的SQl的时候大概发生了一下的步骤： 客户端发送查询语句给服务器。 服务器首先检查缓存中是否存在该查询，若存在，返回缓存中存在的结果。若是不存在就进行下一步。 服务器进行SQl的解析、语法检测和预处理，再由优化器生成对应的执行计划。 Mysql的执行器根据优化器生成的执行计划执行，调用存储引擎的接口进行查询。 服务器将查询的结果返回客户端。 Mysql的执行的流程Mysql的执行的流程图如下图所示：这里以一个实例进行说明Mysql的的执行过程，新建一个User表，如下： 12345678910111213141516171819202122232425// 新建一个表DROP TABLE IF EXISTS User;CREATE TABLE `User` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(10) DEFAULT NULL, `age` int DEFAULT 0, `address` varchar(255) DEFAULT NULL, `phone` varchar(255) DEFAULT NULL, `dept` int, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=40 DEFAULT CHARSET=utf8;// 并初始化数据，如下INSERT INTO User(name,age,address,phone,dept)VALUES('张三',24,'北京','13265543552',2);INSERT INTO User(name,age,address,phone,dept)VALUES('张三三',20,'北京','13265543557',2);INSERT INTO User(name,age,address,phone,dept)VALUES('李四',23,'上海','13265543553',2);INSERT INTO User(name,age,address,phone,dept)VALUES('李四四',21,'上海','13265543556',2);INSERT INTO User(name,age,address,phone,dept)VALUES('王五',27,'广州','13265543558',3);INSERT INTO User(name,age,address,phone,dept)VALUES('王五五',26,'广州','13265543559',3);INSERT INTO User(name,age,address,phone,dept)VALUES('赵六',25,'深圳','13265543550',3);INSERT INTO User(name,age,address,phone,dept)VALUES('赵六六',28,'广州','13265543561',3);INSERT INTO User(name,age,address,phone,dept)VALUES('七七',29,'广州','13265543562',4);INSERT INTO User(name,age,address,phone,dept)VALUES('八八',23,'广州','13265543563',4);INSERT INTO User(name,age,address,phone,dept)VALUES('九九',24,'广州','13265543564',4); 现在针对这个表发出一条SQl查询：查询每个部门中25岁以下的员工个数大于3的员工个数和部门编号，并按照人工个数降序排序和部门编号升序排序的前两个部门。 1SELECT dept,COUNT(phone) AS num FROM User WHERE age&lt; 25 GROUP BY dept HAVING num &gt;= 3 ORDER BY num DESC,dept ASC LIMIT 0,2; 执行连接器开始执行这条sql时，会检查该语句是否有权限，若是没有权限就直接返回错误信息，有权限会进行下一步，校验权限的这一步是在图一的连接器进行的，对连接用户权限的校验。 执行检索内存相连建立之后，履行查询语句的时候，会先行检索内存，Mysql会先行冗余这个sql与否履行过，以此Key-Value的形式平缓适用内存中，Key是检索预定，Value是结果集。 假如内存key遭击中，便会间接回到给客户端，假如没命中，便会履行后续的操作，完工之后亦会将结果内存上去，当下一次进行查询的时候也是如此的循环操作。 执行分析器分析器主要有两步：（1）词法分析（2）语法分析 词法分析主要执行提炼关键性字，比如select，提交检索的表，提交字段名，提交检索条件。语法分析主要执行辨别你输出的sql与否准确，是否合乎mysql的语法。 当Mysql没有命中内存的时候，接着执行的是 FROM student 负责把数据库的表文件加载到内存中去，WHERE age&lt; 60，会把所示表中的数据进行过滤，取出符合条件的记录行，生成一张临时表，如下图所示。GROUP BY dept 会把上图的临时表分成若干临时表，切分的过程如下图所示：查询的结果只有部门2和部门3才有符合条件的值，生成如上两图的临时表。接着执行SELECT后面的字段，SELECT后面可以是表字段也可以是聚合函数。 这里SELECT的情况与是否存在GROUP BY有关，若是不存在Mysql直接按照上图内存中整列读取。若是存在分别SELECT临时表的数据。 最后生成的临时表如下图所示：紧接着执行HAVING num&gt;2过滤员工数小于等于2的部门，对于WHERE和HAVING都是进行过滤，那么这两者有什么不同呢？ 第一点是WHERE后面只能对表字段进行过滤，不能使用聚合函数，而HAVING可以过滤表字段也可以使用聚合函数进行过滤。 第二点是WHERE是对执行from USer操作后，加载表数据到内存后，WHERE是对原生表的字段进行过滤，而HAVING是对SELECT后的字段进行过滤，也就是WHERE不能使用别名进行过滤。 因为执行WHERE的时候，还没有SELECT，还没有给字段赋予别名。接着生成的临时表如下图所示：最后在执行ORDER BY后面的排序以及limit0,2取得前两个数据，因为这里数据比较少，没有体现出来。最后生成得结果也是如上图所示。接着判断这个sql语句是否有语法错误，关键性词与否准确等等。 执行优化器查询优化器会将解析树转化成执行计划。一条查询可以有多种执行方法，最后都是返回相同结果。优化器的作用就是找到这其中最好的执行计划。 生成执行计划的过程会消耗较多的时间，特别是存在许多可选的执行计划时。如果在一条SQL语句执行的过程中将该语句对应的最终执行计划进行缓存。 当相似的语句再次被输入服务器时，就可以直接使用已缓存的执行计划，从而跳过SQL语句生成执行计划的整个过程，进而可以提高语句的执行速度。 MySQL使用基于成本的查询优化器。它会尝试预测一个查询使用某种执行计划时的成本，并选择其中成本最少的一个。 执行执行器由优化器生成得执行计划，交由执行器进行执行，执行器调用存储引擎得接口，存储引擎获取数据并返回，结束整个查询得过程。 这里之讲解了select的过程，对于update这些修改数据或者删除数据的操作，会涉及到事务，会使用两个日志模块，redo log和binlog日志。具体对这两个日志的介绍请看着一篇文章。 以前的Mysql的默认存储引擎MyISAM引擎是没redo log的，而现在的默认存储引擎InnoDB引擎便是透过redo 复杂度来拥护事务的，保证事务能够准确的回滚或者提交，保证事务的ACID。","link":"/2020/10/14/%E9%9D%A2%E8%AF%95%E5%AE%98%EF%BC%9A%E5%90%AC%E8%AF%B4%E4%BD%A0sql%E5%86%99%E7%9A%84%E6%8C%BA%E6%BA%9C%E7%9A%84%EF%BC%8C%E4%BD%A0%E8%AF%B4%E4%B8%80%E8%AF%B4%E6%9F%A5%E8%AF%A2sql%E7%9A%84%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B%E3%80%82/"},{"title":"","text":"作者：fredalhttps://fredal.xin/java-error-check JAVA线上故障排查全套路 线上故障主要会包括cpu、磁盘、内存以及网络问题，而大多数故障可能会包含不止一个层面的问题，所以进行排查时候尽量四个方面依次排查一遍。同时例如jstack、jmap等工具也是不囿于一个方面的问题的，基本上出问题就是df、free、top 三连，然后依次jstack、jmap伺候，具体问题具体分析即可。 CPU 一般来讲我们首先会排查cpu方面的问题。cpu异常往往还是比较好定位的。原因包括业务逻辑问题(死循环)、频繁gc以及上下文切换过多。而最常见的往往是业务逻辑(或者框架逻辑)导致的，可以使用jstack来分析对应的堆栈情况。 使用jstack分析cpu问题 我们先用ps命令找到对应进程的pid(如果你有好几个目标进程，可以先用top看一下哪个占用比较高)。 接着用top -H -p pid来找到cpu使用率比较高的一些线程 然后将占用最高的pid转换为16进制printf '%x\\n' pid得到nid 接着直接在jstack中找到相应的堆栈信息jstack pid |grep 'nid' -C5 –color 可以看到我们已经找到了nid为0x42的堆栈信息，接着只要仔细分析一番即可。 当然更常见的是我们对整个jstack文件进行分析，通常我们会比较关注WAITING和TIMED_WAITING的部分，BLOCKED就不用说了。我们可以使用命令cat jstack.log | grep \"java.lang.Thread.State\" | sort -nr | uniq -c来对jstack的状态有一个整体的把握，如果WAITING之类的特别多，那么多半是有问题啦。 频繁gc 当然我们还是会使用jstack来分析问题，但有时候我们可以先确定下gc是不是太频繁，使用jstat -gc pid 1000命令来对gc分代变化情况进行观察，1000表示采样间隔(ms)，S0C/S1C、S0U/S1U、EC/EU、OC/OU、MC/MU分别代表两个Survivor区、Eden区、老年代、元数据区的容量和使用量。YGC/YGT、FGC/FGCT、GCT则代表YoungGc、FullGc的耗时和次数以及总耗时。如果看到gc比较频繁，再针对gc方面做进一步分析，具体可以参考一下gc章节的描述。 上下文切换 针对频繁上下文问题，我们可以使用vmstat命令来进行查看 cs(context switch)一列则代表了上下文切换的次数。 如果我们希望对特定的pid进行监控那么可以使用 pidstat -w pid命令，cswch和nvcswch表示自愿及非自愿切换。 磁盘 磁盘问题和cpu一样是属于比较基础的。首先是磁盘空间方面，我们直接使用df -hl来查看文件系统状态 更多时候，磁盘问题还是性能上的问题。我们可以通过iostatiostat -d -k -x来进行分析 最后一列%util可以看到每块磁盘写入的程度，而rrqpm/s以及wrqm/s分别表示读写速度，一般就能帮助定位到具体哪块磁盘出现问题了。 另外我们还需要知道是哪个进程在进行读写，一般来说开发自己心里有数，或者用iotop命令来进行定位文件读写的来源。 不过这边拿到的是tid，我们要转换成pid，可以通过readlink来找到pidreadlink -f /proc/*/task/tid/../..。 找到pid之后就可以看这个进程具体的读写情况cat /proc/pid/io 我们还可以通过lsof命令来确定具体的文件读写情况lsof -p pid 内存 内存问题排查起来相对比CPU麻烦一些，场景也比较多。主要包括OOM、GC问题和堆外内存。一般来讲，我们会先用free命令先来检查一发内存的各种情况。 堆内内存 内存问题大多还都是堆内内存问题。表象上主要分为OOM和StackOverflow。 OOM JMV中的内存不足，OOM大致可以分为以下几种： Exception in thread \"main\" java.lang.OutOfMemoryError: unable to create new native thread 这个意思是没有足够的内存空间给线程分配java栈，基本上还是线程池代码写的有问题，比如说忘记shutdown，所以说应该首先从代码层面来寻找问题，使用jstack或者jmap。如果一切都正常，JVM方面可以通过指定Xss来减少单个thread stack的大小。另外也可以在系统层面，可以通过修改/etc/security/limits.confnofile和nproc来增大os对线程的限制 Exception in thread \"main\" java.lang.OutOfMemoryError: Java heap space 这个意思是堆的内存占用已经达到-Xmx设置的最大值，应该是最常见的OOM错误了。解决思路仍然是先应该在代码中找，怀疑存在内存泄漏，通过jstack和jmap去定位问题。如果说一切都正常，才需要通过调整Xmx的值来扩大内存。 Caused by: java.lang.OutOfMemoryError: Meta space 这个意思是元数据区的内存占用已经达到XX:MaxMetaspaceSize设置的最大值，排查思路和上面的一致，参数方面可以通过XX:MaxPermSize来进行调整(这里就不说1.8以前的永久代了)。 Stack Overflow 栈内存溢出，这个大家见到也比较多。 Exception in thread \"main\" java.lang.StackOverflowError 表示线程栈需要的内存大于Xss值，同样也是先进行排查，参数方面通过Xss来调整，但调整的太大可能又会引起OOM。 使用JMAP定位代码内存泄漏 上述关于OOM和StackOverflow的代码排查方面，我们一般使用JMAPjmap -dump:format=b,file=filename pid来导出dump文件 通过mat(Eclipse Memory Analysis Tools)导入dump文件进行分析，内存泄漏问题一般我们直接选Leak Suspects即可，mat给出了内存泄漏的建议。另外也可以选择Top Consumers来查看最大对象报告。和线程相关的问题可以选择thread overview进行分析。除此之外就是选择Histogram类概览来自己慢慢分析，大家可以搜搜mat的相关教程。 日常开发中，代码产生内存泄漏是比较常见的事，并且比较隐蔽，需要开发者更加关注细节。比如说每次请求都new对象，导致大量重复创建对象；进行文件流操作但未正确关闭；手动不当触发gc；ByteBuffer缓存分配不合理等都会造成代码OOM。 另一方面，我们可以在启动参数中指定-XX:+HeapDumpOnOutOfMemoryError来保存OOM时的dump文件。 gc问题和线程 gc问题除了影响cpu也会影响内存，排查思路也是一致的。一般先使用jstat来查看分代变化情况，比如youngGC或者fullGC次数是不是太多呀；EU、OU等指标增长是不是异常呀等。 线程的话太多而且不被及时gc也会引发oom，大部分就是之前说的unable to create new native thread。除了jstack细细分析dump文件外，我们一般先会看下总体线程，通过pstreee -p pid |wc -l。 或者直接通过查看/proc/pid/task的数量即为线程数量。 堆外内存 如果碰到堆外内存溢出，那可真是太不幸了。首先堆外内存溢出表现就是物理常驻内存增长快，报错的话视使用方式都不确定，如果由于使用Netty导致的，那错误日志里可能会出现OutOfDirectMemoryError错误，如果直接是DirectByteBuffer，那会报OutOfMemoryError: Direct buffer memory。 堆外内存溢出往往是和NIO的使用相关，一般我们先通过pmap来查看下进程占用的内存情况pmap -x pid | sort -rn -k3 | head -30，这段意思是查看对应pid倒序前30大的内存段。这边可以再一段时间后再跑一次命令看看内存增长情况，或者和正常机器比较可疑的内存段在哪里。 我们如果确定有可疑的内存端，需要通过gdb来分析gdb --batch --pid {pid} -ex \"dump memory filename.dump {内存起始地址} {内存起始地址+内存块大小}\" 获取dump文件后可用heaxdump进行查看hexdump -C filename | less，不过大多数看到的都是二进制乱码。 NMT是Java7U40引入的HotSpot新特性，配合jcmd命令我们就可以看到具体内存组成了。需要在启动参数中加入 -XX:NativeMemoryTracking=summary 或者 -XX:NativeMemoryTracking=detail，会有略微性能损耗。 一般对于堆外内存缓慢增长直到爆炸的情况来说，可以先设一个基线jcmd pid VM.native_memory baseline。 然后等放一段时间后再去看看内存增长的情况，通过jcmd pid VM.native_memory detail.diff(summary.diff)做一下summary或者detail级别的diff。 可以看到jcmd分析出来的内存十分详细，包括堆内、线程以及gc(所以上述其他内存异常其实都可以用nmt来分析)，这边堆外内存我们重点关注Internal的内存增长，如果增长十分明显的话那就是有问题了。 detail级别的话还会有具体内存段的增长情况，如下图。 此外在系统层面，我们还可以使用strace命令来监控内存分配 strace -f -e \"brk,mmap,munmap\" -p pid 这边内存分配信息主要包括了pid和内存地址。 不过其实上面那些操作也很难定位到具体的问题点，关键还是要看错误日志栈，找到可疑的对象，搞清楚它的回收机制，然后去分析对应的对象。比如DirectByteBuffer分配内存的话，是需要full GC或者手动system.gc来进行回收的(所以最好不要使用-XX:+DisableExplicitGC)。那么其实我们可以跟踪一下DirectByteBuffer对象的内存情况，通过jmap -histo:live pid手动触发fullGC来看看堆外内存有没有被回收。如果被回收了，那么大概率是堆外内存本身分配的太小了，通过-XX:MaxDirectMemorySize进行调整。如果没有什么变化，那就要使用jmap去分析那些不能被gc的对象，以及和DirectByteBuffer之间的引用关系了。 GC问题 堆内内存泄漏总是和GC异常相伴。不过GC问题不只是和内存问题相关，还有可能引起CPU负载、网络问题等系列并发症，只是相对来说和内存联系紧密些，所以我们在此单独总结一下GC相关问题。 我们在cpu章介绍了使用jstat来获取当前GC分代变化信息。而更多时候，我们是通过GC日志来排查问题的，在启动参数中加上-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps来开启GC日志。 常见的Young GC、Full GC日志含义在此就不做赘述了。 针对gc日志，我们就能大致推断出youngGC与fullGC是否过于频繁或者耗时过长，从而对症下药。我们下面将对G1垃圾收集器来做分析，这边也建议大家使用G1-XX:+UseG1GC。 youngGC过频繁 youngGC频繁一般是短周期小对象较多，先考虑是不是Eden区/新生代设置的太小了，看能否通过调整-Xmn、-XX:SurvivorRatio等参数设置来解决问题。如果参数正常，但是young gc频率还是太高，就需要使用Jmap和MAT对dump文件进行进一步排查了。 youngGC耗时过长 耗时过长问题就要看GC日志里耗时耗在哪一块了。以G1日志为例，可以关注Root Scanning、Object Copy、Ref Proc等阶段。Ref Proc耗时长，就要注意引用相关的对象。Root Scanning耗时长，就要注意线程数、跨代引用。Object Copy则需要关注对象生存周期。而且耗时分析它需要横向比较，就是和其他项目或者正常时间段的耗时比较。比如说图中的Root Scanning和正常时间段比增长较多，那就是起的线程太多了。 触发fullGC G1中更多的还是mixedGC，但mixedGC可以和youngGC思路一样去排查。触发fullGC了一般都会有问题，G1会退化使用Serial收集器来完成垃圾的清理工作，暂停时长达到秒级别，可以说是半跪了。 fullGC的原因可能包括以下这些，以及参数调整方面的一些思路： 并发阶段失败：在并发标记阶段，MixGC之前老年代就被填满了，那么这时候G1就会放弃标记周期。这种情况，可能就需要增加堆大小，或者调整并发标记线程数-XX:ConcGCThreads。 晋升失败：在GC的时候没有足够的内存供存活/晋升对象使用，所以触发了Full GC。这时候可以通过-XX:G1ReservePercent来增加预留内存百分比，减少-XX:InitiatingHeapOccupancyPercent来提前启动标记，-XX:ConcGCThreads来增加标记线程数也是可以的。 大对象分配失败：大对象找不到合适的region空间进行分配，就会进行fullGC，这种情况下可以增大内存或者增大-XX:G1HeapRegionSize。 程序主动执行System.gc()：不要随便写就对了。 另外，我们可以在启动参数中配置-XX:HeapDumpPath=/xxx/dump.hprof来dump fullGC相关的文件，并通过jinfo来进行gc前后的dump jinfo -flag +HeapDumpBeforeFullGC pid jinfo -flag +HeapDumpAfterFullGC pid 这样得到2份dump文件，对比后主要关注被gc掉的问题对象来定位问题。 网络 涉及到网络层面的问题一般都比较复杂，场景多，定位难，成为了大多数开发的噩梦，应该是最复杂的了。这里会举一些例子，并从tcp层、应用层以及工具的使用等方面进行阐述。 超时 超时错误大部分处在应用层面，所以这块着重理解概念。超时大体可以分为连接超时和读写超时，某些使用连接池的客户端框架还会存在获取连接超时和空闲连接清理超时。 读写超时。readTimeout/writeTimeout，有些框架叫做so_timeout或者socketTimeout，均指的是数据读写超时。注意这边的超时大部分是指逻辑上的超时。soa的超时指的也是读超时。读写超时一般都只针对客户端设置。 连接超时。connectionTimeout，客户端通常指与服务端建立连接的最大时间。服务端这边connectionTimeout就有些五花八门了，jetty中表示空闲连接清理时间，tomcat则表示连接维持的最大时间。 其他。包括连接获取超时connectionAcquireTimeout和空闲连接清理超时idleConnectionTimeout。多用于使用连接池或队列的客户端或服务端框架。 我们在设置各种超时时间中，需要确认的是尽量保持客户端的超时小于服务端的超时，以保证连接正常结束。 在实际开发中，我们关心最多的应该是接口的读写超时了。 如何设置合理的接口超时是一个问题。如果接口超时设置的过长，那么有可能会过多地占用服务端的tcp连接。而如果接口设置的过短，那么接口超时就会非常频繁。 服务端接口明明rt降低，但客户端仍然一直超时又是另一个问题。这个问题其实很简单，客户端到服务端的链路包括网络传输、排队以及服务处理等，每一个环节都可能是耗时的原因。 TCP队列溢出 tcp队列溢出是个相对底层的错误，它可能会造成超时、rst等更表层的错误。因此错误也更隐蔽，所以我们单独说一说。 如上图所示，这里有两个队列：syns queue(半连接队列）、accept queue（全连接队列）。三次握手，在server收到client的syn后，把消息放到syns queue，回复syn+ack给client，server收到client的ack，如果这时accept queue没满，那就从syns queue拿出暂存的信息放入accept queue中，否则按tcp_abort_on_overflow指示的执行。 tcp_abort_on_overflow 0表示如果三次握手第三步的时候accept queue满了那么server扔掉client发过来的ack。tcp_abort_on_overflow 1则表示第三步的时候如果全连接队列满了，server发送一个rst包给client，表示废掉这个握手过程和这个连接，意味着日志里可能会有很多connection reset / connection reset by peer。 那么在实际开发中，我们怎么能快速定位到tcp队列溢出呢？ netstat命令，执行netstat -s | egrep \"listen|LISTEN\" 如上图所示，overflowed表示全连接队列溢出的次数，sockets dropped表示半连接队列溢出的次数。 ss命令，执行ss -lnt 上面看到Send-Q 表示第三列的listen端口上的全连接队列最大为5，第一列Recv-Q为全连接队列当前使用了多少。 接着我们看看怎么设置全连接、半连接队列大小吧： 全连接队列的大小取决于min(backlog, somaxconn)。backlog是在socket创建的时候传入的，somaxconn是一个os级别的系统参数。而半连接队列的大小取决于max(64, /proc/sys/net/ipv4/tcp_max_syn_backlog)。 在日常开发中，我们往往使用servlet容器作为服务端，所以我们有时候也需要关注容器的连接队列大小。在tomcat中backlog叫做acceptCount，在jetty里面则是acceptQueueSize。 RST异常 RST包表示连接重置，用于关闭一些无用的连接，通常表示异常关闭，区别于四次挥手。 在实际开发中，我们往往会看到connection reset / connection reset by peer错误，这种情况就是RST包导致的。 端口不存在 如果像不存在的端口发出建立连接SYN请求，那么服务端发现自己并没有这个端口则会直接返回一个RST报文，用于中断连接。 主动代替FIN终止连接 一般来说，正常的连接关闭都是需要通过FIN报文实现，然而我们也可以用RST报文来代替FIN，表示直接终止连接。实际开发中，可设置SO_LINGER数值来控制，这种往往是故意的，来跳过TIMED_WAIT，提供交互效率，不闲就慎用。 客户端或服务端有一边发生了异常，该方向对端发送RST以告知关闭连接 我们上面讲的tcp队列溢出发送RST包其实也是属于这一种。这种往往是由于某些原因，一方无法再能正常处理请求连接了(比如程序崩了，队列满了)，从而告知另一方关闭连接。 接收到的TCP报文不在已知的TCP连接内 比如，一方机器由于网络实在太差TCP报文失踪了，另一方关闭了该连接，然后过了许久收到了之前失踪的TCP报文，但由于对应的TCP连接已不存在，那么会直接发一个RST包以便开启新的连接。 一方长期未收到另一方的确认报文，在一定时间或重传次数后发出RST报文 这种大多也和网络环境相关了，网络环境差可能会导致更多的RST报文。 之前说过RST报文多会导致程序报错，在一个已关闭的连接上读操作会报connection reset，而在一个已关闭的连接上写操作则会报connection reset by peer。通常我们可能还会看到broken pipe错误，这是管道层面的错误，表示对已关闭的管道进行读写，往往是在收到RST，报出connection reset错后继续读写数据报的错，这个在glibc源码注释中也有介绍。 我们在排查故障时候怎么确定有RST包的存在呢？当然是使用tcpdump命令进行抓包，并使用wireshark进行简单分析了。tcpdump -i en0 tcp -w xxx.cap，en0表示监听的网卡。 接下来我们通过wireshark打开抓到的包，可能就能看到如下图所示，红色的就表示RST包了。 TIME_WAIT和CLOSE_WAIT TIME_WAIT和CLOSE_WAIT是啥意思相信大家都知道。 在线上时，我们可以直接用命令netstat -n | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}'来查看time-wait和close_wait的数量 用ss命令会更快ss -ant | awk '{++S[$1]} END {for(a in S) print a, S[a]}' TIME_WAIT time_wait的存在一是为了丢失的数据包被后面连接复用，二是为了在2MSL的时间范围内正常关闭连接。它的存在其实会大大减少RST包的出现。 过多的time_wait在短连接频繁的场景比较容易出现。这种情况可以在服务端做一些内核参数调优: #表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭 net.ipv4.tcp_tw_reuse = 1 #表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭 net.ipv4.tcp_tw_recycle = 1 当然我们不要忘记在NAT环境下因为时间戳错乱导致数据包被拒绝的坑了，另外的办法就是改小tcp_max_tw_buckets，超过这个数的time_wait都会被干掉，不过这也会导致报time wait bucket table overflow的错。 CLOSE_WAIT close_wait往往都是因为应用程序写的有问题，没有在ACK后再次发起FIN报文。close_wait出现的概率甚至比time_wait要更高，后果也更严重。往往是由于某个地方阻塞住了，没有正常关闭连接，从而渐渐地消耗完所有的线程。 想要定位这类问题，最好是通过jstack来分析线程堆栈来排查问题，具体可参考上述章节。这里仅举一个例子。 开发同学说应用上线后CLOSE_WAIT就一直增多，直到挂掉为止，jstack后找到比较可疑的堆栈是大部分线程都卡在了countdownlatch.await方法，找开发同学了解后得知使用了多线程但是确没有catch异常，修改后发现异常仅仅是最简单的升级sdk后常出现的class not found。 ![](https://img-blog.csdnimg.cn/202009131842051.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzMjU1MDE3,size_16,color_FFFFFF,t_70#pic_center)","link":"/2020/10/14/JAVA%20%E7%BA%BF%E4%B8%8A%E6%95%85%E9%9A%9C%E6%8E%92%E6%9F%A5%E5%AE%8C%E6%95%B4%E5%A5%97%E8%B7%AF%EF%BC%8C%E4%BB%8E%20CPU%E3%80%81%E7%A3%81%E7%9B%98%E3%80%81%E5%86%85%E5%AD%98%E3%80%81%E7%BD%91%E7%BB%9C%E3%80%81GC%20%E4%B8%80%E6%9D%A1%E9%BE%99%EF%BC%81/"},{"title":"","text":"前言日常的开发中，无不都是使用数据库来进行数据的存储，由于一般的系统任务中通常不会存在高并发的情况，所以这样看起来并没有什么问题。 一旦涉及大数据量的需求，如一些商品抢购的情景，或者主页访问量瞬间较大的时候，单一使用数据库来保存数据的系统会因为面向磁盘，磁盘读/写速度问题有严重的性能弊端，详细的磁盘读写原理请参考这一片[]。 在这一瞬间成千上万的请求到来，需要系统在极短的时间内完成成千上万次的读/写操作，这个时候往往不是数据库能够承受的，极其容易造成数据库系统瘫痪，最终导致服务宕机的严重生产问题。 为了克服上述的问题，项目通常会引入NoSQL技术，这是一种基于内存的数据库，并且提供一定的持久化功能。 Redis技术就是NoSQL技术中的一种。Redis缓存的使用，极大的提升了应用程序的性能和效率，特别是数据查询方面。 但同时，它也带来了一些问题。其中，最要害的问题，就是数据的一致性问题，从严格意义上讲，这个问题无解。如果对数据的一致性要求很高，那么就不能使用缓存。 另外的一些典型问题就是，缓存穿透、缓存击穿和缓存雪崩。本篇文章从实际代码操作，来提出解决这三个缓存问题的方案，毕竟Redis的缓存问题是实际面试中高频问点，理论和实操要兼得。 缓存穿透缓存穿透是指查询一条数据库和缓存都没有的一条数据，就会一直查询数据库，对数据库的访问压力就会增大，缓存穿透的解决方案，有以下两种： 缓存空对象：代码维护较简单，但是效果不好。 布隆过滤器：代码维护复杂，效果很好。 缓存空对象缓存空对象是指当一个请求过来缓存中和数据库中都不存在该请求的数据，第一次请求就会跳过缓存进行数据库的访问，并且访问数据库后返回为空，此时也将该空对象进行缓存。 若是再次进行访问该空对象的时候，就会直接击中缓存，而不是再次数据库，缓存空对象实现的原理图如下：缓存空对象的实现代码如下： 1234567891011121314151617181920212223242526272829public class UserServiceImpl { @Autowired UserDAO userDAO; @Autowired RedisCache redisCache; public User findUser(Integer id) { Object object = redisCache.get(Integer.toString(id)); // 缓存中存在，直接返回 if(object != null) { // 检验该对象是否为缓存空对象，是则直接返回null if(object instanceof NullValueResultDO) { return null; } return (User)object; } else { // 缓存中不存在，查询数据库 User user = userDAO.getUser(id); // 存入缓存 if(user != null) { redisCache.put(Integer.toString(id),user); } else { // 将空对象存进缓存 redisCache.put(Integer.toString(id), new NullValueResultDO()); } return user; } } } 缓存空对象的实现代码很简单，但是缓存空对象会带来比较大的问题，就是缓存中会存在很多空对象，占用内存的空间，浪费资源，一个解决的办法就是设置空对象的较短的过期时间，代码如下： 12// 再缓存的时候，添加多一个该空对象的过期时间60秒redisCache.put(Integer.toString(id), new NullValueResultDO(),60); 布隆过滤器布隆过滤器是一种基于概率的数据结构，主要用来判断某个元素是否在集合内，它具有运行速度快（时间效率），占用内存小的优点（空间效率），但是有一定的误识别率和删除困难的问题。它只能告诉你某个元素一定不在集合内或可能在集合内。 在计算机科学中有一种思想：空间换时间，时间换空间。一般两者是不可兼得，而布隆过滤器运行效率和空间大小都兼得，它是怎么做到的呢？ 在布隆过滤器中引用了一个误判率的概念，即它可能会把不属于这个集合的元素认为可能属于这个集合，但是不会把属于这个集合的认为不属于这个集合，布隆过滤器的特点如下： 一个非常大的二进制位数组 （数组里只有0和1） 若干个哈希函数 空间效率和查询效率高 不存在漏报（False Negative）：某个元素在某个集合中，肯定能报出来。 可能存在误报（False Positive）：某个元素不在某个集合中，可能也被爆出来。 不提供删除方法，代码维护困难。 位数组初始化都为0，它不存元素的具体值，当元素经过哈希函数哈希后的值（也就是数组下标）对应的数组位置值改为1。 实际布隆过滤器存储数据和查询数据的原理图如下：可能很多读者看完上面的特点和原理图，还是看不懂，别急下面通过图解一步一步的讲解布隆过滤器，总而言之一句简单的话概括就是布隆过滤器是一个很大二进制的位数组，数组里面只存0和1。 初始化的布隆过滤器的结构图如下：以上只是画了布隆过滤器的很小很小的一部分，实际布隆过滤器是非常大的数组（这里的大是指它的长度大，并不是指它所占的内存空间大）。 那么一个数据是怎么存进布隆过滤器的呢？ 当一个数据进行存入布隆过滤器的时候，会经过如干个哈希函数进行哈希（若是对哈希函数还不懂的请参考这一片[]），得到对应的哈希值作为数组的下标，然后将初始化的位数组对应的下标的值修改为1，结果图如下： 当再次进行存入第二个值的时候，修改后的结果的原理图如下：所以每次存入一个数据，就会哈希函数的计算，计算的结果就会作为下标，在布隆过滤器中有多少个哈希函数就会计算出多少个下标，布隆过滤器插入的流程如下： 将要添加的元素给m个哈希函数 得到对应于位数组上的m个位置 将这m个位置设为1 那么为什么会有误判率呢？ 假设在我们多次存入值后，在布隆过滤器中存在x、y、z这三个值，布隆过滤器的存储结构图如下所示：当我们要查询的时候，比如查询a这个数，实际中a这个数是不存在布隆过滤器中的，经过2哥哈希函数计算后得到a的哈希值分别为2和13，结构原理图如下：经过查询后，发现2和13位置所存储的值都为1，但是2和13的下标分别是x和z经过计算后的下标位置的修改，该布隆过滤器中实际不存在a，那么布隆过滤器就会误判改值可能存在，因为布隆过滤器不存元素值，所以存在误判率。 那么具体布隆过布隆过滤的判断的准确率和一下两个因素有关： 布隆过滤器大小：越大，误判率就越小，所以说布隆过滤器一般长度都是非常大的。 哈希函数的个数：哈希函数的个数越多，那么误判率就越小。 那么为什么不能删除元素呢？ 原因很简单，因为删除元素后，将对应元素的下标设置为零，可能别的元素的下标也引用改下标，这样别的元素的判断就会收到影响，原理图如下：当你删除z元素之后，将对应的下标10和13设置为0，这样导致x和y元素的下标受到影响，导致数据的判断不准确，所以直接不提供删除元素的api。 以上说的都是布隆过滤器的原理，只有理解了原理，在实际的运用才能如鱼得水，下面就来实操代码，手写一个简单的布隆过滤器。 对于要手写一个布隆过滤器，首先要明确布隆过滤器的核心： 若干哈希函数 存值得Api 判断值得Api 实现得代码如下： 1234567891011121314151617181920212223242526272829303132333435363738public class MyBloomFilter { // 布隆过滤器长度 private static final int SIZE = 2 &lt;&lt; 10; // 模拟实现不同的哈希函数 private static final int[] num= new int[] {5, 19, 23, 31,47, 71}; // 初始化位数组 private BitSet bits = new BitSet(SIZE); // 用于存储哈希函数 private MyHash[] function = new MyHash[num.length]; // 初始化哈希函数 public MyBloomFilter() { for (int i = 0; i &lt; num.length; i++) { function [i] = new MyHash(SIZE, num[i]); } } // 存值Api public void add(String value) { // 对存入得值进行哈希计算 for (MyHash f: function) { // 将为数组对应的哈希下标得位置得值改为1 bits.set(f.hash(value), true); } } // 判断是否存在该值得Api public boolean contains(String value) { if (value == null) { return false; } boolean result= true; for (MyHash f : func) { result= result&amp;&amp; bits.get(f.hash(value)); } return result; }} 哈希函数代码如下： 123456789101112131415161718public static class MyHash { private int cap; private int seed; // 初始化数据 public MyHash(int cap, int seed) { this.cap = cap; this.seed = seed; } // 哈希函数 public int hash(String value) { int result = 0; int len = value.length(); for (int i = 0; i &lt; len; i++) { result = seed * result + value.charAt(i); } return (cap - 1) &amp; result; } } 布隆过滤器测试代码如下： 1234567public static void test { String value = &quot;4243212355312&quot;; MyBloomFilter filter = new MyBloomFilter(); System.out.println(filter.contains(value)); filter.add(value); System.out.println(filter.contains(value));} 以上就是手写了一个非常简单得布隆过滤器，但是实际项目中可能事由牛人或者大公司已经帮你写好的，如谷歌的Google Guava，只需要在项目中引入一下依赖： 12345&lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;27.0.1-jre&lt;/version&gt;&lt;/dependency&gt; 实际项目中具体的操作代码如下： 1234567891011121314public static void MyBloomFilterSysConfig { @Autowired OrderMapper orderMapper // 1.创建布隆过滤器 第二个参数为预期数据量10000000，第三个参数为错误率0.00001 BloomFilter&lt;CharSequence&gt; bloomFilter = BloomFilter.create(Funnels.stringFunnel(Charset.forName(&quot;utf-8&quot;)),10000000, 0.00001); // 2.获取所有的订单，并将订单的id放进布隆过滤器里面 List&lt;Order&gt; orderList = orderMapper.findAll() for (Order order;orderList ) { Long id = order.getId(); bloomFilter.put(&quot;&quot; + id); }} 在实际项目中会启动一个系统任务或者定时任务，来初始化布隆过滤器，将热点查询数据的id放进布隆过滤器里面，当用户再次请求的时候，使用布隆过滤器进行判断，改订单的id是否在布隆过滤器中存在，不存在直接返回null，具体操作代码： 12// 判断订单id是否在布隆过滤器中存在bloomFilter.mightContain(&quot;&quot; + id) 布隆过滤器的缺点就是要维持容器中的数据，因为订单数据肯定是频繁变化的，实时的要更新布隆过滤器中的数据为最新。 缓存击穿缓存击穿是指一个key非常热点，在不停的扛着大并发，大并发集中对这一个点进行访问，当这个key在失效的瞬间，持续的大并发就穿破缓存，直接请求数据库，瞬间对数据库的访问压力增大。 缓存击穿这里强调的是并发，造成缓存击穿的原因有以下两个： 该数据没有人查询过 ，第一次就大并发的访问。（冷门数据） 添加到了缓存，reids有设置数据失效的时间 ，这条数据刚好失效，大并发访问（热点数据） 对于缓存击穿的解决方案就是加锁，具体实现的原理图如下：当用户出现大并发访问的时候，在查询缓存的时候和查询数据库的过程加锁，只能第一个进来的请求进行执行，当第一个请求把该数据放进缓存中，接下来的访问就会直接集中缓存，防止了缓存击穿。 业界比价普遍的一种做法，即根据key获取value值为空时，锁上，从数据库中load数据后再释放锁。若其它线程获取锁失败，则等待一段时间后重试。这里要注意，分布式环境中要使用分布式锁，单机的话用普通的锁（synchronized、Lock）就够了。 下面以一个获取商品库存的案例进行代码的演示，单机版的锁实现具体实现的代码如下： 123456789101112131415161718192021// 获取库存数量public String getProduceNum(String key) { try { synchronized (this) { //加锁 // 缓存中取数据，并存入缓存中 int num= Integer.parseInt(redisTemplate.opsForValue().get(key)); if (num&gt; 0) { //没查一次库存-1 redisTemplate.opsForValue().set(key, (num- 1) + &quot;&quot;); System.out.println(&quot;剩余的库存为num：&quot; + (num- 1)); } else { System.out.println(&quot;库存为0&quot;); } } } catch (NumberFormatException e) { e.printStackTrace(); } finally { } return &quot;OK&quot;;} 分布式的锁实现具体实现的代码如下： 1234567891011121314151617181920212223public String getProduceNum(String key) { // 获取分布式锁 RLock lock = redissonClient.getLock(key); try { // 获取库存数 int num= Integer.parseInt(redisTemplate.opsForValue().get(key)); // 上锁 lock.lock(); if (num&gt; 0) { //减少库存，并存入缓存中 redisTemplate.opsForValue().set(key, (num - 1) + &quot;&quot;); System.out.println(&quot;剩余库存为num：&quot; + (num- 1)); } else { System.out.println(&quot;库存已经为0&quot;); } } catch (NumberFormatException e) { e.printStackTrace(); } finally { //解锁 lock.unlock(); } return &quot;OK&quot;;} 缓存雪崩缓存雪崩 是指在某一个时间段，缓存集中过期失效。此刻无数的请求直接绕开缓存，直接请求数据库。 造成缓存雪崩的原因，有以下两种： reids宕机 大部分数据失效 比如天猫双11，马上就要到双11零点，很快就会迎来一波抢购，这波商品在23点集中的放入了缓存，假设缓存一个小时，那么到了凌晨24点的时候，这批商品的缓存就都过期了。 而对这批商品的访问查询，都落到了数据库上，对于数据库而言，就会产生周期性的压力波峰，对数据库造成压力，甚至压垮数据库。 缓存雪崩的原理图如下，当正常的情况下，key没有大量失效的用户访问原理图如下：当某一时间点，key大量失效，造成的缓存雪崩的原理图如下：对于缓存雪崩的解决方案有以下两种： 搭建高可用的集群，防止单机的redis宕机。 设置不同的过期时间，防止同意之间内大量的key失效。 针对业务系统，永远都是具体情况具体分析，没有最好，只有最合适。于缓存其它问题，缓存满了和数据丢失等问题，我们后面继续深入的学习。最后也提一下三个词LRU、RDB、AOF，通常我们采用LRU策略处理溢出，Redis的RDB和AOF持久化策略来保证一定情况下的数据安全。","link":"/2020/10/14/%E3%80%90%E8%BF%9B%E5%A4%A7%E5%8E%82%E7%B3%BB%E5%88%97%E3%80%91Redis%E7%BC%93%E5%AD%98%E4%B8%89%E5%A4%A7%E9%97%AE%E9%A2%98/"},{"title":"","text":"TCP/IP概念 TCP/IP（Transmission Control Protocol/Internet Protocol，传输控制协议/网际协议）是指能够在多个不同网络间实现信息传输的协议簇。TCP/IP协议不仅仅指的是TCP 和IP两个协议，而是指一个由FTP、SMTP、TCP、UDP、IP等协议构成的协议簇，同时是Internet最基本的协议、Internet国际互联网络的基础，由网络层的IP协议和传输层的TCP协议组成。 TCP/IP 定义了电子设备如何连入因特网，以及数据如何在它们之间传输的标准。 我的理解： 互联网中的设备要相互通信，必须基于相同的方式，比如由哪一方发起通讯，使用什么语言进行通讯，怎么结束通讯这些都要事先确定，不同设备之间的通讯都需要一种规则，我们将这种规则成为协议。 TCP/IP 的分层管理图TCP/IP协议中最重要的特点就是分层。由上往下分别为 应用层，传输层，网络层，数据链路层，物理层。当然也有按不同的模型分为4层或者7层的。 为什么要分层呢？在设计的角度来讲变得灵活了，当某一层需要修改时，只需要拿掉对相应的层，实现可拔插，无需变动所有层。对于使用者来讲，屏蔽了底层复杂的传输过程。 应用层TCP/IP模型将OSI参考模型中的会话层和表示层的功能合并到应用层实现。这一层主要的代表有DNS域名解析/http协议 传输层在TCP/IP模型中，传输层的功能是使源端主机和目标端主机上的对等实体可以进行会话。在传输层定义了两种服务质量不同的协议。即：传输控制协议TCP和用户数据报协议UDP. 网络层网络互连层是整个TCP/IP协议栈的核心。它的功能是把分组发往目标网络或主机。同时，为了尽快地发送分组，可能需要沿不同的路径同时进行分组传递。因此，分组到达的顺序和发送的顺序可能不同，这就需要上层必须对分组进行排序。网络互连层定义了分组格式和协议，即IP协议（Internet Protocol ）。 物理层该层负责 比特流在节点之间的传输，即负责物理传输，这一层的协议既与链路有关，也与传输的介质有关。通俗来说就是把计算机连接起来的物理手段。 数据链路层控制网络层与物理层之间的通信，主要功能是保证物理线路上进行可靠的数据传递。为了保证传输，从网络层接收到的数据被分割成特定的可被物理层传输的帧。帧是用来移动数据结构的结构包，他不仅包含原始数据，还包含发送方和接收方的物理地址以及纠错和控制信息。其中的地址确定了帧将发送到何处，而纠错和控制信息则确保帧无差错到达。如果在传达数据时，接收点检测到所传数据中有差错，就要通知发送方重发这一帧。 UDP 和 TCP 的特点: 用户数据报协议 UDP（User Datagram Protocol）:无连接；尽最大努力的交付；面向报文；无拥塞控制；支持一对一、一对多、多对一、多对多的交互通信；首部开销小(只有四个字段：源端口、目的端口、长度、检验和)。UDP是面向报文的传输方式是应用层交给UDP多长的报文，UDP发送多长的报文，即一次发送一个报文。因此，应用程序必须选择合适大小的报文。 传输控制协议 TCP（Transmission Control Protocol）:面向连接；每一个TCP连接只能是点对点的(一对一)；提供可靠交付服务；提供全双工通信；面向字节流。应用程序和TCP的交互是一次一个数据块(大小不等)，但TCP把应用程序看成是一连串的无结构的字节流。TCP有一个缓冲，当应该程序传送的数据块太长，TCP就可以把它划分短一些再传送。 UDP的首部格式:用户数据报有两个字段：数据字段和首部字段，数据字段很简单，只有8个字节，由四个字段组成，每个字段的长度都是两个字节。各字段意义如下： 源端口： 源端口号，在需要给对方回信时使用。不需要是可全用0. 目的端口号： 这在终点交付报文时必须使用。 长度： 用户数据报UDP的长度，最小为8（仅首部）。 校验和： 用于校验用户数据报在传输过程是否出错，出错则丢弃该报文。 TCP报文首部格式:源端口和目的端口: 各占两个字节，分别写入源端口号和目的端口号。序号 ： 占4个字节；用于对字节流进行编号，例如序号为 301，表示第一个字节的编号为 301，如果携带的数据长度为 100 字节，那么下一个报文段的序号应为 401。确认号 ： 占4个字节；期望收到的下一个报文段的序号。例如 B 正确收到 A 发送来的一个报文段，序号为 501，携带的数据长度为 200 字节，因此 B 期望下一个报文段的序号为 701，B 发送给 A 的确认报文段中确认号就为 701。数据偏移 ： 占4位；指的是数据部分距离报文段起始处的偏移量，实际上指的是首部的长度。确认 ACK ： 当 ACK=1 时确认号字段有效，否则无效。TCP 规定，在连接建立后所有传送的报文段都必须把 ACK 置 1。同步 SYN ：在连接建立时用来同步序号。当 SYN=1，ACK=0 时表示这是一个连接请求报文段。若对方同意建立连接，则响应报文中 SYN=1，ACK=1。终止 FIN ： 用来释放一个连接，当 FIN=1 时，表示此报文段的发送方的数据已发送完毕，并要求释放连接。窗口 ： 占2字节；窗口值作为接收方让发送方设置其发送窗口的依据。之所以要有这个限制，是因为接收方的数据缓存空间是有限的。检验和： 占2个字节；检验和字段检验的范围包括首部和数据这两个部分。在计算检验和时，在TCP报文段的前面加上12字节的伪首部。套接字： TCP连接的端点叫做套接字或插口。端口号拼接到IP地址即构成了套接字。 面试灵魂拷问TCP的三次握手与四次挥手 第一次握手：Client将标志位SYN置为1，随机产生一个值seq=J，并将该数据包发送给Server，Client进入SYN_SENT状态，等待Server确认。 第二次握手：Server收到数据包后由标志位SYN=1知道Client请求建立连接，Server将标志位SYN和ACK都置为1，ack=J+1，随机产生一个值seq=K，并将该数据包发送给Client以确认连接请求，Server进入SYN_RCVD状态。 第三次握手：Client收到确认后，检查ack是否为J+1，ACK是否为1，如果正确则将标志位ACK置为1，ack=K+1，并将该数据包发送给Server，Server检查ack是否为K+1，ACK是否为1，如果正确则连接建立成功，Client和Server进入ESTABLISHED状态，完成三次握手，随后Client与Server之间可以开始传输数据了。 为什么要进行三次握手呢?第三次握手是为了防止失效的连接请求到达服器，让服务器错误打开连接。客户端发送的连接请求如果在网络中滞留，那么就会隔很长一段时间才能收到服务器端发回的连接确认。客户端等待一个超时重传时间之后，就会重新请求连接。但是这个滞留的连接请求最后还是会到达服务器，如果不进行三次握手，那么服务器就会打开两个连接。如果有第三次握手，客户端会忽略服务器之后发送的对滞留连接请求的连接确认，不进行第三次握手，因此就不会再次打开连接。 如果此时变成两次挥手行不行？举个打电话的例子，比如：第一次握手： A给B打电话说，你可以听到我说话吗？第二次握手： B收到了A的信息，然后对A说： 我可以听得到你说话啊，你能听得到我说话吗？第三次握手： A收到了B的信息，然后说可以的，我要给你发信息啦！结论：在三次握手之后，A和B都能确定这么一件事：我能听到你，你也能听到我。这样，就可以开始正常通信了。如果是两次，那将无法确定。 当数据传送完毕,断开连接就需要进行TCP的四次挥手： 第一次挥手，客户端设置seq和 ACK ,向服务器发送一个 FIN(终结)报文段。此时，客户端进入 FIN_WAIT_1状态，表示客户端没有数据要发送给服务端了。 第二次挥手，服务端收到了客户端发送的 FIN 报文段，向客户端回了一个 ACK 报文段。 第三次挥手，服务端向客户端发送FIN 报文段，请求关闭连接，同时服务端进入 LAST_ACK 状态。 第四次挥手，客户端收到服务端发送的 FIN 报文段后，向服务端发送 ACK 报文段,然后客户端进入 TIME_WAIT状态。服务端收到客户端的 ACK 报文段以后，就关闭连接。此时，客户端等待2MSL（指一个片段在网络中最大的存活时间）后依然没有收到回复，则说明服务端已经正常关闭，这样客户端就可以关闭连接了。最后完整的过程 为什么要四次挥手？客户端发送了 FIN 连接释放报文之后，服务器收到了这个报文，就进入了 CLOSE-WAIT 状态。这个状态是为了让服务器端发送还未传送完毕的数据，传送完毕之后，服务器会发送 FIN 连接释放报文。 HTTP持久连接 如果有大量的连接，每次在连接，关闭都要经历三次握手，四次挥手，这显然会造成性能低下。因此。Http 有一种叫做 长连接（keepalive connections） 的机制。它可以在传输数据后仍保持连接，当客户端需要再次获取数据时，直接使用刚刚空闲下来的连接而无需再次握手。 HTTP和HTTPS什么是HTTP? 超文本传输协议，是一个基于请求与响应，无状态的，应用层的协议，常基于TCP/IP协议传输数据，互联网上应用最为广泛的一种网络协议,所有的WWW文件都必须遵守这个标准。设计HTTP的初衷是为了提供一种发布和接收HTML页面的方法。 HTTP特点： 无状态：协议对客户端没有状态存储，对事物处理没有“记忆”能力，比如访问一个网站需要反复进行登录操作。 无连接：HTTP/1.1之前，由于无状态特点，每次请求需要通过TCP三次握手四次挥手，和服务器重新建立连接。比如某个客户机在短时间多次请求同一个资源，服务器并不能区别是否已经响应过用户的请求，所以每次需要重新响应请求，需要耗费不必要的时间和流量。 基于请求和响应：基本的特性，由客户端发起请求，服务端响应。 简单快速、灵活。 通信使用明文、请求和响应不会对通信方进行确认、无法保护数据的完整性。 HTTP报文组成: 请求行：包括请求方法、URL、协议/版本 请求头(Request Header) 请求正文 状态行 响应头 响应正文 HTTP的缺点 通信使用明文（不加密），内容可能会被窃听。 不验证通信方的身份，因此有可能遭遇伪装。 无法证明报文的完整性，所以有可能已遭篡改。 什么是HTTPS? HTTPS：是以安全为目标的HTTP通道，简单讲是HTTP的安全版，即HTTP下加入SSL层，HTTPS的安全基础是SSL，因此加密的详细内容就需要SSL。HTTPS协议的主要作用可以分为两种：一种是建立一个信息安全通道，来保证数据传输的安全；另一种就是确认网站的真实性。 HTTPS 并非是应用层的一种新协议。只是 HTTP 通信接口部分用SSL（Secure Socket Layer）和 TLS（Transport Layer Security）协议代替而已。通常，HTTP 直接和 TCP 通信。当使用 SSL时，则演变成先和 SSL通信，再由 SSL和 TCP 通信了。简言之，所谓 HTTPS，其实就是身披SSL协议这层外壳的 HTTP。 HTTPS通讯方式： 客户使用https的URL访问Web服务器，要求与Web服务器建立SSL连接。 Web服务器收到客户端请求后，会将网站的证书信息（证书中包含公钥）传送一份给客户端。 客户端的浏览器与Web服务器开始协商SSL连接的安全等级，也就是信息加密的等级。 客户端的浏览器根据双方同意的安全等级，建立会话密钥，然后利用网站的公钥将会话密钥加密，并传送给网站。 Web服务器利用自己的私钥解密出会话密钥。 Web服务器利用会话密钥加密与客户端之间的通信。 为什么HTTPS安全 SSL不仅提供加密处理，加密方式为混合加密。 SSL而且还使用了一种被称为证书的手段，可用于确定方。证书由值得信任的第三方机构颁发，用以证明服务器和客户端是实际存在的。另外，伪造证书从技术角度来说是异常困难的一件事。所以只要能够确认通信方（服务器或客户端）持有的证书。 加密方法 对称加密：加密和解密同用一个密钥的方式称为共享密钥加密（Common keycrypto system），也被叫做对称密钥加密. 对成加密的方式效率比较低，加密速度慢。另外对称加密存在安全隐患的问题，堆成加密的密钥必须要传到对方对方才能解密，要是对方在密钥传输的过程获取到密钥，那不是密钥失去了加密的意义，所以完全使用对称加密也是不安全的。 非对称加密：公开密钥加密使用一对非对称的密钥。一把叫做私有密钥（private key），另一把叫做公开密钥（public key）。顾名思义，私有密钥不能让其他任何人知道，而公开密钥则可以随意发布，任何人都可以获得。公钥加密，私钥解密使用公开密钥加密方式，发送密文的一方使用对方的公开密钥进行加密处理，对方收到被加密的信息后，再使用自己的私有密钥进行解密。 那么非对称个加密就一定安全吗？非对称加密也不安全，为什么呢？因为存在中间伪造公钥和私钥，假如在公钥传给对方的时候，有人获取到公钥，虽然她不能用你的公钥做什么，但是它截获公钥后，把自己伪造的公钥发送给对方，这样对方获取的就不是真正的公钥，当对方用公钥进行加密文件，再将文件发送给对方，这样即使截获人没有获取到真正的私钥，但是加密时的公钥是截获人的，他获取到加密文件，只需要用自己的私钥进行解密就成功获取到文件了。 混合加密机制（对称加密与非对称加密结合的方式）顾名思义也就是对称加密和非对称加密的方式相结合。 如何证明公开没要本身的真实性。因为在公开秘钥传输的过程中，可能真正的公开秘钥已经被攻击者替换掉了。 为了解决上述问题，于是除了CA认证证书。服务器将CA证书发送给客户端，以进行公开密钥加密方式通信。接到证书的客户端可使用数字证书认证机构的公开密钥，对那张证书上的数字签名进行验证，一旦验证通过，客户端便可明确两件事：一，认证服务器的公开密钥的是真实有效的数字证书认证机构。二，服务器的公开密钥是值得信赖的。 那么公开密钥如何交接给客户端是一件非常重要的事，因此多数浏览器开发商发布版本时，会事先在内部植入常用认证机关的公开密钥，这样就确保公钥是使用认证机构的公钥避免了公钥伪造的过程，进而确保了安全。","link":"/2020/10/14/%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82TCP-IP%E5%92%8CHTTP%E3%80%81HTTPS/"},{"title":"","text":"前言面试官：小伙子，你还记得我吗？我是上次面试你的那个面试官。 我心想：我去，怎么会不记得，我又不是青年痴呆，上次害我画了那么多图，还使劲敲了一个多钟的电脑，满脑子都是你的阴影。 我：记得记得，您好，很高兴能通过二面，能够继续和您交流技术问题。 我违背良心说这话真的好吗，姑且就那么一次吧，面个试都那么难？ 面试官又快速的扫了一下的简历，可能上次看过一次，都快过了一个多星期了，都忘了吧。 面试官：我看你简历上面写着深入了解分布式，并且也做过分布式项目，挺好的，那你知道分布式项目中生成分布式ID的方法有哪些吗？ 我：这个我知道，生成分布式Id的方法主要有以下几种： 数据库自增ID。 数据库水平拆分，设置初始值和相同的自增步长。 批量申请自增ID。 UUID生成。 Redis的方式。 雪花算法。 百度UidGenerator算法 美团Leaf算法 面试官：哦，不错能说出那么多，你能说一说对于上面的每一种方式的分析和理解吗？ 我心想：我去，这下可糗大了，那么多，我只是大概知道主要的，怎么可能每一种都去了解和深入，一下子说了那么多不是给自己挖坑吗？ 哎，没办法出来混，总是要还的，只能说自己知道的吧？不知道的大概粗糙的略过。 数据库的自增我：嗯嗯，好的。数据库的自增，很容易理解，开发过的人员都知道，在创建表的时候，指定主键auto_increment（自增）便可以实现。 我：但是使用数据库的自增ID，虽然简单，会带来ID重复的问题，并且单机版的ID自增，并且每次生成一个ID都会访问数据库一次，DB的压力也很大，并没有什么并发性能可言。 面试官：恩额。 我看看面试官正听着有味，时不时摸摸他稀少的发量额头，深邃的目光透露出他的沉稳，这可能就是一个成熟架构师的魅力吧，让多少码渣苦读《Java编程思想》《Java核心技术》《Effectice java》《Java并发编程实战》《代码整洁之道》《重构: 改善既有代码的设计》……，都无法达到的境界，我乘热打铁，接着下面的回答。 我：针对上面的数据库自增ID出现的问题：ID重复、性能不好。就出现了集群版的生成分布式ID方案。数据库水平拆分，设置初始值和相同的自增步长和批量申请自增ID。 数据库水平拆分，设置初始值和相同的自增步长我：数据库水平拆分，设置初始值和相同的自增步长是指在DB集群的环境下，将数据库进行水平划分，然后每个数据库设置不同的初始值和相同的步长，这样就能避免ID重复的情况。 面试官：小伙子，不好意思打断一下，你可以画个图吗，这个我有点没明白你讲的意思？ 我能有什么办法阿，完全没办法，只能从裤兜里拿出笔和纸，快速的画了一张图。 我：我这里假设有三个数据库，为每一个数据库设置初始值，设置初始值可以通过下面的sql进行设置： 12set @@auto_increment_offset = 1; // 设置初始值set @@auto_increment_increment = 2; // 设置步长 我：三个数据的初始值分别设置为1、2、3，一般步长设置为数据库的数据，这里数据库数量为3，所以步长也设置为3。 面试官：若是面对再次扩容的情况呢？ 我：恩额，扩容的情况是这种方法的一个缺点，上面我说的步长一般设置为数据库的数量，这是在确保后期不会扩容的情况下，若是确定后期会有扩容情况，在前期设计的的时候可以将步长设置长一点，预留一些初始值给后续扩容使用。 我：总之，这种方案还是优缺点的，但是也有自己的优点，缺点就是：后期可能会面对无ID初始值可分的窘境，数据库总归是数据库，抗高并发也是有限的。 我：它的优点就是算是解决了DB单点的问题。 面试官：恩额。 批量申请自增ID我：批量申请自增ID的解决方案可以解决无ID可分的问题，它的原理就是一次性给对应的数据库上分配一批的id值进行消费，使用完了，再回来申请。 这次我很自觉的从裤兜里拿出笔和纸，画出了下面的这张图，历史总是那么惊人的相似。我：在设计的初始阶段可以设计一个有初始值字段，并有步长字段的表，当每次要申请批量ID的时候，就可以去该表中申请，每次申请后初始值=上一次的初始值+步长。 我：这样就能保持初始值是每一个申请的ID的最大值，避免了ID的重复，并且每次都会有ID使用，一次就会生成一批的id来使用，这样访问数据库的次数大大减少。 我：但是这一种方案依旧有自己的缺点，依然不能抗真正意义上的高并发。 UUID生成我：第四种方式是使用UUID生成的方式生成分布式ID，UUID的核心思想是使用机器的网卡、当地时间、一个随机数来生成UUID。 我：使用UUID的方式只需要调用UUID.randomUUID().toString()就可以生成，这种方式方便简单，本地生成，不会消耗网络。 我：当时简单的东西，出现的问题就会越多，不利于存储，16字节128位，通常是以36位长度的字符串表示，很多的场景都不适合。 我：并且UUID生成的无序的字符串，查询效率低下，没有实际的业务含义，不具备自增特性，所以都不会使用UUID作为分布式ID来使用。 面试官：恩额，那你知道生成UUID的方式有几种吗？不知道没关系，这个只是作为一个扩展。 我：这个我只知道可以通过当前的时间戳及机器mac地址来生成，可以确保生成的UUID全球唯一，其它的没有了解过。 面试官：嗯嗯，没关系的。 Redis的方式我：为了解决上面纯关系型数据库生成分布式ID无法抗高并发的问题，可以使用Redis的方式来生成分布式ID。 我：Redis本身有incr和increby 这样自增的命令，保证原子性，生成的ID也是有序的。 我：Redis基于内存操作，性能高效，不依赖于数据库，数据天然有序，利于分页和排序。 我：但是这个方案也会有自己的缺点，因为增加了中间件，需要自己编码实现工作量增大，增加复杂度。 我：使用Redis的方式还要考虑持久化，Redis的持久化有两种RDB和AOF，RDB是以快照的形式进行持久化，会丢失上一次快照至此时间的数据。 我：AOF可以设置一秒持久化一次，丢失的数据是秒内的，也会存在可能上一次自增后的秒内的ID没有持久化的问题。 我：但是这种方法相对于上面的关系型数据库生成分布式ID的方法而言，已经优越了许多。 我：若是数据量比较大的话，重启Redis的时间也会比较长，可以采用Redis的集群方式。 面试官：你能手写一下Redis的生成分布式ID的工具类代码吗？ 我奔溃了，我最怕手写了，因为工具类这种东西，基本就是项目开始的时候写一次，后面对后市重复使用，记不住，还要手写，这也太难为我怕虎了吧。 我：手写应该不行，因为有些API记不住，工具类基本就是项目开始的时候写一些，后续都没有去看过了，没有专门去记它。 我：我可以使用您的电脑吗？使用电脑应该可以敲出这些工具类。 面试官：可以的，这边电脑给你，你在这个测试项目下吧。 我：好的，谢谢。 时间流逝中…….. 大概敲了几分钟，废了九牛二虎之力，终于敲出来了，有好多API记不住，只能慢慢的找了，写了主要两种方式来生成分布式ID。 第一种是使用RedisAtomicLong 原子类使用CAS操作来生成ID。 123456789101112131415161718192021222324252627282930313233343536373839@Servicepublic class RedisSequenceFactory { @Autowired RedisTemplate&lt;String, String&gt; redisTemplate; public void setSeq(String key, int value, Date expireTime) { RedisAtomicLong counter = new RedisAtomicLong(key, redisTemplate.getConnectionFactory()); counter.set(value); counter.expireAt(expireTime); } public void setSeq(String key, int value, long timeout, TimeUnit unit) { RedisAtomicLong counter = new RedisAtomicLong(key, redisTemplate.getConnectionFactory()); counter.set(value); counter.expire(timeout, unit); } public long generate(String key) { RedisAtomicLong counter = new RedisAtomicLong(key, redisTemplate.getConnectionFactory()); return counter.incrementAndGet(); } public long incr(String key, Date expireTime) { RedisAtomicLong counter = new RedisAtomicLong(key, redisTemplate.getConnectionFactory()); counter.expireAt(expireTime); return counter.incrementAndGet(); } public long incr(String key, int increment) { RedisAtomicLong counter = new RedisAtomicLong(key, redisTemplate.getConnectionFactory()); return counter.addAndGet(increment); } public long incr(String key, int increment, Date expireTime) { RedisAtomicLong counter = new RedisAtomicLong(key, redisTemplate.getConnectionFactory()); counter.expireAt(expireTime); return counter.addAndGet(increment); }} 第二种是使用redisTemplate.opsForHash()和结合UUID的方式来生成生成ID。 123456789101112131415public Long getSeq(String key,String hashKey,Long delta) throws BusinessException{ try { if (null == delta) { delta=1L; } return redisTemplate.opsForHash().increment(key, hashKey, delta); } catch (Exception e) { // 若是redis宕机就采用uuid的方式 int first = new Random(10).nextInt(8) + 1; int randNo=UUID.randomUUID().toString().hashCode(); if (randNo &lt; 0) { randNo=-randNo; } return Long.valueOf(first + String.format(&quot;%16d&quot;, randNo)); } } 我把电脑移回给面试官，他很快的扫了一下我的代码，说了一句。 面试官：小伙子，不写注释哦，这个习惯不好哦。 我：哦哦，谢谢提醒，不好意思，下次我会注意的。 雪花算法我：第六种方式是雪花算法，也是现在市面上比较流行的生成分布式ID的方法。 说着说着，我知道画图又是必不可少的了，于是在桌子上有画了起来，面试官好奇的看看我，知道了我在干啥，又耐心的等了等。 我：他是采用64bit作为id生成类型，并且将64bit划分为，如下图的几段。 我顺手把我画的图递给他看了看，接着对着这个图进行解释。我：第一位作为标识位，因为Java中long类型的时代符号的，因为ID位正数，所以第一位位0。 我：接着的41bit是时间戳，毫秒级位单位，注意这里的时间戳并不是指当前时间的时间戳，而是值之间差（当前时间-开始时间）。 我：这里的开始时间一般是指ID生成器的开始时间，是由我们程序自己指定的。 我：接着后面的10bit：包括5位的数据中心标识ID（datacenterId）和5位的机器标识ID（workerId），可以最多标识1024个节点（1&lt;&lt;10=1024）。 我：最的12位是序列号，12位的计数顺序支持每个节点每毫秒差生4096序列号（1&lt;&lt;12=4096）。 我：雪花算法使用数据中心ID和机器ID作为标识，不会产生ID的重复，并且是在本地生成，不会消耗网络，效率高，有数据显示，每秒能生成26万个ID。 我：但是雪花算法也是又自己的缺点，因为雪花算法的计算依赖于时间，若是系统时间回拨，就会产生重复ID的情况。 面试官：那对于时间回拨产生重复ID的情况，你有什么比较好的解决方案吗？ 我：在雪花算法的实现中，若是其前置的时间等于当前的时间，就抛出异常，也可以关闭掉时间回拨。 我：对于回拨时间比较短的，可以等待回拨时间过后再生成ID。 面试官：你可以帮我敲一个雪花算法吗？我这键盘给你。 我：。。。 我：好的。 时间流逝中…… 过了几分钟时间，也总算是把雪花算法给敲出来了，真正要老命，面个试怎么就那么难呢？ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120/** * 雪花算法 * @author：黎杜 */public class SnowflakeIdWorker { /** 开始时间截 */ private final long twepoch = 1530051700000L; /** 机器id的位数 */ private final long workerIdBits = 5L; /** 数据标识id的位数 */ private final long datacenterIdBits = 5L; /** 最大的机器id，结果是31 */ private final long maxWorkerId = -1L ^ (-1L &lt;&lt; workerIdBits); /** 最大的数据标识id，结果是31 */ private final long maxDatacenterId = -1L ^ (-1L &lt;&lt; datacenterIdBits); /** 序列的位数 */ private final long sequenceBits = 12L; /** 机器ID向左移12位 */ private final long workerIdShift = sequenceBits; /** 数据标识id向左移17位 */ private final long datacenterIdShift = sequenceBits + workerIdBits; /** 时间截向左移22位*/ private final long timestampLeftShift = sequenceBits + workerIdBits + datacenterIdBits; /** 生成序列的掩码 */ private final long sequenceMask = -1L ^ (-1L &lt;&lt; sequenceBits); /** 工作机器ID(0~31) */ private long workerId; /** 数据中心ID(0~31) */ private long datacenterId; /** 毫秒内序列(0~4095) */ private long sequence = 0L; /** 上次生成ID的时间截 */ private long lastTimestamp = -1L; /** * 构造函数 * @param workerId 工作ID (0~31) * @param datacenterId 数据中心ID (0~31) */ public SnowflakeIdWorker(long workerId, long datacenterId) { if (workerId &gt; maxWorkerId || workerId &lt; 0) { throw new IllegalArgumentException(String.format(&quot;worker Id can't be greater than %d or less than 0&quot;, maxWorkerId)); } if (datacenterId &gt; maxDatacenterId || datacenterId &lt; 0) { throw new IllegalArgumentException(String.format(&quot;datacenter Id can't be greater than %d or less than 0&quot;, maxDatacenterId)); } this.workerId = workerId; this.datacenterId = datacenterId; } /** * 获得下一个ID (该方法是线程安全的) * @return SnowflakeId */ public synchronized long nextId() { long timestamp = getCurrentTime(); //如果当前时间小于上一次生成的时间戳，说明系统时钟回退过就抛出异常 if (timestamp &lt; lastTimestamp) { throw new BusinessionException(&quot;回拨的时间为：&quot;+lastTimestamp - timestamp); } //如果是同一时间生成的，则进行毫秒内序列 if (lastTimestamp == timestamp) { sequence = (sequence + 1) &amp; sequenceMask; //毫秒内序列溢出 if (sequence == 0) { //获得新的时间戳 timestamp = tilNextMillis(lastTimestamp); } } else { //时间戳改变，毫秒内序列重置 sequence = 0L; } //上次生成ID的时间截 lastTimestamp = timestamp; //移位并通过或运算拼到一起组成64位的ID return ((timestamp - twepoch) &lt;&lt; timestampLeftShift) // 计算时间戳 | (datacenterId &lt;&lt; datacenterIdShift) // 计算数据中心 | (workerId &lt;&lt; workerIdShift) // 计算机器ID | sequence; // 序列号 } /** *获得新的时间戳 * @param lastTimestamp 上次生成ID的时间截 * @return 当前时间戳 */ protected long tilNextMillis(long lastTimestamp) { long timestamp = getCurrentTime(); // 若是当前时间等于上一次的1时间就一直阻塞，知道获取到最新的时间（回拨后的时间） while (timestamp &lt;= lastTimestamp) { timestamp = getCurrentTime(); } return timestamp; } /** * 获取当前时间 * @return 当前时间(毫秒) */ protected long getCurrentTime() { return System.currentTimeMillis(); } 为了给面试官留下个好印象，这下也写上了注解，免得他又说我，敲完我又把电脑移回给他，他快速的看了看，点了点头，嘴角露出思思的笑意。 面试官：嗯，你的底子还算比价扎实，面试之前早有准备吧，看了很多的面试资料。 我心想怎么是面试之前准备呢？我是一直再准备，从工作到现在都在总结自己的知识点，形成自己的知识体系，为了迎合他，也只能说是。 我：嗯嗯，是的，准备了很久，算是比较充分。 面试官：嗯，最后的两种算法，你还深入了解吗？ UidGenerator和Leaf我：最后两种确实没有深入了解，之前有看网上的资料说美团Leaf算法需要依赖于数据库，ZK，并且也能保证去全局ID的唯一性，单项递增。 我：而百度UidGenerator算法是基于雪花算法进行实现的，也是需要借助于数据库，与雪花算法不同的是，UidGenerator支持自定义时间戳、主句中心ID和机器ID、序列号的位数。 面试官：嗯嗯，好的，小伙子今天的面试就到这里，下次我们再见吧。 得意洋洋中……","link":"/2020/10/14/%E5%88%86%E5%B8%83%E5%BC%8Fid%E7%94%9F%E6%88%90%E7%AD%96%E7%95%A5%EF%BC%8C%E6%88%91%E5%92%8C%E9%9D%A2%E8%AF%95%E5%AE%98%E6%89%AF%E4%BA%86%E4%B8%80%E4%B8%AA%E5%8D%8A%E5%B0%8F%E6%97%B6/"},{"title":"","text":"前言迎面走来了一个风尘仆仆的身穿格子衫的男子，手里拿着一个MacBook Pro，看着那稀少的发量，和那从容淡定的眼神。 我心里一颤，我去，这是架构师，架构师来面我技术面，我心里顿时不淡定了，表面很稳实则心里慌得一批。 果然，他手里拿着我的简历，快速的扫了一下，然后用眼角余光看了一下我，上来就开问。 Mysql事务简介面试官： 看你简历上说精通Mysql优化方法，你先来说说你对Mysql的事务的了解吧。 我心里喜了一下，这个简单啊，哥我可是北大(背大)的，再来面试之前，早就有准备的，二话不说，上去就是背。 我： 好的，数据库的事务是指一组sql语句组成的数据库逻辑处理单元，在这组的sql操作中，要么全部执行成功，要么全部执行失败。 我： 这里的一组sql操作，举个简单又经典的例子就是转账了，事务A中要进行转账，那么转出的账号要扣钱，转入的账号要加钱，这两个操作都必须同时执行成功，为了确保数据的一致性。 面试官： 刚才你提到了数据一致性，你知道事务的特性吗？说说你的理解。 ACID简介我： 在Mysql中事务的四大特性主要包含：原子性（Atomicity）、一致性（Consistent）、隔离性（Isalotion）、**持久性(Durable)**，简称为ACID。 我： 原子性是指事务的原子性操作，对数据的修改要么全部执行成功，要么全部失败，实现事务的原子性，是基于日志的Redo/Undo机制。 我： 一致性是指执行事务前后的状态要一致，可以理解为数据一致性。隔离性侧重指事务之间相互隔离，不受影响，这个与事务设置的隔离级别有密切的关系。 我： 持久性则是指在一个事务提交后，这个事务的状态会被持久化到数据库中，也就是事务提交，对数据的新增、更新将会持久化到书库中。 我： 在我的理解中，原子性、隔离性、持久性都是为了保障一致性而存在的，一致性也是最终的目的。 心里暗自欢喜，背完了，平时背的多，面试就会说，幸好难不倒我。 ACID原理面试官： 刚才你说原子性是基于日志的Redo/Undo机制，你能说一说Redo/Undo机制吗？ 啊哈？我都说了什么，不小心给自己埋了一颗大雷。不慌，哥脑子里还有货，假装若有所思的停了几十秒，接着背。 我： Redo/Undo机制比较简单，它们将所有对数据的更新操作都写到日志中。 我： Redo log用来记录某数据块被修改后的值，可以用来恢复未写入 data file 的已成功事务更新的数据；Undo log是用来记录数据更新前的值，保证数据更新失败能够回滚。 我： 假如数据库在执行的过程中，不小心崩了，可以通过该日志的方式，回滚之前已经执行成功的操作，实现事务的一致性。 面试官： 可以举一个场景，说一下具体的实现流程吗？ 我： 可以的，假如某个时刻数据库崩溃，在崩溃之前有事务A和事务B在执行，事务A已经提交，而事务B还未提交。当数据库重启进行 crash-recovery 时，就会通过Redo log将已经提交事务的更改写到数据文件，而还没有提交的就通过Undo log进行roll back。 事务隔离级别面试官： 之前你还提到事务的隔离级别，你能说一说吗？ 我： 可以的，在Mysql中事务的隔离级别分为四大等级，读未提交（READ UNCOMMITTED）、读提交 （READ COMMITTED）、可重复读 （REPEATABLE READ）、串行化 （SERIALIZABLE）。 我： 读未提交会读到另一个事务的未提交的数据，产生脏读问题，读提交则解决了脏读的，出现了不可重复读，即在一个事务任意时刻读到的数据可能不一样，可能会受到其它事务对数据修改提交后的影响，一般是对于update的操作。 我： 可重复读解决了之前不可重复读和脏读的问题，但是由带来了幻读的问题，幻读一般是针对inser操作。 我： 例如：第一个事务查询一个User表id=100发现不存在该数据行，这时第二个事务又进来了，新增了一条id=100的数据行并且提交了事务。 我： 这时第一个事务新增一条id=100的数据行会报主键冲突，第一个事务再select一下，发现id=100数据行已经存在，这就是幻读。 面试官： 小伙子你能演示一下吗？我不太会你能教教我吗？我电脑在这里，你演示我看一看。 男人的嘴骗人的鬼，我信你个鬼，你这糟老头子坏得很，出来装X总是要还的，只能默默含泪把它敲完。 我： 首先创建一个User表，最为一个测试表，测试表里面有三个字段，并插入两条测试数据。 12345678CREATE TABLE User ( id INT(11) NOT NULL PRIMARY KEY AUTO_INCREMENT, name VARCHAR(20), age INT DEFAULT 0) ENGINE=InnoDB DEFAULT CHARSET=gb2312;INSERT INTO `user` VALUES (1, 'zhangsan', 23);INSERT INTO `user` VALUES (2, 'lisi', 20); 我： 再Mysql中可以先查询一下他的默认隔离级别，可以看出Mysql的默认隔离级别是REPEATABLE-READ。我： 先来演示一下读未提交，先把默认的隔离级别修改为READ UNCOMMITTED。我： 他设置隔离级别的语句中set global transaction isolation level read uncommitted，这里的global也可以换成session，global表示全局的，而session表示当前会话，也就是当前窗口有效。 我： 当设置完隔离级别后对于之前打开的会话，是无效的，要重新打开一个窗口设置隔离级别才生效。我： 然后是开启事务，Mysql中开启事务有两种方式begin/start transaction，最后提交事务执行commit，或者回滚事务rollback。 我： 在执行begin/start transaction命令，它们并不是一个事务的起点，在执行完它们后的第一个sql语句，才表示事务真正的启动 。 我： 这里直接打开两个新的窗口，同时开启事务，再第一个窗口先update一个id=1的数据行name改为’非科班的科班’，执行成功。我： 然后再第二个窗口执行两次的查询，分别是窗口一update之前的查询和update之后的查询。我： 第一个session产生的未提交的事务的状态就会直接影响到第二sesison，也就是脏读。 我： 对于读提交也是一样的，开启事务后，第一个事务先执行查询数据，然后第二个session执行update操作，但是还没有commit，这是第一个session再次select，数据并没有改变，再第二个session执行commit之后，第一个session再次select就是改变后的数据了。我： 这样第一个事务的查询结果就会收到第二事务的影响，这个也就是产生不可重复读的问题。 面试官： 小伙子你能画一下他执行的过程图吗？你讲的我有点乱，我还没有彻底明白。 我心里一万只什么马在飞过，欲哭无泪，这面试官真难伺候，说时迟那时快，从左屁股兜抽出笔，从右屁股兜拿出纸，开始画。我： 这个是读提交的时间轴图，读未提交的时间轴突，原理也一样的，第二个select的时候数据就已经改变了。 这是面试官拿过我的图看了一点，微微的点了点头，嘴角露出思思的笑意，我想你这糟老头子应该不会再刁难我了吧。 面试官： 嗯，你接着演示你的可重复读吧。 我： 嗯，好的，然后就是可重复读，和之前一样的操作。我： 将两个session开启为REPEATABLE READ，同时开启事务，再第一个事务中先select，然后在第二个事务里面update数据行，可以发现即使第二个事务已经commit，第一个事务再次select数据也还是没有改变，这就解决了不可重复读的问题。 我： 这里有个不同的地方就是在Mysql中，默认的不可重复读个隔离级别也解决了幻读的问题。 我： 从上面的演示中可以看出第一个事务中先select一个id=3的数据行，这条数据行是不存在的，返回Empty set，然后第二个事务中insert一条id=3的数据行并且commit，第一个事务中再次select的，数据也好是没有id=3的数据行。 我： 最后的串行化，样式步骤也是一样的，结果也和Mysql中默认的个可重复读隔离级别的结果一样，串行化的执行流程相当于把事务的执行过程变为顺序执行，我这边就不再做演示了。 我： 这四大等级从上到下，隔离的效果是逐渐增强，但是性能却是越来越差。 Mysql的锁机制面试官： 哦？性能越来越差？为什么会性能越来越差？你能说一说原因吗？ 哎呀，我这嘴，少说一句会死啊，这下好了，这个得说底层实现原理了，从原来得假装若有所思，变成了真正得若有所思。 我： 这个得从Mysq的锁说起，在Mysql中的锁可以分为分享锁/读锁（Shared Locks）、排他锁/写锁（Exclusive Locks） 、间隙锁、行锁（Record Locks）、表锁。 我： 在四个隔离级别中加锁肯定是要消耗性能的，而读未提交是没有加任何锁的，所以对于它来说也就是没有隔离的效果，所以它的性能也是最好的。 我： 对于串行化加的是一把大锁，读的时候加共享锁，不能写，写的时候，家的是排它锁，阻塞其它事务的写入和读取，若是其它的事务长时间不能写入就会直接报超时，所以它的性能也是最差的，对于它来就没有什么并发性可言。 我： 对于读提交和可重复读，他们俩的实现是兼顾解决数据问题，然后又要有一定的并发行，所以在实现上锁机制会比串行化优化很多，提高并发性，所以性能也会比较好。 事务底层实现原理我： 他们俩的底层实现采用的是MVCC（多版本并发控制）方式进行实现。 面试官： 你能先说一下先这几个锁的概念吗？我不是很懂，说说你的理解。 我： 哦，好的，共享锁是针对同一份数据，多个读操作可以同时进行，简单来说即读加锁，不能写并且可并行读；排他锁针对写操作，假如当前写操作没有完成，那么它会阻断其它的写锁和读锁，即写加锁，其它读写都阻塞 。 我： 而行锁和表锁，是从锁的粒度上进行划分的，行锁锁定当前数据行，锁的粒度小，加锁慢，发生锁冲突的概率小，并发度高，行锁也是MyISAM和InnoDB的区别之一，InnoDB支持行锁并且支持事务 。 我： 而表锁则锁的粒度大，加锁快，开销小，但是锁冲突的概率大，并发度低。 我： 间隙锁则分为两种：Gap Locks和Next-Key Locks。Gap Locks会锁住两个索引之间的区间，比如select * from User where id&gt;3 and id&lt;5 for update，就会在区间（3，5）之间加上Gap Locks。 我： Next-Key Locks是Gap Locks+Record Locks形成闭区间锁select * from User where id&gt;=3 and id=&lt;5 for update，就会在区间[3,5]之间加上Next-Key Locks。 面试官： 那Mysql中什么时候会加锁呢？ 我： 在数据库的增、删、改、查中，只有增、删、改才会加上排它锁，而只是查询并不会加锁，只能通过在select语句后显式加lock in share mode或者for update来加共享锁或者排它锁。 面试官： 你在上面提到MVCC（多版本并发控制），你能说一说原理吗？ 我： 在实现MVCC时用到了一致性视图，用于支持读提交和可重复读的实现。 我： 在实现可重复读的隔离级别，只需要在事务开始的时候创建一致性视图，也叫做快照，之后的查询里都共用这个一致性视图，后续的事务对数据的更改是对当前事务是不可见的，这样就实现了可重复读。 我： 而读提交，每一个语句执行前都会重新计算出一个新的视图，这个也是可重复读和读提交在MVCC实现层面上的区别。 面试官： 那你知道快照（视图）在MVCC底层是怎么工作的吗？ 我： 在InnoDB 中每一个事务都有一个自己的事务id，并且是唯一的，递增的 。 我： 对于Mysql中的每一个数据行都有可能存在多个版本，在每次事务更新数据的时候，都会生成一个新的数据版本，并且把自己的数据id赋值给当前版本的row trx_id。 面试官： 小伙子你可以画个图我看看吗？我不是很明白。 我有什么办法呢？完全没办法，只能又从屁股兜李拿出笔和纸，迅速的画了起来，相当这次面试要是不过血亏啊，浪费了我两张纸和笔水，多贵啊，只能豁出去了。 我： 如图中所示，假如三个事务更新了同一行数据，那么就会有对应的三个数据版本。 我： 实际上版本1、版本2并非实际物理存在的，而图中的U1和U2实际就是undo log，这v1和v2版本是根据当前v3和undo log计算出来的。 面试官： 那对于一个快照来说，你直到它要遵循什么规则吗？ 我： 嗯，对于一个事务视图来说除了对自己更新的总是可见，另外还有三种情况：版本未提交的，都是不可见的；版本已经提交，但是是在创建视图之后提交的也是不可见的；版本已经提交，若是在创建视图之前提交的是可见的。 面试官： 假如两个事务执行写操作，又怎么保证并发呢？ 我： 假如事务1和事务2都要执行update操作，事务1先update数据行的时候，先回获取行锁，锁定数据，当事务2要进行update操作的时候，也会取获取该数据行的行锁，但是已经被事务1占有，事务2只能wait。 我： 若是事务1长时间没有释放锁，事务2就会出现超时异常 。 面试官： 这个是在update的where后的条件是在有索引的情况下吧？ 我： 嗯，是的 。 面试官： 那没有索引的条件下呢？没办法快速定位到数据行呢？ 我： 若是没有索引的条件下，就获取所有行，都加上行锁，然后Mysql会再次过滤符合条件的的行并释放锁，只有符合条件的行才会继续持有锁。 我： 这样的性能消耗也会比较大。 面试官： 嗯嗯 此时面试官看看手表一个多钟已经过去了，也已经到了饭点时刻，我想他应该是肚子饿了，不会继续追问吧，两人持续僵了三十秒，他终于开口了。 面试官： 小伙子，现在时间也已经到了饭点了，今天的面试就到此结束吧，你回去等通知吧。 我： 。。。。。。。。。。","link":"/2020/10/14/%E6%88%91%E4%BB%A5%E4%B8%BA%E6%88%91%E5%AF%B9Mysql%E4%BA%8B%E5%8A%A1%E5%BE%88%E7%86%9F%EF%BC%8C%E7%9B%B4%E5%88%B0%E6%88%91%E9%81%87%E5%88%B0%E4%BA%86%E9%98%BF%E9%87%8C%E9%9D%A2%E8%AF%95%E5%AE%98/"},{"title":"","text":"前言前几天有粉丝和我聊到他找工作面试大厂时被问的问题，因为现在疫情期间，找工作也特别难找。他说面试的题目也比较难，都偏向于一两年的工作经验的面试题。 他说在一面的时候被问到Mysql的面试题，索引那块自己都回答比较满意，但是问到Mysql的锁机制就比较懵了。 因为平时没有关注Mysql的锁机制，当被问到高并发场景下锁机制是怎么保证数据的一致性的和事务隔离性的。 他把他面试的过程分享给了我，Mysql高并发锁机制的问题，几乎面大厂都有被问到，Mysql怎么在高并发下控制并发访问的？ 我细想了一下，Mysql的锁机制确实非常重要，所以在这里做一个全面的总结整理，便于以后的查阅，也分享给各位读者大大们。 Mysql的锁机制还是有点难理解的，所以这篇文章采用图文结合的方式讲解难点，帮助大家理解，讲解的主要内容如下图的脑图所示，基本涵盖了Mysql锁机制的所有知识点。 本文脑图 锁种类Mysql中锁的分类按照不同类型的划分可以分成不同的锁，按照锁的粒度划分可以分成：表锁、页锁、行锁；按照使用的方式划分可以分为：共享锁和排它锁；按照思想的划分：乐观锁和悲观锁。 下面我们对着这几种划分的锁进行详细的解说和介绍，在了解设计者设计锁的概念的同时，也能深入的理解设计者的设计思想。 表锁是粒度最大的锁，开销小，加锁快，不会出现死锁，但是由于粒度太大，因此造成锁的冲突几率大，并发性能低。 Mysql中MyISAM储存引擎就支持表锁，MyISAM的表锁模式有两种：表共享读锁和表独占写锁。 当一个线程获取到MyISAM表的读锁的时候，会阻塞其他用户对该表的写操作，但是不会阻塞其它用户对该用户的读操作。 相反的，当一个线程获取到MyISAM表的写锁的时候，就会阻塞其它用户的读写操作对其它的线程具有排它性。 页锁的粒度是介于行锁和表锁之间的一种锁，因为页锁是在BDB中支持的一种锁机制，也很少没人提及和使用，所以这里制作概述，不做详解。 行锁是粒度最小的锁机制，行锁的加锁开销性能大，加锁慢，并且会出现死锁，但是行锁的锁冲突的几率低，并发性能高。 行锁是InnoDB默认的支持的锁机制，MyISAM不支持行锁，这个也是InnoDB和MyISAM的区别之一。 行锁在使用的方式上可以划分为：共享读锁（S锁）和排它写锁（X锁）。 当一个事务对Mysql中的一条数据行加上了S锁，当前事务不能修改该行数据只能执行度操作，其他事务只能对该行数据加S锁不能加X锁。 若是一个事务对一行数据加了X锁，该事物能够对该行数据执行读和写操作，其它事务不能对该行数据加任何的锁，既不能读也不能写。 悲观锁和乐观锁是在很多框架都存在的一种思想，不要狭义地认为它们是某一种框架的锁机制。 数据库管理系统中为了控制并发，保证在多个事务执行时的数据一致性以及事务的隔离性，使用悲观锁和乐观锁来解决并发场景下的问题。 Mysql中悲观锁的实现是基于Mysql自身的锁机制实现，而乐观锁需要程序员自己去实现的锁机制，最常见的乐观锁实现就锁机制是使用版本号实现。 乐观锁设计思想的在CAS的运用也是比较经典，之前我写过一篇关于CAS的文章，大家感兴趣的可以参考这一篇[]。 从上面的介绍中说了每一种锁的概念，但是很难说哪一种锁就是最好的，锁没有最好的，只有哪种业务场景最适合哪种锁，具体业务具体分析。 下面我们就具体基于Mysql的存储引擎详细的分析每一种锁在存储引擎中的运用和实现。 MyISAMMyISAM中默认支持的表级锁有两种：共享读锁和独占写锁。表级锁在MyISAM和InnoDB的存储引擎中都支持，但是InnoDB默认支持的是行锁。 Mysql中平时读写操作都是隐式的进行加锁和解锁操作，Mysql已经自动帮我们实现加锁和解锁操作了，若是想要测试锁机制，我们就要显示的自己控制锁机制。 Mysql中可以通过以下sql来显示的在事务中显式的进行加锁和解锁操作： 123456// 显式的添加表级读锁LOCK TABLE 表名 READ// 显示的添加表级写锁LOCK TABLE 表名 WRITE// 显式的解锁（当一个事务commit的时候也会自动解锁）unlock tables; 下面我们就来测试一下MyISAM中的表级锁机制，首先创建一个测试表employee ，这里要指定存储引擎为MyISAM，并插入两条测试数据： 12345678CREATE TABLE IF NOT EXISTS employee ( id INT PRIMARY KEY auto_increment, name VARCHAR(40), money INT)ENGINE MyISAMINSERT INTO employee(name, money) VALUES('黎杜', 1000);INSERT INTO employee(name, money) VALUES('非科班的科班', 2000); 查看一下，表结果如下图所示： MyISAM表级写锁（1）与此同时再开启一个session窗口，然后在第一个窗口执行下面的sql，在session1中给表添加写锁： 1LOCK TABLE employee WRITE （2）可以在session2中进行查询或者插入、更新该表数据，可以发现都会处于等待状态，也就是session1锁住了整个表，导致session2只能等待：（3）在session1中进行查询、插入、更新数据，都可以执行成功：总结： 从上面的测试结果显示当一个线程获取到表级写锁后，只能由该线程对表进行读写操作，别的线程必须等待该线程释放锁以后才能操作。 MyISAM表级共享读锁（1）接下来测试一下表级共享读锁，同样还是利用上面的测试数据，第一步还是在session1给表加读锁。（2）然后在session1中尝试进行插入、更新数据，发现都会报错，只能查询数据。（3）最后在session2中尝试进行插入、更新数据，程序都会进入等待状态，只能查询数据，直到session1解锁表session2才能插入、更新数据。总结： 从上面的测试结果显示当一个线程获取到表级读锁后，该线程只能读取数据不能修改数据，其它线程也只能加读锁，不能加写锁。 MyISAM表级锁竞争情况MyISAM存储引擎中，可以通过查询变量来查看并发场景锁的争夺情况，具体执行下面的sql语句： 1show status like 'table%'; 主要是查看table_locks_waited和table_locks_immediate的值的大小分析锁的竞争情况。 Table_locks_immediate：表示能够立即获得表级锁的锁请求次数；Table_locks_waited表示不能立即获取表级锁而需要等待的锁请求次数分析，值越大竞争就越严重。 并发插入通过上面的操作演示，详细的说明了表级共享锁和表级写锁的特点。但是在平时的执行sql的时候，这些解锁和释放锁都是Mysql底层隐式的执行的。 上面的演示只是为了证明显式的执行事务的过程共享锁和表级写锁的加锁和解锁的特点，实际并不会这么做的。 在我们平时执行select语句的时候就会隐式的加读锁，执行增、删、改的操作时就会隐式的执行加写锁。 MyISAM存储引擎中，虽然读写操作是串行化的，但是它也支持并发插入，这个需要设置内部变量concurrent_insert的值。 它的值有三个值0、1、2。可以通过以下的sql查看concurrent_insert的默认值为**AUTO(或者1)**。concurrent_insert的值为NEVER (or 0)表示不支持比并发插入；值为AUTO(或者1）表示在MyISAM表中没有被删除的行，运行另一个线程从表尾插入数据；值为ALWAYS (or 2)表示不管是否有删除的行，都允许在表尾插入数据。 锁调度MyISAM存储引擎中，假如同时一个读请求，一个写请求过来的话，它会优先处理写请求，因为MyISAM存储引擎中认为写请求比都请求重要。 这样就会导致，假如大量的读写请求过来，就会导致读请求长时间的等待，或者”线程饿死”，因此MyISAM不适合运用于大量读写操作的场景，这样会导致长时间读取不到用户数据，用户体验感极差。 当然可以通过设置low-priority-updates参数，设置请求链接的优先级，使得Mysql优先处理读请求。 InnoDBInnoDB和MyISAM不同的是，InnoDB支持行锁和事务，行级锁的概念前面以及说了，这里就不再赘述，事务的四大特性的概述以及实现的原理可以参考这一篇[]。 InnoDB中除了有表锁和行级锁的概念，还有Gap Lock（间隙锁）、Next-key Lock锁，间隙锁主要用于范围查询的时候，锁住查询的范围，并且间隙锁也是解决幻读的方案。 InnoDB中的行级锁是对索引加的锁，在不通过索引查询数据的时候，InnoDB就会使用表锁。 但是通过索引查询的时候是否使用索引，还要看Mysql的执行计划，Mysql的优化器会判断是一条sql执行的最佳策略。 若是Mysql觉得执行索引查询还不如全表扫描速度快，那么Mysql就会使用全表扫描来查询，这是即使sql语句中使用了索引，最后还是执行为全表扫描，加的是表锁。 若是对于Mysql的sql执行原理不熟悉的可以参考这一篇文章[]。最后是否执行了索引查询可以通过explain来查看，我相信这个大家都是耳熟能详的命令了。 InnoDB行锁和表锁InnoDB的行锁也是分为行级共享读锁（S锁）和排它写锁（X锁），原理特点和MyISAM的表级锁两种模式是一样的。 若想显式的给表加行级读锁和写锁，可以执行下面的sql语句： 1234// 给查询sql显示添加读锁select ... lock in share mode;// 给查询sql显示添加写锁select ... for update； （1）下面我们直接进入锁机制的测试阶段，还是创建一个测试表，并插入两条数据： 12345678910// 先把原来的MyISAM表给删除了DROP TABLE IF EXISTS employee;CREATE TABLE IF NOT EXISTS employee ( id INT PRIMARY KEY auto_increment, name VARCHAR(40), money INT)ENGINE INNODB;// 插入测试数据INSERT INTO employee(name, money) VALUES('黎杜', 1000);INSERT INTO employee(name, money) VALUES('非科班的科班', 2000); （2）创建的表中可以看出对表中的字段只有id添加了主键索引，接着就是在session1窗口执行begin开启事务，并执行下面的sql语句： 12// 使用非索引字段查询，并显式的添加写锁select * from employee where name='黎杜' for update; （3）然后在session2中执行update语句，上面查询的式id=1的数据行，下面update的是id=2的数据行，会发现程序也会进入等待状态： 1update employee set name='ldc' where id =2; 可见若是使用非索引查询，直接就是使用的表级锁，锁住了整个表。（4）若是session1使用的是id来查询，如下图所示：（5）那么session2是可以成功update其它数据行的，但是这里我建议使用数据量大的表进行测试，因为前面我说过了是否执行索引还得看Mysql的执行计划，对于一些小表的操作，可能就直接使用全表扫描。（6）还有一种情况就是：假如我们给name字段也加上了普通索引，那么通过普通索引来查询数据，并且查询到多行数据，拿它是锁这多行数据还是锁整个表呢？ 下面我们来测试一下，首先给name字段添加普通索引，如下图所示：（6）并插入一条新的数据name值与id=2的值相同，并显式的加锁，如下若是：（7）当update其它数据行name值不是ldc的也会进入等待状态，并且通过explain来查看是否name=’ldc’有执行索引，可以看到sql语句是有执行索引条件的。结论：从上面的测试锁机制的演示可以得出以下几个结论： 执行非索引条件查询执行的是表锁。 执行索引查询是否是加行锁，还得看Mysql的执行计划，可以通过explain关键字来查看。 用普通键索引的查询，遇到索引值相同的，也会对其他的操作数据行的产生影响。 InnoDB间隙锁当我们使用范围条件查询而不是等值条件查询的时候，InnoDB就会给符合条件的范围索引加锁，在条件范围内并不存的记录就叫做”间隙（GAP）” 大家大概都知道在事务的四大隔离级别中，不可重复读会产生幻读的现象，只能通过提高隔离级别到串行化来解决幻读现象。 但是Mysql中的不可重复是已经解决了幻读问题，它通过引入间隙锁的实现来解决幻读，通过给符合条件的间隙加锁，防止再次查询的时候出现新数据产生幻读的问题。 例如我们执行下面的sql语句，就会对id大于100的记录加锁，在id&gt;100的记录中肯定是有不存在的间隙： 1Select * from employee where id&gt; 100 for update; （1）接着来测试间隙锁，新增一个字段num，并将num添加为普通索引、修改之前的数据使得num之间的值存在间隙，操作如下sql所示： 12345alter table employee add num int not null default 0;update employee set num = 1 where id = 1;update employee set num = 1 where id = 2;update employee set num = 3 where id = 3;insert into employee values(4,'kris',4000,5); （2）接着在session1的窗口开启事务，并执行下面操作：（3）同时打开窗口session2，并执行新增语句： 1234insert into employee values(5,'ceshi',5000,2); // 程序出现等待insert into employee values(5,'ceshi',5000,4); // 程序出现等待insert into employee values(5,'ceshi',5000,6); // 新增成功insert into employee values(6,'ceshi',5000,0); // 新增成功 从上面的测试结果显示在区间（1,3]U[3,5)之间加了锁，是不能够新增数据行，这就是新增num=2和num=4失败的原因，但是在这个区间以外的数据行是没有加锁的，可以新增数据行。 根据索引的有序性，而普通索引是可以出现重复值，那么当我们第一个sesson查询的时候只出现一条数据num=3，为了解决第二次查询的时候出现幻读，也就是出现两条或者更多num=3这样查询条件的数据。 Mysql在满足where条件的情况下，给（1,3]U[3,5)区间加上了锁不允许插入num=3的数据行，这样就解决了幻读。 这里抛出几种情况接着来测试间隙锁。主键索引（唯一索引）是否会加上间隙所呢？范围查询是否会加上间隙锁？使用不存在的检索条件是否会加上间隙锁？ 先来说说：主键索引（唯一索引）是否会加上间隙所呢？ 因为主键索引具有唯一性，不允许出现重复，那么当进行等值查询的时候id=3，只能有且只有一条数据，是不可能再出现id=3的第二条数据。 因此它只要锁定这条数据（锁定索引），在下次查询当前读的时候不会被删除、或者更新id=3的数据行，也就保证了数据的一致性，所以主键索引由于他的唯一性的原因，是不需要加间隙锁的。 再来说说第二个问题：范围查询是否会加上间隙锁？ 直接在session1中执行下面的sql语句，并在session2中在这个num&gt;=3的查询条件内和外新增数据： 1234select * from employee where num&gt;=3 for update;insert into employee values(6,'ceshi',5000,2); // 程序出现等待insert into employee values(7,'ceshi',5000,4); // 程序出现等待insert into employee values(8,'ceshi',5000,1); // 新增数据成功 我们来分析以下原理：单查询num&gt;=3的时候，在现有的employee表中满足条件的数据行，如下所示：|id| num ||–|–|| 3 | 3|| 4 |5 || 5 |6 | 那么在设计者的角度出发，我为了解决幻读的现象：在num&gt;=3的条件下是必须加上间隙锁的。 而在小于num=3中，下一条数据行就是num=1了，为了防止在（1，3]的范围中加入了num=3的数据行，所以也给这个间隙加上了锁，这就是添加num=2数据行出现等待的原因。 最后来说一说：使用不存在的检索条件是否会加上间隙锁？ 假如是查询num&gt;=8的数据行呢？因为employee表并不存在中num=8的数据行，num最大num=6，所以为了解决幻读（6，8]与num&gt;=8也会加上锁。 说到这里我相信很多人已经对间隙锁有了清晰和深入的认识，可以说是精通了，又可以和面试官互扯了。 假如你是第一次接触Mysql的锁机制，第一次肯定是懵的，建议多认真的看几遍，跟着案例敲一下自己深刻的去体会，慢慢的就懂了。 死锁死锁在InnoDB中才会出现死锁，MyISAM是不会出现死锁，因为MyISAM支持的是表锁，一次性获取了所有得锁，其它的线程只能排队等候。 而InnoDB默认支持行锁，获取锁是分步的，并不是一次性获取所有得锁，因此在锁竞争的时候就会出现死锁的情况。 虽然InnoDB会出现死锁，但是并不影响InnoDB最受欢成为迎的存储引擎，MyISAM可以理解为串行化操作，读写有序，因此支持的并发性能低下。 死锁案例一举一个例子，现在数据库表employee中六条数据，如下所示：其中name=ldc的有两条数据，并且name字段为普通索引，分别是id=2和id=3的数据行，现在假设有两个事务分别执行下面的两条sql语句： 1234// session1执行update employee set num = 2 where name ='ldc';// session2执行select * from employee where id = 2 or id =3; 其中session1执行的sql获取的数据行是两条数据，假设先获取到第一个id=2的数据行，然后cpu的时间分配给了另一个事务，另一个事务执行查询操作获取了第二行数据也就是id=3的数据行。 当事务2继续执行的时候获取到id=3的数据行，锁定了id=3的数据行，此时cpu又将时间分配给了第一个事务，第一个事务执行准备获取第二行数据的锁，发现已经被其他事务获取了，它就处于等待的状态。 当cpu把时间有分配给了第二个事务，第二个事务准备获取第一行数据的锁发现已经被第一个事务获取了锁，这样就行了死锁，两个事务彼此之间相互等待。 死锁案例二第二种死锁情况就是当一个事务开始并且update一条id=1的数据行时，成功获取到写锁，此时另一个事务执行也update另一条id=2的数据行时，也成功获取到写锁（id为主键）。 此时cpu将时间分配给了事务一，事务一接着也是update id=2的数据行，因为事务二已经获取到id=2数据行的锁，所以事务已处于等待状态。 事务二有获取到了时间，像执行update id=1的数据行，但是此时id=1的锁被事务一获取到了，事务二也处于等待的状态，因此形成了死锁。 session1 session2 begin;update t set name=’测试’ where id=1; begin update t set name=’测试’ where id=2; update t set name=’测试’ where id=2; 等待….. update t set name=’测试’ where id=1; 等待….. 等待…… 死锁的解决方案首先要解决死锁问题，在程序的设计上，当发现程序有高并发的访问某一个表时，尽量对该表的执行操作串行化，或者锁升级，一次性获取所有的锁资源。 然后也可以设置参数innodb_lock_wait_timeout，超时时间，并且将参数innodb_deadlock_detect 打开，当发现死锁的时候，自动回滚其中的某一个事务。 总结上面详细的介绍了MyISAM和InnoDB两种存储引擎的锁机制的实现，并进行了测试。 MyISAM的表锁分为两种模式：共享读锁和排它写锁。获取的读锁的线程对该数据行只能读，不能修改，其它线程也只能对该数据行加读锁。 获取到写锁的线程对该数据行既能读也能写，对其他线程对该数据行的读写具有排它性。 MyISAM中默认写优先于去操作，因此MyISAM一般不适合运用于大量读写操作的程序中。 InnoDB的行锁虽然会出现死锁的可能，但是InnoDB的支持的并发性能比MyISAM好，行锁的粒度最小，一定的方法和措施可以解决死锁的发生，极大的发挥InnoDB的性能。 InnoDB中引入了间隙锁的概念来决解出现幻读的问题，也引入事务的特性，通过事务的四种隔离级别，来降低锁冲突，提高并发性能。","link":"/2020/10/14/%E9%98%BF%E9%87%8CP6+%E7%9A%84Mysql%E9%94%81%E6%9C%BA%E5%88%B6%E4%BA%8C%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%EF%BC%8C%E5%80%BC%E5%BE%97%E6%94%B6%E8%97%8F/"},{"title":"","text":"学习Redis一篇就够本文脑图 redis基本数据结构本文脑图 前言Redis是基于c语言编写的开源非关系型内存数据库，可以用作数据库、缓存、消息中间件，这么优秀的东西客定要一点一点的吃透它。 这是关于Redis五种数据结构详解，包括这五种的数据结构的底层原理实现。 理论肯定是要用于实践的，因此最重要的还是实战部分，也就是这里还会讲解五种数据结构的应用场景。 话不多说，我们直接进入主题，很多人都知道Redis的五种数据结构包括以下五种： String：字符串类型 List：列表类型 Set：无序集合类型 ZSet：有序集合类型 Hash：哈希表类型 但是作为一名优秀的程序员可能不能只停留在只会用着五种类型进行crud工作，还是得深入了解这五种数据结构的底层原理。 Redis核心对象在Redis中有一个核心的对象叫做redisObject ，是用来表示所有的key和value的，用redisObject结构体来表示String、Hash、List、Set、ZSet五种数据类型。 redisObject的源代码在redis.h中，使用c语言写的，感兴趣的可以自行查看，关于redisObject我这里画了一张图，表示redisObject的结构如下所示： 在redisObject中type表示属于哪种数据类型，encoding表示该数据的存储方式，也就是底层的实现的该数据类型的数据结构。因此这篇文章具体介绍的也是encoding对应的部分。 那么encoding中的存储类型又分别表示什么意思呢？具体数据类型所表示的含义，如下图所示： 可能看完这图，还是觉得一脸懵。不慌，会进行五种数据结构的详细介绍，这张图只是让你找到每种中数据结构对应的储存类型有哪些，大概脑子里有个印象。 举一个简单的例子，你在Redis中设置一个字符串key 234，然后查看这个字符串的存储类型就会看到为int类型，非整数型的使用的是embstr储存类型，具体操作如下图所示： String类型String是Redis最基本的数据类型，上面的简介中也说到Redis是用c语言开发的。但是Redis中的字符串和c语言中的字符串类型却是有明显的区别。 String类型的数据结构存储方式有三种int、raw、embstr。那么这三种存储方式有什么区别呢？ intRedis中规定假如存储的是整数型值，比如set num 123这样的类型，就会使用 int的存储方式进行存储，在redisObject的ptr属性中就会保存该值。 SDS假如存储的字符串是一个字符串值并且长度大于32个字节就会使用SDS（simple dynamic string）方式进行存储，并且encoding设置为raw；若是字符串长度小于等于32个字节就会将encoding改为embstr来保存字符串。 SDS称为简单动态字符串，对于SDS中的定义在Redis的源码中有的三个属性int len、int free、char buf[]。 len保存了字符串的长度，free表示buf数组中未使用的字节数量，buf数组则是保存字符串的每一个字符元素。 因此当你在Redsi中存储一个字符串Hello时，根据Redis的源代码的描述可以画出SDS的形式的redisObject结构图如下图所示： SDS与c语言字符串对比Redis使用SDS作为存储字符串的类型肯定是有自己的优势，SDS与c语言的字符串相比，SDS对c语言的字符串做了自己的设计和优化，具体优势有以下几点： （1）c语言中的字符串并不会记录自己的长度，因此**每次获取字符串的长度都会遍历得到，时间的复杂度是O(n)**，而Redis中获取字符串只要读取len的值就可，时间复杂度变为O(1)。 （2）c语言中两个字符串拼接，若是没有分配足够长度的内存空间就会出现缓冲区溢出的情况；而SDS会先根据len属性判断空间是否满足要求，若是空间不够，就会进行相应的空间扩展，所以不会出现缓冲区溢出的情况。 （3）SDS还提供空间预分配和惰性空间释放两种策略。在为字符串分配空间时，分配的空间比实际要多，这样就能减少连续的执行字符串增长带来内存重新分配的次数。 当字符串被缩短的时候，SDS也不会立即回收不适用的空间，而是通过free属性将不使用的空间记录下来，等后面使用的时候再释放。 具体的空间预分配原则是：当修改字符串后的长度len小于1MB，就会预分配和len一样长度的空间，即len=free；若是len大于1MB，free分配的空间大小就为1MB。 （4）SDS是二进制安全的，除了可以储存字符串以外还可以储存二进制文件（如图片、音频，视频等文件的二进制数据）；而c语言中的字符串是以空字符串作为结束符，一些图片中含有结束符，因此不是二进制安全的。 为了方便易懂，做了一个c语言的字符串和SDS进行对比的表格，如下所示：| c语言字符串 | SDS ||–|–|| 获取长度的时间复杂度为O(n) | 获取长度的时间复杂度为O(1) || 不是二进制安全的 | 是二进制安全的 || 只能保存字符串 | 还可以保存二进制数据 || n次增长字符串必然会带来n次的内存分配 | n次增长字符串内存分配的次数&lt;=n | String类型应用说到这里我相信很多人可以说已经精通Redis的String类型了，但是纯理论的精通，理论还是得应用实践，上面说到String可以用来存储图片，现在就以图片存储作为案例实现。 （1）首先要把上传得图片进行编码，这里写了一个工具类把图片处理成了Base64得编码形式，具体得实现代码如下： 123456789101112131415/** * 将图片内容处理成Base64编码格式 * @param file * @return */ public static String encodeImg(MultipartFile file) { byte[] imgBytes = null; try { imgBytes = file.getBytes(); } catch (IOException e) { e.printStackTrace(); } BASE64Encoder encoder = new BASE64Encoder(); return imgBytes==null?null:encoder.encode(imgBytes ); } （2）第二步就是把处理后的图片字符串格式存储进Redis中，实现得代码如下所示： 1234567891011/** * Redis存储图片 * @param file * @return */public void uploadImageServiceImpl(MultipartFile image) { String imgId = UUID.randomUUID().toString(); String imgStr= ImageUtils.encodeImg(image); redisUtils.set(imgId , imgStr); // 后续操作可以把imgId存进数据库对应的字段，如果需要从redis中取出，只要获取到这个字段后从redis中取出即可。} 这样就是实现了图片得二进制存储，当然String类型得数据结构得应用也还有常规计数：统计微博数、统计粉丝数等。 Hash类型Hash对象的实现方式有两种分别是ziplist、hashtable，其中hashtable的存储方式key是String类型的，value也是以key value的形式进行存储。 字典类型的底层就是hashtable实现的，明白了字典的底层实现原理也就是明白了hashtable的实现原理，hashtable的实现原理可以于HashMap的是底层原理相类比。 字典两者在新增时都会通过key计算出数组下标，不同的是计算法方式不同，HashMap中是以hash函数的方式，而hashtable中计算出hash值后，还要通过sizemask 属性和哈希值再次得到数组下标。 我们知道hash表最大的问题就是hash冲突，为了解决hash冲突，假如hashtable中不同的key通过计算得到同一个index，就会形成单向链表（链地址法），如下图所示： rehash在字典的底层实现中，value对象以每一个dictEntry的对象进行存储，当hash表中的存放的键值对不断的增加或者减少时，需要对hash表进行一个扩展或者收缩。 这里就会和HashMap一样也会就进行rehash操作，进行重新散列排布。从上图中可以看到有ht[0]和ht[1]两个对象，先来看看对象中的属性是干嘛用的。 在hash表结构定义中有四个属性分别是dictEntry **table、unsigned long size、unsigned long sizemask、unsigned long used，分别表示的含义就是哈希表数组、hash表大小、用于计算索引值，总是等于size-1、hash表中已有的节点数。 ht[0]是用来最开始存储数据的，当要进行扩展或者收缩时，ht[0]的大小就决定了ht[1]的大小，ht[0]中的所有的键值对就会重新散列到ht[1]中。 扩展操作：ht[1]扩展的大小是比当前 ht[0].used 值的二倍大的第一个 2 的整数幂；收缩操作：ht[0].used 的第一个大于等于的 2 的整数幂。 当ht[0]上的所有的键值对都rehash到ht[1]中，会重新计算所有的数组下标值，当数据迁移完后ht[0]就会被释放，然后将ht[1]改为ht[0]，并新创建ht[1]，为下一次的扩展和收缩做准备。 渐进式rehash假如在rehash的过程中数据量非常大，Redis不是一次性把全部数据rehash成功，这样会导致Redis对外服务停止，Redis内部为了处理这种情况采用渐进式的rehash。 Redis将所有的rehash的操作分成多步进行，直到都rehash完成，具体的实现与对象中的rehashindex属性相关，若是rehashindex 表示为-1表示没有rehash操作。 当rehash操作开始时会将该值改成0，在渐进式rehash的过程更新、删除、查询会在ht[0]和ht[1]中都进行，比如更新一个值先更新ht[0]，然后再更新ht[1]。 而新增操作直接就新增到ht[1]表中，ht[0]不会新增任何的数据，这样保证ht[0]只减不增，直到最后的某一个时刻变成空表，这样rehash操作完成。 上面就是字典的底层hashtable的实现原理，说完了hashtable的实现原理，我们再来看看Hash数据结构的两一种存储方式ziplist（压缩列表） ziplist压缩列表（ziplist）是一组连续内存块组成的顺序的数据结构，压缩列表能够节省空间，压缩列表中使用多个节点来存储数据。 压缩列表是列表键和哈希键底层实现的原理之一，压缩列表并不是以某种压缩算法进行压缩存储数据，而是它表示一组连续的内存空间的使用，节省空间，压缩列表的内存结构图如下： 压缩列表中每一个节点表示的含义如下所示： zlbytes：4个字节的大小，记录压缩列表占用内存的字节数。 zltail：4个字节大小，记录表尾节点距离起始地址的偏移量，用于快速定位到尾节点的地址。 zllen：2个字节的大小，记录压缩列表中的节点数。 entry：表示列表中的每一个节点。 zlend：表示压缩列表的特殊结束符号'0xFF'。 再压缩列表中每一个entry节点又有三部分组成，包括previous_entry_ength、encoding、content。 previous_entry_ength表示前一个节点entry的长度，可用于计算前一个节点的其实地址，因为他们的地址是连续的。 encoding：这里保存的是content的内容类型和长度。 content：content保存的是每一个节点的内容。 说到这里相信大家已经都hash这种数据结构已经非常了解，若是第一次接触Redis五种基本数据结构的底层实现的话，建议多看几遍，下面来说一说hash的应用场景。 应用场景哈希表相对于String类型存储信息更加直观，擦欧总更加方便，经常会用来做用户数据的管理，存储用户的信息。 hash也可以用作高并发场景下使用Redis生成唯一的id。下面我们就以这两种场景用作案例编码实现。 存储用户数据第一个场景比如我们要储存用户信息，一般使用用户的ID作为key值，保持唯一性，用户的其他信息（地址、年龄、生日、电话号码等）作为value值存储。 若是传统的实现就是将用户的信息封装成为一个对象，通过序列化存储数据，当需要获取用户信息的时候，就会通过反序列化得到用户信息。 但是这样必然会造成序列化和反序列化的性能的开销，并且若是只修改其中的一个属性值，就需要把整个对象序列化出来，操作的动作太大，造成不必要的性能开销。 若是使用Redis的hash来存储用户数据，就会将原来的value值又看成了一个k v形式的存储容器，这样就不会带来序列化的性能开销的问题。 分布式生成唯一ID第二个场景就是生成分布式的唯一ID，这个场景下就是把redis封装成了一个工具类进行实现，实现的代码如下： 1234567891011121314151617// offset表示的是id的递增梯度值public Long getId(String key,String hashKey,Long offset) throws BusinessException{ try { if (null == offset) { offset=1L; } // 生成唯一id return redisUtil.increment(key, hashKey, offset); } catch (Exception e) { //若是出现异常就是用uuid来生成唯一的id值 int randNo=UUID.randomUUID().toString().hashCode(); if (randNo &lt; 0) { randNo=-randNo; } return Long.valueOf(String.format(&quot;%16d&quot;, randNo)); }} List类型Redis中的列表在3.2之前的版本是使用ziplist和linkedlist进行实现的。在3.2之后的版本就是引入了quicklist。 ziplist压缩列表上面已经讲过了，我们来看看linkedlist和quicklist的结构是怎么样的。 linkedlist是一个双向链表，他和普通的链表一样都是由指向前后节点的指针。插入、修改、更新的时间复杂度尾O(1)，但是查询的时间复杂度确实O(n)。 linkedlist和quicklist的底层实现是采用链表进行实现，在c语言中并没有内置的链表这种数据结构，Redis实现了自己的链表结构。 Redis中链表的特性： 每一个节点都有指向前一个节点和后一个节点的指针。 头节点和尾节点的prev和next指针指向为null，所以链表是无环的。 链表有自己长度的信息，获取长度的时间复杂度为O(1)。 Redis中List的实现比较简单，下面我们就来看看它的应用场景。 应用场景Redis中的列表可以实现阻塞队列，结合lpush和brpop命令就可以实现。生产者使用lupsh从列表的左侧插入元素，消费者使用brpop命令从队列的右侧获取元素进行消费。 （1）首先配置redis的配置，为了方便我就直接放在application.yml配置文件中，实际中可以把redis的配置文件放在一个redis.properties文件单独放置，具体配置如下： 123456789101112spring redis: host: 127.0.0.1 port: 6379 password: user timeout: 0 database: 2 pool: max-active: 100 max-idle: 10 min-idle: 0 max-wait: 100000 （2）第二步创建redis的配置类，叫做RedisConfig，并标注上@Configuration注解，表明他是一个配置类。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051@Configurationpublic class RedisConfiguration {@Value(&quot;${spring.redis.host}&quot;)private String host;@Value(&quot;${spring.redis.port}&quot;)private int port;@Value(&quot;${spring.redis.password}&quot;)private String password;@Value(&quot;${spring.redis.pool.max-active}&quot;)private int maxActive;@Value(&quot;${spring.redis.pool.max-idle}&quot;)private int maxIdle;@Value(&quot;${spring.redis.pool.min-idle}&quot;)private int minIdle;@Value(&quot;${spring.redis.pool.max-wait}&quot;)private int maxWait;@Value(&quot;${spring.redis.database}&quot;)private int database;@Value(&quot;${spring.redis.timeout}&quot;)private int timeout;@Beanpublic JedisPoolConfig getRedisConfiguration(){ JedisPoolConfig jedisPoolConfig= new JedisPoolConfig(); jedisPoolConfig.setMaxTotal(maxActive); jedisPoolConfig.setMaxIdle(maxIdle); jedisPoolConfig.setMinIdle(minIdle); jedisPoolConfig.setMaxWaitMillis(maxWait); return jedisPoolConfig;}@Beanpublic JedisConnectionFactory getConnectionFactory() { JedisConnectionFactory factory = new JedisConnectionFactory(); factory.setHostName(host); factory.setPort(port); factory.setPassword(password); factory.setDatabase(database); JedisPoolConfig jedisPoolConfig= getRedisConfiguration(); factory.setPoolConfig(jedisPoolConfig); return factory;}@Beanpublic RedisTemplate&lt;?, ?&gt; getRedisTemplate() { JedisConnectionFactory factory = getConnectionFactory(); RedisTemplate&lt;?, ?&gt; redisTemplate = new StringRedisTemplate(factory); return redisTemplate;}} （3）第三步就是创建Redis的工具类RedisUtil，自从学了面向对象后，就喜欢把一些通用的东西拆成工具类，好像一个一个零件，需要的时候，就把它组装起来。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950@Componentpublic class RedisUtil {@Autowiredprivate RedisTemplate&lt;String, Object&gt; redisTemplate;/*** 存消息到消息队列中* @param key 键* @param value 值* @return*/public boolean lPushMessage(String key, Object value) { try { redisTemplate.opsForList().leftPush(key, value); return true; } catch (Exception e) { e.printStackTrace(); return false; }}/*** 从消息队列中弹出消息 - &lt;rpop：非阻塞式&gt;* @param key 键* @return*/public Object rPopMessage(String key) { try { return redisTemplate.opsForList().rightPop(key); } catch (Exception e) { e.printStackTrace(); return null; }}/*** 查看消息* @param key 键* @param start 开始* @param end 结束 0 到 -1代表所有值* @return*/public List&lt;Object&gt; getMessage(String key, long start, long end) { try { return redisTemplate.opsForList().range(key, start, end); } catch (Exception e) { e.printStackTrace(); return null; }} 这样就完成了Redis消息队列工具类的创建，在后面的代码中就可以直接使用。 Set集合Redis中列表和集合都可以用来存储字符串，但是Set是不可重复的集合，而List列表可以存储相同的字符串，Set集合是无序的这个和后面讲的ZSet有序集合相对。 Set的底层实现是ht和intset，ht（哈希表）前面已经详细了解过，下面我们来看看inset类型的存储结构。 inset也叫做整数集合，用于保存整数值的数据结构类型，它可以保存int16_t、int32_t 或者int64_t 的整数值。 在整数集合中，有三个属性值encoding、length、contents[]，分别表示编码方式、整数集合的长度、以及元素内容，length就是记录contents里面的大小。 在整数集合新增元素的时候，若是超出了原集合的长度大小，就会对集合进行升级，具体的升级过程如下： 首先扩展底层数组的大小，并且数组的类型为新元素的类型。 然后将原来的数组中的元素转为新元素的类型，并放到扩展后数组对应的位置。 整数集合升级后就不会再降级，编码会一直保持升级后的状态。 应用场景Set集合的应用场景可以用来去重、抽奖、共同好友、二度好友等业务类型。接下来模拟一个添加好友的案例实现： 1234567891011@RequestMapping(value = &quot;/addFriend&quot;, method = RequestMethod.POST)public Long addFriend(User user, String friend) { String currentKey = null; // 判断是否是当前用户的好友 if (AppContext.getCurrentUser().getId().equals(user.getId)) { currentKey = user.getId.toString(); } //若是返回0则表示不是该用户好友 return currentKey==null?0l:setOperations.add(currentKey, friend);} 假如两个用户A和B都是用上上面的这个接口添加了很多的自己的好友，那么有一个需求就是要实现获取A和B的共同好友，那么可以进行如下操作： 123public Set intersectFriend(User userA, User userB) { return setOperations.intersect(userA.getId.toString(), userB.getId.toString());} 举一反三，还可以实现A用户自己的好友，或者B用户自己的好友等，都可以进行实现。 ZSet集合ZSet是有序集合，从上面的图中可以看到ZSet的底层实现是ziplist和skiplist实现的，ziplist上面已经详细讲过，这里来讲解skiplist的结构实现。 skiplist也叫做跳跃表，跳跃表是一种有序的数据结构，它通过每一个节点维持多个指向其它节点的指针，从而达到快速访问的目的。 skiplist由如下几个特点： 有很多层组成，由上到下节点数逐渐密集，最上层的节点最稀疏，跨度也最大。 每一层都是一个有序链表，只扫包含两个节点，头节点和尾节点。 每一层的每一个每一个节点都含有指向同一层下一个节点和下一层同一个位置节点的指针。 如果一个节点在某一层出现，那么该以下的所有链表同一个位置都会出现该节点。 具体实现的结构图如下所示： 在跳跃表的结构中有head和tail表示指向头节点和尾节点的指针，能后快速的实现定位。level表示层数，len表示跳跃表的长度，BW表示后退指针，在从尾向前遍历的时候使用。 BW下面还有两个值分别表示分值（score）和成员对象（各个节点保存的成员对象）。 跳跃表的实现中，除了最底层的一层保存的是原始链表的完整数据，上层的节点数会越来越少，并且跨度会越来越大。 跳跃表的上面层就相当于索引层，都是为了找到最后的数据而服务的，数据量越大，条表所体现的查询的效率就越高，和平衡树的查询效率相差无几。 应用场景因为ZSet是有序的集合，因此ZSet在实现排序类型的业务是比较常见的，比如在首页推荐10个最热门的帖子，也就是阅读量由高到低，排行榜的实现等业务。 下面就选用获取排行榜前前10名的选手作为案例实现，实现的代码如下所示： 123456789101112131415161718192021222324@Autowiredprivate RedisTemplate redisTemplate; /** * 获取前10排名 * @return */ public static List&lt;levelVO &gt; getZset(String key, long baseNum, LevelService levelService){ ZSetOperations&lt;Serializable, Object&gt; operations = redisTemplate.opsForZSet(); // 根据score分数值获取前10名的数据 Set&lt;ZSetOperations.TypedTuple&lt;Object&gt;&gt; set = operations.reverseRangeWithScores(key,0,9); List&lt;LevelVO&gt; list= new ArrayList&lt;LevelVO&gt;(); int i=1; for (ZSetOperations.TypedTuple&lt;Object&gt; o:set){ int uid = (int) o.getValue(); LevelCache levelCache = levelService.getLevelCache(uid); LevelVO levelVO = levelCache.getLevelVO(); long score = (o.getScore().longValue() - baseNum + levelVO .getCtime())/CommonUtil.multiplier; levelVO .setScore(score); levelVO .setRank(i); list.add( levelVO ); i++; } return list; } 以上的代码实现大致逻辑就是根据score分数值获取前10名的数据，然后封装成lawyerVO对象的列表进行返回。 到这里我们已经精通Redis的五种基本数据类型了，又可以去和面试官扯皮了，扯不过就跑路吧，或者这篇文章多看几遍，相信对你总是有好处的。 Redis内存分配策略概述今天就带来了一个面试常问的一个问题：假如你的Redis内存满了怎么办？ 长期的把Redis作为缓存使用，总有一天会存满的时候对吧。 这个面试题不慌呀，在Redis中有配置参数maxmemory可以设置Redis内存的大小。 在Redis的配置文件redis.conf文件中，配置maxmemory的大小参数如下所示： 实际生产中肯定不是100mb的大小哈，不要给误导了，这里我只是让大家认识这个参数，一般小的公司都是设置为3G左右的大小。 除了在配置文件中配置生效外，还可以通过命令行参数的形式，进行配置，具体的配置命令行如下所示： 1234//获取maxmemory配置参数的大小127.0.0.1:6379&gt; config get maxmemory//设置maxmemory参数为100mb127.0.0.1:6379&gt; config set maxmemory 100mb 倘若实际的存储中超出了Redis的配置参数的大小时，Redis中有淘汰策略，把需要淘汰的key给淘汰掉，整理出干净的一块内存给新的key值使用。 接下来我们就详细的聊一聊Redis中的淘汰策略，并且深入的理解每个淘汰策略的原理和应用的场景。 淘汰策略Redis提供了6种的淘汰策略，其中默认的是noeviction，这6中淘汰策略如下： noeviction(默认策略)：若是内存的大小达到阀值的时候，所有申请内存的指令都会报错。 allkeys-lru：所有key都是使用LRU算法进行淘汰。 volatile-lru：所有设置了过期时间的key使用LRU算法进行淘汰。 allkeys-random：所有的key使用随机淘汰的方式进行淘汰。 volatile-random：所有设置了过期时间的key使用随机淘汰的方式进行淘汰。 volatile-ttl：所有设置了过期时间的key根据过期时间进行淘汰，越早过期就越快被淘汰。 假如在Redis中的数据有一部分是热点数据，而剩下的数据是冷门数据，或者我们不太清楚我们应用的缓存访问分布状况，这时可以使用allkeys-lru。 假如所有的数据访问的频率大概一样，就可以使用allkeys-random的淘汰策略。 假如要配置具体的淘汰策略，可以在redis.conf配置文件中配置，具体配置如下所示： 这只需要把注释给打开就可以，并且配置指定的策略方式，另一种的配置方式就是命令的方式进行配置，具体的执行命令如下所示： 1234// 获取maxmemory-policy配置127.0.0.1:6379&gt; config get maxmemory-policy// 设置maxmemory-policy配置为allkeys-lru127.0.0.1:6379&gt; config set maxmemory-policy allkeys-lru 在介绍6种的淘汰策略方式的时候，说到了LRU算法，那么什么是LRU算法呢？ LRU算法LRU(Least Recently Used)即表示最近最少使用，也就是在最近的时间内最少被访问的key，算法根据数据的历史访问记录来进行淘汰数据。 它的核心的思想就是：假如一个key值在最近很少被使用到，那么在将来也很少会被访问。 实际上Redis实现的LRU并不是真正的LRU算法，也就是名义上我们使用LRU算法淘汰键，但是实际上被淘汰的键并不一定是真正的最久没用的。 Redis使用的是近似的LRU算法，通过随机采集法淘汰key，每次都会随机选出5个key，然后淘汰里面最近最少使用的key。 这里的5个key只是默认的个数，具体的个数也可以在配置文件中进行配置，在配置文件中的配置如下图所示： 当近似LRU算法取值越大的时候就会越接近真实的LRU算法，可以这样理解，因为取值越大那么获取的数据就越全，淘汰中的数据的就越接近最近最少使用的数据。 那么为了实现根据时间实现LRU算法，Redis必须为每个key中额外的增加一个内存空间用于存储每个key的时间，大小是3字节。 在Redis 3.0中对近似的LRU算法做了一些优化，Redis中会维护大小是16的一个候选池的内存。 当第一次随机选取的采样数据，数据都会被放进候选池中，并且候选池中的数据会根据时间进行排序。 当第二次以后选取的数据，只有小于候选池内的最小时间的才会被放进候选池中。 当某一时刻候选池的数据满了，那么时间最大的key就会被挤出候选池。当执行淘汰时，直接从候选池中选取最近访问时间最小的key进行淘汰。 这样做的目的就是选取出最近似符合最近最少被访问的key值，能够正确的淘汰key值，因为随机选取的样本中的最小时间可能不是真正意义上的最小时间。 但是LRU算法有一个弊端：就是假如一个key值在以前都没有被访问到，然而最近一次被访问到了，那么就会认为它是热点数据，不会被淘汰。 然而有些数据以前经常被访问到，只是最近的时间内没有被访问到，这样就导致这些数据很可能被淘汰掉，这样一来就会出现误判而淘汰热点数据。 于是在Redis 4.0的时候除了LRU算法，新加了一种LFU算法，那么什么是LFU算法算法呢？ LFU算法LFU(Least Frequently Used)即表示最近频繁被使用，也就是最近的时间段内，频繁被访问的key，它以最近的时间段的被访问次数的频率作为一种判断标准。 它的核心思想就是：根据key最近被访问的频率进行淘汰，比较少被访问的key优先淘汰，反之则优先保留。 LFU算法反映了一个key的热度情况，不会因为LRU算法的偶尔一次被访问被认为是热点数据。 在LFU算法中支持volatile-lfu策略和allkeys-lfu策略。 以上介绍了Redis的6种淘汰策略，这6种淘汰策略旨在告诉我们怎么做，但是什么时候做？这个还没说，下面我们就来详细的了解Redis什么时候执行淘汰策略。 删除过期键策略在Redis种有三种删除的操作此策略，分别是： 定时删除：创建一个定时器，定时的执行对key的删除操作。 惰性删除：每次只有再访问key的时候，才会检查key的过期时间，若是已经过期了就执行删除。 定期删除：每隔一段时间，就会检查删除掉过期的key。 定时删除对于内存来说是友好的，定时清理出干净的空间，但是对于cpu来说并不是友好的，程序需要维护一个定时器，这就会占用cpu资源。 惰性的删除对于cpu来说是友好的，cpu不需要维护其它额外的操作，但是对于内存来说是不友好的，因为要是有些key一直没有被访问到，就会一直占用着内存。 定期删除是上面两种方案的折中方案，每隔一段时间删除过期的key，也就是根据具体的业务，合理的取一个时间定期的删除key。 通过最合理控制删除的时间间隔来删除key，减少对cpu的资源的占用消耗，使删除操作合理化。 RDB和AOF 的淘汰处理在Redis中持久化的方式有两种RDB和AOF，具体这两种详细的持久化介绍，可以参考这一篇文章[]。 在RDB中是以快照的形式获取内存中某一时间点的数据副本，在创建RDB文件的时候可以通过save和bgsave命令执行创建RDB文件。 这两个命令都不会把过期的key保存到RDB文件中，这样也能达到删除过期key的效果。 当在启动Redis载入RDB文件的时候，Master不会把过期的key载入，而Slave会把过期的key载入。 在AOF模式下，Redis提供了Rewite的优化措施，执行的命令分别是REWRITEAOF和BGREWRITEAOF，这两个命令都不会把过期的key写入到AOF文件中，也能删除过期key。 Redis缓存三大问题前言日常的开发中，无不都是使用数据库来进行数据的存储，由于一般的系统任务中通常不会存在高并发的情况，所以这样看起来并没有什么问题。 一旦涉及大数据量的需求，如一些商品抢购的情景，或者主页访问量瞬间较大的时候，单一使用数据库来保存数据的系统会因为面向磁盘，磁盘读/写速度问题有严重的性能弊端，详细的磁盘读写原理请参考这一片[]。 在这一瞬间成千上万的请求到来，需要系统在极短的时间内完成成千上万次的读/写操作，这个时候往往不是数据库能够承受的，极其容易造成数据库系统瘫痪，最终导致服务宕机的严重生产问题。 为了克服上述的问题，项目通常会引入NoSQL技术，这是一种基于内存的数据库，并且提供一定的持久化功能。 Redis技术就是NoSQL技术中的一种。Redis缓存的使用，极大的提升了应用程序的性能和效率，特别是数据查询方面。 但同时，它也带来了一些问题。其中，最要害的问题，就是数据的一致性问题，从严格意义上讲，这个问题无解。如果对数据的一致性要求很高，那么就不能使用缓存。 另外的一些典型问题就是，缓存穿透、缓存击穿和缓存雪崩。本篇文章从实际代码操作，来提出解决这三个缓存问题的方案，毕竟Redis的缓存问题是实际面试中高频问点，理论和实操要兼得。 缓存穿透缓存穿透是指查询一条数据库和缓存都没有的一条数据，就会一直查询数据库，对数据库的访问压力就会增大，缓存穿透的解决方案，有以下两种： 缓存空对象：代码维护较简单，但是效果不好。 布隆过滤器：代码维护复杂，效果很好。 缓存空对象缓存空对象是指当一个请求过来缓存中和数据库中都不存在该请求的数据，第一次请求就会跳过缓存进行数据库的访问，并且访问数据库后返回为空，此时也将该空对象进行缓存。 若是再次进行访问该空对象的时候，就会直接击中缓存，而不是再次数据库，缓存空对象实现的原理图如下： 缓存空对象的实现代码如下： 1234567891011121314151617181920212223242526272829public class UserServiceImpl { @Autowired UserDAO userDAO; @Autowired RedisCache redisCache; public User findUser(Integer id) { Object object = redisCache.get(Integer.toString(id)); // 缓存中存在，直接返回 if(object != null) { // 检验该对象是否为缓存空对象，是则直接返回null if(object instanceof NullValueResultDO) { return null; } return (User)object; } else { // 缓存中不存在，查询数据库 User user = userDAO.getUser(id); // 存入缓存 if(user != null) { redisCache.put(Integer.toString(id),user); } else { // 将空对象存进缓存 redisCache.put(Integer.toString(id), new NullValueResultDO()); } return user; } } } 缓存空对象的实现代码很简单，但是缓存空对象会带来比较大的问题，就是缓存中会存在很多空对象，占用内存的空间，浪费资源，一个解决的办法就是设置空对象的较短的过期时间，代码如下： 12// 再缓存的时候，添加多一个该空对象的过期时间60秒redisCache.put(Integer.toString(id), new NullValueResultDO(),60); 布隆过滤器布隆过滤器是一种基于概率的数据结构，主要用来判断某个元素是否在集合内，它具有运行速度快（时间效率），占用内存小的优点（空间效率），但是有一定的误识别率和删除困难的问题。它只能告诉你某个元素一定不在集合内或可能在集合内。 在计算机科学中有一种思想：空间换时间，时间换空间。一般两者是不可兼得，而布隆过滤器运行效率和空间大小都兼得，它是怎么做到的呢？ 在布隆过滤器中引用了一个误判率的概念，即它可能会把不属于这个集合的元素认为可能属于这个集合，但是不会把属于这个集合的认为不属于这个集合，布隆过滤器的特点如下： 一个非常大的二进制位数组 （数组里只有0和1） 若干个哈希函数 空间效率和查询效率高 不存在漏报（False Negative）：某个元素在某个集合中，肯定能报出来。 可能存在误报（False Positive）：某个元素不在某个集合中，可能也被爆出来。 不提供删除方法，代码维护困难。 位数组初始化都为0，它不存元素的具体值，当元素经过哈希函数哈希后的值（也就是数组下标）对应的数组位置值改为1。 实际布隆过滤器存储数据和查询数据的原理图如下： 可能很多读者看完上面的特点和原理图，还是看不懂，别急下面通过图解一步一步的讲解布隆过滤器，总而言之一句简单的话概括就是布隆过滤器是一个很大二进制的位数组，数组里面只存0和1。 初始化的布隆过滤器的结构图如下： 以上只是画了布隆过滤器的很小很小的一部分，实际布隆过滤器是非常大的数组（这里的大是指它的长度大，并不是指它所占的内存空间大）。 那么一个数据是怎么存进布隆过滤器的呢？ 当一个数据进行存入布隆过滤器的时候，会经过如干个哈希函数进行哈希（若是对哈希函数还不懂的请参考这一片[]），得到对应的哈希值作为数组的下标，然后将初始化的位数组对应的下标的值修改为1，结果图如下： 当再次进行存入第二个值的时候，修改后的结果的原理图如下： 所以每次存入一个数据，就会哈希函数的计算，计算的结果就会作为下标，在布隆过滤器中有多少个哈希函数就会计算出多少个下标，布隆过滤器插入的流程如下： 将要添加的元素给m个哈希函数 得到对应于位数组上的m个位置 将这m个位置设为1 那么为什么会有误判率呢？ 假设在我们多次存入值后，在布隆过滤器中存在x、y、z这三个值，布隆过滤器的存储结构图如下所示： 当我们要查询的时候，比如查询a这个数，实际中a这个数是不存在布隆过滤器中的，经过2哥哈希函数计算后得到a的哈希值分别为2和13，结构原理图如下： 经过查询后，发现2和13位置所存储的值都为1，但是2和13的下标分别是x和z经过计算后的下标位置的修改，该布隆过滤器中实际不存在a，那么布隆过滤器就会误判改值可能存在，因为布隆过滤器不存元素值，所以存在误判率。 那么具体布隆过布隆过滤的判断的准确率和一下两个因素有关： 布隆过滤器大小：越大，误判率就越小，所以说布隆过滤器一般长度都是非常大的。 哈希函数的个数：哈希函数的个数越多，那么误判率就越小。 那么为什么不能删除元素呢？ 原因很简单，因为删除元素后，将对应元素的下标设置为零，可能别的元素的下标也引用改下标，这样别的元素的判断就会收到影响，原理图如下： 当你删除z元素之后，将对应的下标10和13设置为0，这样导致x和y元素的下标受到影响，导致数据的判断不准确，所以直接不提供删除元素的api。 以上说的都是布隆过滤器的原理，只有理解了原理，在实际的运用才能如鱼得水，下面就来实操代码，手写一个简单的布隆过滤器。 对于要手写一个布隆过滤器，首先要明确布隆过滤器的核心： 若干哈希函数 存值得Api 判断值得Api 实现得代码如下： 1234567891011121314151617181920212223242526272829303132333435363738public class MyBloomFilter { // 布隆过滤器长度 private static final int SIZE = 2 &lt;&lt; 10; // 模拟实现不同的哈希函数 private static final int[] num= new int[] {5, 19, 23, 31,47, 71}; // 初始化位数组 private BitSet bits = new BitSet(SIZE); // 用于存储哈希函数 private MyHash[] function = new MyHash[num.length]; // 初始化哈希函数 public MyBloomFilter() { for (int i = 0; i &lt; num.length; i++) { function [i] = new MyHash(SIZE, num[i]); } } // 存值Api public void add(String value) { // 对存入得值进行哈希计算 for (MyHash f: function) { // 将为数组对应的哈希下标得位置得值改为1 bits.set(f.hash(value), true); } } // 判断是否存在该值得Api public boolean contains(String value) { if (value == null) { return false; } boolean result= true; for (MyHash f : func) { result= result&amp;&amp; bits.get(f.hash(value)); } return result; }} 哈希函数代码如下： 123456789101112131415161718public static class MyHash { private int cap; private int seed; // 初始化数据 public MyHash(int cap, int seed) { this.cap = cap; this.seed = seed; } // 哈希函数 public int hash(String value) { int result = 0; int len = value.length(); for (int i = 0; i &lt; len; i++) { result = seed * result + value.charAt(i); } return (cap - 1) &amp; result; } } 布隆过滤器测试代码如下： 1234567public static void test { String value = &quot;4243212355312&quot;; MyBloomFilter filter = new MyBloomFilter(); System.out.println(filter.contains(value)); filter.add(value); System.out.println(filter.contains(value));} 以上就是手写了一个非常简单得布隆过滤器，但是实际项目中可能事由牛人或者大公司已经帮你写好的，如谷歌的Google Guava，只需要在项目中引入一下依赖： 12345&lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;27.0.1-jre&lt;/version&gt;&lt;/dependency&gt; 实际项目中具体的操作代码如下： 1234567891011121314public static void MyBloomFilterSysConfig { @Autowired OrderMapper orderMapper // 1.创建布隆过滤器 第二个参数为预期数据量10000000，第三个参数为错误率0.00001 BloomFilter&lt;CharSequence&gt; bloomFilter = BloomFilter.create(Funnels.stringFunnel(Charset.forName(&quot;utf-8&quot;)),10000000, 0.00001); // 2.获取所有的订单，并将订单的id放进布隆过滤器里面 List&lt;Order&gt; orderList = orderMapper.findAll() for (Order order;orderList ) { Long id = order.getId(); bloomFilter.put(&quot;&quot; + id); }} 在实际项目中会启动一个系统任务或者定时任务，来初始化布隆过滤器，将热点查询数据的id放进布隆过滤器里面，当用户再次请求的时候，使用布隆过滤器进行判断，改订单的id是否在布隆过滤器中存在，不存在直接返回null，具体操作代码： 12// 判断订单id是否在布隆过滤器中存在bloomFilter.mightContain(&quot;&quot; + id) 布隆过滤器的缺点就是要维持容器中的数据，因为订单数据肯定是频繁变化的，实时的要更新布隆过滤器中的数据为最新。 缓存击穿缓存击穿是指一个key非常热点，在不停的扛着大并发，大并发集中对这一个点进行访问，当这个key在失效的瞬间，持续的大并发就穿破缓存，直接请求数据库，瞬间对数据库的访问压力增大。 缓存击穿这里强调的是并发，造成缓存击穿的原因有以下两个： 该数据没有人查询过 ，第一次就大并发的访问。（冷门数据） 添加到了缓存，reids有设置数据失效的时间 ，这条数据刚好失效，大并发访问（热点数据） 对于缓存击穿的解决方案就是加锁，具体实现的原理图如下： 当用户出现大并发访问的时候，在查询缓存的时候和查询数据库的过程加锁，只能第一个进来的请求进行执行，当第一个请求把该数据放进缓存中，接下来的访问就会直接集中缓存，防止了缓存击穿。 业界比价普遍的一种做法，即根据key获取value值为空时，锁上，从数据库中load数据后再释放锁。若其它线程获取锁失败，则等待一段时间后重试。这里要注意，分布式环境中要使用分布式锁，单机的话用普通的锁（synchronized、Lock）就够了。 下面以一个获取商品库存的案例进行代码的演示，单机版的锁实现具体实现的代码如下： 123456789101112131415161718192021// 获取库存数量public String getProduceNum(String key) { try { synchronized (this) { //加锁 // 缓存中取数据，并存入缓存中 int num= Integer.parseInt(redisTemplate.opsForValue().get(key)); if (num&gt; 0) { //没查一次库存-1 redisTemplate.opsForValue().set(key, (num- 1) + &quot;&quot;); System.out.println(&quot;剩余的库存为num：&quot; + (num- 1)); } else { System.out.println(&quot;库存为0&quot;); } } } catch (NumberFormatException e) { e.printStackTrace(); } finally { } return &quot;OK&quot;;} 分布式的锁实现具体实现的代码如下： 1234567891011121314151617181920212223public String getProduceNum(String key) { // 获取分布式锁 RLock lock = redissonClient.getLock(key); try { // 获取库存数 int num= Integer.parseInt(redisTemplate.opsForValue().get(key)); // 上锁 lock.lock(); if (num&gt; 0) { //减少库存，并存入缓存中 redisTemplate.opsForValue().set(key, (num - 1) + &quot;&quot;); System.out.println(&quot;剩余库存为num：&quot; + (num- 1)); } else { System.out.println(&quot;库存已经为0&quot;); } } catch (NumberFormatException e) { e.printStackTrace(); } finally { //解锁 lock.unlock(); } return &quot;OK&quot;;} 缓存雪崩缓存雪崩 是指在某一个时间段，缓存集中过期失效。此刻无数的请求直接绕开缓存，直接请求数据库。 造成缓存雪崩的原因，有以下两种： reids宕机 大部分数据失效 比如天猫双11，马上就要到双11零点，很快就会迎来一波抢购，这波商品在23点集中的放入了缓存，假设缓存一个小时，那么到了凌晨24点的时候，这批商品的缓存就都过期了。 而对这批商品的访问查询，都落到了数据库上，对于数据库而言，就会产生周期性的压力波峰，对数据库造成压力，甚至压垮数据库。 缓存雪崩的原理图如下，当正常的情况下，key没有大量失效的用户访问原理图如下： 当某一时间点，key大量失效，造成的缓存雪崩的原理图如下： 对于缓存雪崩的解决方案有以下两种： 搭建高可用的集群，防止单机的redis宕机。 设置不同的过期时间，防止同意之间内大量的key失效。 针对业务系统，永远都是具体情况具体分析，没有最好，只有最合适。于缓存其它问题，缓存满了和数据丢失等问题，我们后面继续深入的学习。最后也提一下三个词LRU、RDB、AOF，通常我们采用LRU策略处理溢出，Redis的RDB和AOF持久化策略来保证一定情况下的数据安全。 redis持久化本文脑图 Redis是一个基于内存的非关系型的数据库，数据保存在内存中，但是内存中的数据也容易发生丢失。这里Redis就为我们提供了持久化的机制，分别是RDB(Redis DataBase)和AOF(Append Only File)。 Redis在以前的版本中是单线程的，而在6.0后对Redis的io模型做了优化，io Thread为多线程的，但是worker Thread仍然是单线程。 在Redis启动的时候就会去加载持久化的文件，如果没有就直接启动，在启动后的某一时刻由继续持久化内存中产生的数据。 接下来我们就来详细了解Redis的两种持久化机制RDB(Redis DataBase)和AOF(Append Only File)。 RDB持久化机制什么是RDB持久化呢？RDB持久化就是将当前进程的数据以生成快照的形式持久化到磁盘中。对于快照的理解，我们可以理解为将当前线程的数据以拍照的形式保存下来。 RDB持久化的时候会单独fork一个与当前进程一摸一样的子进程来进行持久化，因此RDB持久化有如下特点： 开机恢复数据快。 写入持久化文件快。 RDB的持久化也是Redis默认的持久化机制，它会把内存中的数据以快照的形式写入默认文件名为dump.rdb中保存。 在安装后的Redis中，Redis的配置都在redis.conf文件中，如下图所示，dbfilename就是配置RDB的持久化文件名。 持久化触发时机在RDB机制中触发内存中的数据进行持久化，有以下三种方式： （1）save命令： save命令不会fork子进程，通过阻塞当前Redis服务器，直到RDB完成为止，所以该命令在生产中一般不会使用。save命令执行原理图如下: 在redis.conf的配置中dir的配置就是RDB持久化后生成rdb二进制文件所在的位置，默认的位置是./，表示当前位置，哪里启动redis，就会在哪里生成持久化文件，如下图所示： 下面我们进行一下实操，演示一下二进制文件生成的过程，在我本机的电脑虚拟机中，我所在的位置如下，该文件夹是新创建的redis的数据存储文件夹。 然后我们直接在该位置启动我们的Redis服务，启动的命令如下： 1/root/redis-4.0.6/src/redis-server /root/redis-4.0.6/redis.conf 接着通过该命令：ps -aux | grep redis，查看我们的redis服务是否正常启动，若是显示如下图所示，则表示Redis是正常启动的： 正常启动后，直接登陆Redis，可以通过以下命令登陆Redis，如下图所示： 因为当前中Redis是新安装的，数据都是为空，什么都没有，然后通过下图的命令随意向Redis中输入几条命令，最后执行save命令，在该文件夹下就会出现dump.rdb持久化的数据文件。 当然上面说到，在新安装的Redis中默认的RDB数据持久化位置为./文件，一般我们会把它改成服务器自己的特定位置下，原理都是一样的，可以自己进行尝试，这里不再进行演示。 （2）bgsave命令： bgsave命令会在后台fork一个与Redis主线程一摸一样的子线程，由子线程负责内存中的数据持久化。 这样fork与主线程一样的子线程消耗了内存，但是不会阻塞主线程处理客户端请求，是以空间换时间的方式快照内存中的数据到到文件中。 bgsave命令阻塞只会发生在fork子线程的时候，这段时间发生的非常短，可以忽略不计，如下图是 bgsave执行的流程图： 上面说到redis.conf中的dir配置是配置持久化文件生成的指定的目录，dbfilename是配置生成的文件名，也可以通过命令行使用命令来动态的设置这两个配置，命令如下： 12config set dir{newDir}config set dbfilename{newFileName} （3）自动化 除了上面在命令行使用save和bgsave命令触发持久化，也可以在redis.conf配置文件中，完成配置，如下图所示： 在新安装的redis中由默认的以上三个save配置，save 900 1表示900秒内如果至少有1个key值变化，则进行持久化保存数据； save 300 10则表示300秒内如果至少有10个key值发生变化，则进行持久化，save 60 10000以此类推。 通过以上的分析可以得出以下save和bgsave的对比区别： save是同步持久化数据，而bgsave是异步持久化数据。 save不会fork子进程，通过主进程持久化数据，会阻塞处理客户端的请求，而bdsave会fork子进程持久化数据，同时还可以处理客户端请求，高效。 save不会消耗内存，而bgsave会消耗内存。 RDB的优缺点缺点： RDB持久化后的文件是紧凑的二进制文件，适合于备份、全量复制、大规模数据恢复的场景，对数据完整性和一致性要求不高，RDB会丢失最后一次快照的数据。 优点： 开机的恢复数据快，写入持久化文件快。 AOF持久化机制AOF持久化机制是以日志的形式记录Redis中的每一次的增删改操作，不会记录查询操作，以文本的形式记录，打开记录的日志文件就可以查看操作记录。 AOF是默认不开启的，若是像开启AOF，在如下图的配置修改即可： 只需要把appendonly no修改为appendonly yes即可开启，在AOF中通过appendfilename配置生成的文件名，该文件名默认为appendonly.aof，路径也是通过dir配置的，这个于RDB的一样，具体的配置信息如下图所示： AOF触发机制AOF带来的持久化更加安全可靠，默认提供三种触发机制，如下所示： no：表示等操作系统等数据缓存同步到磁盘中（快、持久化没保证）。 always：同步持久化，每次发生数据变更时，就会立即记录到磁盘中（慢，安全）。 everysec：表示每秒同步一次（默认值，很快，但是会丢失一秒内的数据）。 AOF中每秒同步也是异步完成的，效率是非常高的，由于该机制对日志文件的写入操作是采用append的形式。 因此在写入的过程即使宕机，也不会丢失已经存入日志文件的数据，数据的完整性是非常高的。 在新安装的Redis的配置文件中，AOF的配置如下所示： AOF重写机制但是，在写入所有的操作到日志文件中时，就会出现日志文件很多重复的操作，甚至是无效的操作，导致日志文件越来越大。 所谓的无效的的操作，举个例子，比如某一时刻对一个k++，然后后面的某一时刻k–，这样k的值是保持不变的，那么这两次的操作就是无效的。 如果像这样的无效操作很多，记录的文件臃肿，就浪费了资源空间，所以在Redis中出现了rewrite机制。 redis提供了bgrewriteaof命令。将内存中的数据以命令的方式保存到临时文件中，同时会fork出一条新进程来将文件重写。 重写AOF的日志文件不是读取旧的日志文件瘦身，而是将内存中的数据用命令的方式重写一个AOF文件，重新保存替换原来旧的日志文件，因此内存中的数据才是最新的。 重写操作也会fork一个子进程来处理重写操作，重写以内存中的数据作为重写的源，避免了操作的冗余性，保证了数据的最新。 在Redis以append的形式将修改的数据写入老的磁盘中 ，同时Redis也会创建一个新的文件用于记录此期间有哪些命令被执行。 下面进行演示一下AOF的操作，首先先打开AOF机制，修改配置文件中的appendonly no为appendonly yes，然后执行如下图的操作： 都显示执行成功，ls以下查看此时当前的文件夹终究会出现appendonly.aof，AOF的数据持久化文件，通过cat命令查看内容： 从上面的存储的文件中可以看出，每一个命令是非常有规律的，比如第一次执行key *映射到该配置文件中的命令如下： 12345*2 //表示该命令两组key 为一组 * 为一组$6 //表示SELECT有6字符SELECT$1 //表示下面的0一个字符0 然后执行set k1 1的命令，此命令映射到文件中的命令如下： 1234567*3 //表示该命令有三组set为一组 k1为一组 1为一组$3 // 表示set有三个字符set // 表示执行了set命令$2 // 表示k1有两个字符k1 // key值$1 // 便是value值的字符长度为11 // value值 当AOF的日志文件增长到一定大小的时候Redis就能够bgrewriteaof对日志文件进行重写瘦身。当AOF配置文件大于改配置项时自动开启重写（这里指超过原大小的100%）。 该配置可以通过如下的配置项进行配置： AOF的优缺点优点： AOF更好保证数据不会被丢失，最多只丢失一秒内的数据，通过foek一个子进程处理持久化操作，保证了主进程不会进程io操作，能高效的处理客户端的请求。 另外重写操作保证了数据的有效性，即使日志文件过大也会进行重写。 AOF的日志文件的记录可读性非常的高，即使某一时刻有人执行flushall清空了所有数据，只需要拿到aof的日志文件，然后把最后一条的flushall给删除掉，就可以恢复数据。 缺点： 对于相同数量的数据集而言，AOF文件通常要大于RDB文件。RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。AOF在运行效率上往往会慢于RDB。 混合持久化在redis4.0后混合持久化（RDB+AOF）对重写的优化，4.0版本的混合持久化默认是关闭的，可以通过以下的配置开启混合持久化： 混合持久化也是通过bgrewriteaof来完成的，不同的是当开启混合持久化时，fork出的子进程先将共享内存的数据以RDB方式写入aof文件中，然后再将重写缓冲区的增量命令以AOF方式写入文件中。 写入完成后通知主进程统计信息，并将新的含有RDB格式和AOF格式的AOF文件替换旧的AOF文件。简单的说：新的AOF文件前半段是以RDB格式的全量数据后半段是AOF格式的增量数据。 优点： 混合持久化结合RDB持久化和AOF持久化的优点，由于绝大部分的格式是RDB格式，加载速度快，增量数据以AOF方式保存，数据更少的丢失。 RDB和AOF优势和劣势rdb适合大规模的数据恢复，由于rdb时异快照的形式持久化数据，恢复的数据快，在一定的时间备份一次，而aof的保证数据更加完整，损失的数据只在秒内。 具体哪种更适合生产，在官方的建议中两种持久化机制同时开启，如果两种机制同时开启，优先使用aof持久化机制。 redis事务前言前几天有读者说自己面试被问到Redis的事务，虽然不常用，但是面试竟然被问到，平时自己没有注意Redis的事务这一块，面试的时候被问到非常不好受。 虽然，这位读者面试最后算是过了，但是薪资方面没有拿到自己理想的薪资。 其实这个也是正常的，一般面试被问到烂大街的，谁还问你啊，专门挑一些不常见的来问你，就是为了压你的薪资。 所以在这里写一篇文章对Redis的事务进行详细的讲解，估计对Redis事务从理解到原理深入这一篇就够了。 以后面试都不用担心了再被问道Redis的事务了，这一篇主要讲解Redis事务原理和实操的演练，理解理论的同时也通过实操来证实理论。 事务介绍Redis事务是一组命令的集合，将多个命令进行打包，然后这些命令会被顺序的添加到队列中，并且按顺序的执行这些命令。 Redis事务中没有像Mysql关系型数据库事务隔离级别的概念，不能保证原子性操作，也没有像Mysql那样执行事务失败会进行回滚操作。 这个与Redis的特点：快速、高效有着密切的关联，因为一些列回滚操作、像事务隔离级别那这样加锁、解锁，是非常消耗性能的。所以，Redis中执行事务的流程只需要简单的下面三个步骤： 开始事务（MULTI） 命令入队 执行事务（EXEC）、撤销事务（DISCARD ） 在Redis中事务的实现主要是通过如下的命令实现的：| 命令 | 功能描述 ||–|–|| MULTI | 事务开始的命令，执行该命令后，后面执行的对Redis数据类型的操作命令都会顺序的放进队列中，等待执行EXEC命令后队列中的命令才会被执行 || DISCARD | 放弃执行队列中的命令，你可以理解为Mysql的回滚操作，并且将当前的状态从事务状态改为非事务状态。 || EXEC | 执行该命令后表示顺序执行队列中的命令，执行完后并将结果显示在客户端，将当前状态从事务状态改为非事务状态。若是执行该命令之前有key被执行WATCH命令并且又被其它客户端修改，那么就会放弃执行队列中的所有命令，在客户端显示报错信息，若是没有修改就会执行队列中的所有命令。 || WATCH key | 表示指定监视某个key，该命令只能在MULTI命令之前执行，如果监视的key被其他客户端修改，EXEC将会放弃执行队列中的所有命令 || UNWATCH | 取消监视之前通过WATCH 命令监视的key，通过执行EXEC 、DISCARD 两个命令之前监视的key也会被取消监视 | 以上就是一个Redis事务的执行过程包含的命令，下面就来详细的围绕着这几个命令进行讲解。 开始事务MULTI 命令表示事务的开始，当看到OK表示已经进入事务的状态：该命令执行后客户端会将当前的状态从非事务状态修改为事务状态，这一状态的切换是将客户端的flags属性中打开REDIS_MULTI来完成的，该命令可以理解关系型数据库Mysql的BEGIN TRANCATION语句： 命令入队执行完MULTI命令后，后面执行的操作Redis五种类型的命令都会按顺序的进入命令队列中，该部分也是真正的业务逻辑的部分。 Redis客户端的命令执行后若是当前状态处于事务状态命令就会进入队列中，并且返回QUEUED字符串，表示该命令已经进入了命令队列中，并且事务队列是以先进先出（FIFO）的方式保存入队的命令的。若是当前状态是非事务状态就会立即执行命令，并将结果返回客户端。在事务状态执行操作事务的命令就会被立即执行，如EXEC、DISCARD、UNWATCH。结合上面的分析，Redis执行命令的流程如下图所示：事务的命令队列中有三个参数分别是：要执行的命令、命令的参数、参数的个数。例如：通过执行如下的命令： 123456redis&gt; MULTIOKredis&gt; SET name &quot;黎杜&quot;QUEUEDredis&gt; GET nameQUEUED 那么对应上面的队列中三个参数如下表格所示：| 执行的命令 | 命令的参数 | 参数的个数||–|–| –|| SET | [“name”, “黎杜”] | 2 || GET | [“name”] | 1 | 执行事务当客户端执行EXEC命令的时候，上面的命令队列就会被按照先进先出的顺序被执行，当然执行的结果有成功有失败，这个后面分析。 上面说到当客户端处于非事务的状态命令发送到服务端会被立即执行，若是客户端处于事务状态命令就会被放进命令队列。 命令入队的时候，会按照顺序进入队列，队列以先进先出的特点来执行队列中的命令。 若是客户端处于事务状态，执行的是EXEC、DISCARD、UNWATCH这些操作事务的命令，也会被立即执行。（1）正常执行 还是上面的例子，执行如下的代码： 123456redis&gt; MULTIOKredis&gt; SET name &quot;黎杜&quot;QUEUEDredis&gt; GET nameQUEUED 所有的命令进入了队列，当最后执行EXEC，首先会执行SET命令，然后执行GET命令，并且执行后的结果也会进入一个队列中保存，最后返回给客户端：| 回复的类型 | 回复的内容 ||–|–| –|| status code reply | OK || bulk reply | “黎杜” | 所以最后你会在客户端看到OK、黎杜，这样的结果显示，这个也就是一个事务成功执行的过程。 至此一个事务就完整的执行完成，并且此时客户端也从事务状态更改为非事务状态。 （2）放弃事务 当然你也可以放弃执行该事务，只要你再次执行DISCARD操作就会放弃执行此次的事务。具体代码如下所示： 12345678redis&gt; MULTIOKredis&gt; SET name &quot;黎杜&quot;QUEUEDredis&gt; GET nameQUEUEDredis&gt; DISCARD // 放弃执行事务OK DISCARD命令取消一个事务的时候，就会将命令队列清空，并且将客户端的状态从事务状态修改为非事务的状态。 Redis的事务是不可重复的，当客户端处于事务状态的时候，再次向服务端发送MULTI命令时，直接就会向客户端返回错误。 WATCH 命令WATCH命令是在MULTI命令之前执行的，表示监视任意数量的key，与它对应的命令就是UNWATCH命令，取消监视的key。 WATCH命令有点类似于乐观锁机制，在事务执行的时候，若是被监视的任意一个key被更改，则队列中的命令不会被执行，直接向客户端返回(nil)表示事务执行失败。 下面我们来演示一下WATCH命令的操作流程，具体实现代码如下： 123456789redis&gt; WATCH numOKredis&gt; MULTIOKredis&gt; incrby num 10QUEUEDredis&gt; decrby num 1QUEUEDredis&gt; EXEC // 执行成功 这个是WATCH命令的正常的操作流程，若是在其它的客户端，修改了被监视的任意key，就会放弃执行该事务，如下图所示：| 客户端一 | 客户端二 ||–|–|| WATCH num | || MULTI | || incrby num 10 | get num || | decrby num 1 || EXEC | || 执行失败，返回(nil) | | WATCH命令的底层实现中保存了watched_keys 字典，字典的键保存的是监视的key，值是一个链表，链表中的每个节点值保存的是监视该key的客户端。若是某个客户端不再监视某个key，该客户端就会从链表中脱离。如client3，通过执行UNWATCH命令，不再监视key1： 错误处理上面说到Redis是没有回滚机制的，那么执行的过程，若是不小心敲错命令，Redis的命令发送到服务端没有被立即执行，所以是暂时发现不到该错误。 那么在Redis中的错误处理主要分为两类：语法错误、运行错误。下面主要来讲解一下这两类错误的区别。 （1）语法错误 比如执行命令的时候，命令的不存在或者错误的敲错命令、参数的个数不对等都会导致语法错误。 下面来演示一下，执行下面的四个命令，前后的两个命令是正确的，中间的两个命令是错误的，如下所示： 123456789101112127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; set num 1QUEUED127.0.0.1:6379&gt; set num(error) ERR wrong number of arguments for 'set' command127.0.0.1:6379&gt; ssset num 3(error) ERR unknown command 'ssset'127.0.0.1:6379&gt; set num 2QUEUED127.0.0.1:6379&gt; exec(error) EXECABORT Transaction discarded because of previous errors. 语法错误是在Redis语法检测的时候就能发现的，所以当你执行错误命令的时候，也会即使的返回错误的提示。 最后，即使命令进入队列，只要存在语法错误，该队列中的命令都不会被执行，会直接向客户端返回事务执行失败的提示。 （2）运行错误 执行时使用不同类型的操作命令操作不同数据类型就会出现运行时错误，这种错误时Redis在不执行命令的情况下，是无法发现的。 1234567891011121314127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; set num 3QUEUED127.0.0.1:6379&gt; sadd num 4QUEUED127.0.0.1:6379&gt; set num 6QUEUED127.0.0.1:6379&gt; exec1) OK2) (error) WRONGTYPE Operation against a key holding the wrong kind of value3) OK127.0.0.1:6379&gt; get key&quot;6&quot; 这样就会导致，正确的命令被执行，而错误的命令不会不执行，这也显示出Redis的事务并不能保证数据的一致性，因为中间出现了错误，有些语句还是被执行了。 这样的结果只能程序员自己根据之前执行的命令，自己一步一步正确的回退，所谓自己的烂摊子，自己收拾。 Redis事务与Mysql事务我们知道关系性数据库Mysql中具有事务的四大特性：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）。 但是Redis的事务为了保证Redis除了客户端的请求高效，去除了传统关系型数据库的事务回滚、加锁、解锁这些消耗性能的操作，Redis的事务实现简单。 原子性中Redis的事务只能保证单个命令的原子性，多个命令就无法保证，如上面索道的运行时错误，即使中间有运行时错误出现也会正确的执行后面正确的命令，不具有回滚操作。 既然没有了原子性，数据的一致性也就无法保证，这些都需要程序员自己手动去实现。 Reids在进行事务的时候，不会被中断知道事务的运行结束，也具有一定的隔离性，并且Redis也能持久化数据。 集群集群概述Redis作为缓存的高效中间件，在我们日常的开发中被频繁的使用，今天就来说一说Redis的四种模式，分别是单机版、主从复制、哨兵、以及集群模式。 可能，在一般公司的程序员使用单机版基本都能解决问题，在Redis的官网给出的数据是10W QPS，这对于应付一般的公司绰绰有余了，再不行就来个主从模式，实现都写分离，性能又大大提高。 但是，我们作为有抱负的程序员，仅限于单机版和主从模式的crud是不行的，至少也要了解哨兵和集群模式的原理，这样面试的时候才能和面试官扯皮啊。 单机单机版的Redis就比较简单了，基本90%的程序员都是用过，官网推荐操作Redis的第三方依赖库是Jedis，在SpringBoot项目中，引入下面依赖就可以直接使用了： 12345&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;${jedis.version}&lt;/version&gt;&lt;/dependency&gt; 优点单机版的Redis也有很多优点，比如实现实现简单、维护简单、部署简单、维护成本非常低，不需要其它额外的开支。 缺点但是，因为是单机版的Redis所以也存在很多的问题，比如最明显的单点故障问题，一个Redis挂了，所有的请求就会直接打在了DB上。 并且一个Redis抗并发数量也是有限的，同时要兼顾读写两种请求，只要访问量一上来，Redis就受不了了，另一方面单机版的Redis数据量存储也是有限的，数据量一大，再重启Redis的时候，就会非常的慢，所以局限性也是比较大的。 实操搭建单机版的搭建教程，在网上有非常多的全面的教程，基本就是傻瓜式操作，特别是在本地搭建的话，基本使用yum快捷方便，几句命令就搞定了，这里推荐一个搭建教程：https://www.cnblogs.com/zuidongfeng/p/8032505.html。 上面这个教程讲的非常的详细，环境的搭建本来是运维的工作，但是作为程序员尝试自己去搭建环境还是有必要的，而且搭建环境这种东西，基本就是一劳永逸，搭建一次，可能下次换电脑或者重装虚拟机才会再次搭建。 这里也放出redis常用的redis.conf的配置项，并且附带注释，看我是不是很暖男： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102daemonize yes // 设置后台启动，一般设置yespidfile /var/run/redis.pid // edis以守护进程方式运行时,redis默认会把pid写入/var/run/redis.pid文件port 6379 // 默认端口为6379bind 127.0.0.1 //主机地址，设置未0.0.0.0表示都可以访问。127.0.0.1表示只允许本机访问timeout 900 // 客户端闲置多长时间后关闭连接，如果指定为0，表示关闭该功能logfile stdout // 日志记录方式，默认为标准输出logfile &quot;./redis7001.log&quot; # 指明日志文件名databases 16 // 设置数据库的数量，默认数据库为0save //有多少次更新操作，就将数据同步到数据文件 Redis默认配置文件中提供了三个条件： save 900 1 //900秒（15分钟）内有1个更改 save 300 10 //300秒（5分钟）内有10个更改 save 60 10000 // 60秒内有10000个更改rdbcompression yes // 指定存储至本地数据库时是否压缩数据dbfilename dump.rdb //指定本地数据库文件名dir ./ //指定本地数据库存放目录slaveof // 主从同步设置，设置主数据库的ip和端口# 如果非零，则设置SO_KEEPALIVE选项来向空闲连接的客户端发送ACKtcp-keepalive 60# 默认如果开启RDB快照(至少一条save指令)并且最新的后台保存失败，Redis将会停止接受写操作# 这将使用户知道数据没有正确的持久化到硬盘，否则可能没人注意到并且造成一些灾难stop-writes-on-bgsave-error yes# 默认如果开启RDB快照(至少一条save指令)并且最新的后台保存失败，Redis将会停止接受写操作。stop-writes-on-bgsave-error yes# 当导出到 .rdb 数据库时是否用LZF压缩字符串对象rdbcompression yes# 版本5的RDB有一个CRC64算法的校验和放在了文件的最后。这将使文件格式更加可靠。rdbchecksum yes# 持久化数据库的文件名dbfilename dump-master.rdb# 工作目录dir /usr/local/redis-4.0.8/redis_master/# slav服务连接master的密码masterauth testmaster123# 当一个slave失去和master的连接，或者同步正在进行中，slave的行为可以有两种：#1) 如果 slave-serve-stale-data 设置为 &quot;yes&quot; (默认值)，slave会继续响应客户端请求，可能是正常数据，或者是过时了的数据，也可能是还没获得值的空数据。# 2) 如果 slave-serve-stale-data 设置为 &quot;no&quot;，slave会回复&quot;正在从master同步# （SYNC with master in progress）&quot;来处理各种请求，除了 INFO 和 SLAVEOF 命令。slave-serve-stale-data yes# 配置是否仅读slave-read-only yes# 如果你选择“yes”Redis将使用更少的TCP包和带宽来向slaves发送数据。但是这将使数据传输到slave上有延迟，Linux内核的默认配置会达到40毫秒# 如果你选择了 &quot;no&quot; 数据传输到salve的延迟将会减少但要使用更多的带宽repl-disable-tcp-nodelay no# slave的优先级，优先级数字小的salve会优先考虑提升为masterslave-priority 100# 密码验证requirepass testmaster123# redis实例最大占用内存，一旦内存使用达到上限，Redis会根据选定的回收策略（参见：# maxmemmory-policy）删除keymaxmemory 3gb# 最大内存策略：如果达到内存限制了，Redis如何选择删除key。# volatile-lru -&gt; 根据LRU算法删除带有过期时间的key。# allkeys-lru -&gt; 根据LRU算法删除任何key。# volatile-random -&gt; 根据过期设置来随机删除key, 具备过期时间的key。 # allkeys-&gt;random -&gt; 无差别随机删, 任何一个key。 # volatile-ttl -&gt; 根据最近过期时间来删除（辅以TTL）, 这是对于有过期时间的key # noeviction -&gt; 谁也不删，直接在写操作时返回错误。maxmemory-policy volatile-lru# AOF开启appendonly no# aof文件名appendfilename &quot;appendonly.aof&quot;# fsync() 系统调用告诉操作系统把数据写到磁盘上，而不是等更多的数据进入输出缓冲区。# 有些操作系统会真的把数据马上刷到磁盘上；有些则会尽快去尝试这么做。# Redis支持三种不同的模式：# no：不要立刻刷，只有在操作系统需要刷的时候再刷。比较快。# always：每次写操作都立刻写入到aof文件。慢，但是最安全。# everysec：每秒写一次。折中方案。 appendfsync everysec# 如果AOF的同步策略设置成 &quot;always&quot; 或者 &quot;everysec&quot;，并且后台的存储进程（后台存储或写入AOF# 日志）会产生很多磁盘I/O开销。某些Linux的配置下会使Redis因为 fsync()系统调用而阻塞很久。# 注意，目前对这个情况还没有完美修正，甚至不同线程的 fsync() 会阻塞我们同步的write(2)调用。# 为了缓解这个问题，可以用下面这个选项。它可以在 BGSAVE 或 BGREWRITEAOF 处理时阻止主进程进行fsync()。# 这就意味着如果有子进程在进行保存操作，那么Redis就处于&quot;不可同步&quot;的状态。# 这实际上是说，在最差的情况下可能会丢掉30秒钟的日志数据。（默认Linux设定）# 如果你有延时问题把这个设置成&quot;yes&quot;，否则就保持&quot;no&quot;，这是保存持久数据的最安全的方式。no-appendfsync-on-rewrite yes# 自动重写AOF文件auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb# AOF文件可能在尾部是不完整的（这跟system关闭有问题，尤其是mount ext4文件系统时# 没有加上data=ordered选项。只会发生在os死时，redis自己死不会不完整）。# 那redis重启时load进内存的时候就有问题了。# 发生的时候，可以选择redis启动报错，并且通知用户和写日志，或者load尽量多正常的数据。# 如果aof-load-truncated是yes，会自动发布一个log给客户端然后load（默认）。# 如果是no，用户必须手动redis-check-aof修复AOF文件才可以。# 注意，如果在读取的过程中，发现这个aof是损坏的，服务器也是会退出的，# 这个选项仅仅用于当服务器尝试读取更多的数据但又找不到相应的数据时。aof-load-truncated yes# Lua 脚本的最大执行时间，毫秒为单位lua-time-limit 5000# Redis慢查询日志可以记录超过指定时间的查询slowlog-log-slower-than 10000# 这个长度没有限制。只是要主要会消耗内存。你可以通过 SLOWLOG RESET 来回收内存。slowlog-max-len 128# 客户端的输出缓冲区的限制，可用于强制断开那些因为某种原因从服务器读取数据的速度不够快的客户端client-output-buffer-limit normal 0 0 0client-output-buffer-limit slave 256mb 64mb 60client-output-buffer-limit pubsub 32mb 8mb 60# 当一个子进程重写AOF文件时，文件每生成32M数据会被同步aof-rewrite-incremental-fsync yes 由于，单机版的Redis在并发量比较大的时候，并且需要较高性能和可靠性的时候，单机版基本就不适合了，于是就出现了主从模式。 主从模式原理主从的原理还算是比较简单的，一主多从，主数据库（master）可以读也可以写（read/write），从数据库仅读（only read）。 但是，主从模式一般实现读写分离，主数据库仅写（only write），减轻主数据库的压力，下面一张图搞懂主从模式的原理： 主从模式原理就是那么简单，那他执行的过程（工作机制）又是怎么样的呢？再来一张图： 当开启主从模式的时候，他的具体工作机制如下： 当slave启动后会向master发送SYNC命令，master节后到从数据库的命令后通过bgsave保存快照（RDB持久化），并且期间的执行的些命令会被缓存起来。 然后master会将保存的快照发送给slave，并且继续缓存期间的写命令。 slave收到主数据库发送过来的快照就会加载到自己的数据库中。 最后master讲缓存的命令同步给slave，slave收到命令后执行一遍，这样master与slave数据就保持一致了。 优点之所以运用主从，是因为主从一定程度上解决了单机版并发量大，导致请求延迟或者redis宕机服务停止的问题。 从数据库分担主数据库的读压力，若是主数据库是只写模式，那么实现读写分离，主数据库就没有了读压力了。 另一方面解决了单机版单点故障的问题，若是主数据库挂了，那么从数据库可以随时顶上来，综上来说，主从模式一定程度上提高了系统的可用性和性能，是实现哨兵和集群的基础。 主从同步以异步方式进行同步，期间Redis仍然可以响应客户端提交的查询和更新的请求。 缺点主从模式好是好，他也有自己的缺点，比如数据的一致性问题，假如主数据库写操作完成，那么他的数据会被复制到从数据库，若是还没有即使复制到从数据库，读请求又来了，此时读取的数据就不是最新的数据。 若是从主同步的过程网络出故障了，导致主从同步失败，也会出现问题数据一致性的问题。 主从模式不具备自动容错和恢复的功能，一旦主数据库，从节点晋升未主数据库的过程需要人为操作，维护的成本就会升高，并且主节点的写能力、存储能力都会受到限制。 实操搭建下面的我们来实操搭建一下主从模式，主从模式的搭建还是比较简单的，我这里一台centos 7虚拟机，使用开启redis多实例的方法搭建主从。 redis中开启多实例的方法，首先创建一个文件夹，用于存放redis集群的配置文件： 1mkdir redis 然后粘贴复制redis.conf配置文件： 123cp /root/redis-4.0.6/redis.conf /root/redis/redis-6379.confcp /root/redis-4.0.6/redis.conf /root/redis/redis-6380.confcp /root/redis-4.0.6/redis.conf /root/redis/redis-6381.conf 复制三份配置文件，一主两从，6379端口作为主数据库（master），6380、6381作为从数据库（slave）。 首先是配置主数据库的配置文件：vi redis-6379.conf： 12345678910111213141516bind 0.0.0.0 # 注释掉或配置成0.0.0.0表示任意IP均可访问。protected-mode no # 关闭保护模式，使用密码访问。port 6379 # 设置端口，6380、6381依次为6380、6381。timeout 30 # 客户端连接空闲多久后断开连接，单位秒，0表示禁用daemonize yes # 在后台运行pidfile /var/run/redis_6379.pid # pid进程文件名，6380、6381依次为redis_6380.pid、redis_6381.pidlogfile /root/reids/log/6379.log # 日志文件，6380、6381依次为6380.log、6381.logsave 900 1 # 900s内至少一次写操作则执行bgsave进行RDB持久化save 300 10save 60 10000 rdbcompression yes #是否对RDB文件进行压缩，建议设置为no，以（磁盘）空间换（CPU）时间dbfilename dump.rdb # RDB文件名称dir /root/redis/datas # RDB文件保存路径，AOF文件也保存在这里appendonly yes # 表示使用AOF增量持久化的方式appendfsync everysec # 可选值 always， everysec，no，建议设置为everysecrequirepass 123456 # 设置密码 然后，就是修改从数据库的配置文件，在从数据库的配置文件中假如以下的配置信息： 123slaveof 127.0.0.1 6379 # 配置master的ip，portmasterauth 123456 # 配置访问master的密码slaveof-serve-stale-data no 接下来就是启动三个redis实例，启动的命令，先cd到redis的src目录下，然后执行： 123./redis-server /root/redis/6379.conf./redis-server /root/redis/6380.conf./redis-server /root/redis/6381.conf 通过命令ps -aux | grep redis，查看启动的redis进程：如上图所示，表示启动成功，下面就开始进入测试阶段。 测试我这里使用SecureCRT作为redis连接的客户端，同时启动三个SecureCRT，分别连接redis1的三个实例，启动时指定端口以及密码： 1./redis-cli -p 6379 -a 123456 启动后，在master（6379），输入：set name ‘ldc’，在slave中通过get name，可以查看： 数据同步成功，这有几个坑一个是redis.conf中没有设置对bind，会导致非本机的ip被过滤掉，一般配置0.0.0.0就可以了。 另一个是没有配置密码requirepass 123456，会导致IO一直连接异常，这个是我遇到的坑，后面配置密码后就成功了。 还有，就是查看redis的启动日志可以发现有两个warning，虽然不影响搭建主从同步，看着挺烦人的，但是有些人会遇到，有些人不会遇到。 但是，我这个人比较有强迫症，百度也是有解决方案的，这里就不讲了，交给你们自己解决，这里只是告诉你有这个问题，有些人看都不看日志的，看到启动成功就认为万事大吉了，也不看日志，这习惯并不好。 哨兵模式原理哨兵模式是主从的升级版，因为主从的出现故障后，不会自动恢复，需要人为干预，这就很蛋疼啊。 在主从的基础上，实现哨兵模式就是为了监控主从的运行状况，对主从的健壮进行监控，就好像哨兵一样，只要有异常就发出警告，对异常状况进行处理。 所以，总的概括来说，哨兵模式有以下的优点（功能点）： 监控：监控master和slave是否正常运行，以及哨兵之间也会相互监控 自动故障恢复：当master出现故障的时候，会自动选举一个slave作为master顶上去。 哨兵模式的监控配置信息，是通过配置从数据库的sentinel monitor &lt;master-name&gt; &lt;ip&gt; &lt;redis-port&gt; &lt;quorum&gt; 来指定的，比如： 12// mymaster 表示给master数据库定义了一个名字，后面的是master的ip和端口，1表示至少需要一个Sentinel进程同意才能将master判断为失效，如果不满足这个条件，则自动故障转移（failover）不会执行sentinel monitor mymaster 127.0.0.1 6379 1 节点通信当然还有其它的配置信息，其它配置信息，在环境搭建的时候再说。当哨兵启动后，会与master建立一条连接，用于订阅master的_sentinel_:hello频道。 该频道用于获取监控该master的其它哨兵的信息。并且还会建立一条定时向master发送INFO命令获取master信息的连接。 当哨兵与master建立连接后，定期会向（10秒一次）master和slave发送INFO命令，若是master被标记为主观下线，频率就会变为1秒一次。 并且，定期向_sentinel_:hello频道发送自己的信息，以便其它的哨兵能够订阅获取自己的信息，发送的内容包含哨兵的ip和端口、运行id、配置版本、master名字、master的ip端口还有master的配置版本等信息。 以及，定期的向master、slave和其它哨兵发送PING命令（每秒一次），以便检测对象是否存活，若是对方接收到了PING命令，无故障情况下，会回复PONG命令。 所以，哨兵通过建立这两条连接、通过定期发送INFO、PING命令来实现哨兵与哨兵、哨兵与master之间的通信。 这里涉及到一些概念需要理解，INFO、PING、PONG等命令，后面还会有MEET、FAIL命令，以及主观下线，当然还会有客观下线，这里主要说一下这几个概念的理解： INFO：该命令可以获取主从数据库的最新信息，可以实现新结点的发现 PING：该命令被使用最频繁，该命令封装了自身节点和其它节点的状态数据。 PONG：当节点收到MEET和PING，会回复PONG命令，也把自己的状态发送给对方。 MEET：该命令在新结点加入集群的时候，会向老节点发送该命令，表示自己是个新人 FAIL：当节点下线，会向集群中广播该消息。 上线和下线当哨兵与master相同之后就会定期一直保持联系，若是某一时刻哨兵发送的PING在指定时间内没有收到回复（sentinel down-after-milliseconds master-name milliseconds 配置），那么发送PING命令的哨兵就会认为该master主观下线（Subjectively Down）。 因为有可能是哨兵与该master之间的网络问题造成的，而不是master本身的原因，所以哨兵同时会询问其它的哨兵是否也认为该master下线，若是认为该节点下线的哨兵达到一定的数量（前面的quorum字段配置），就会认为该节点客观下线（Objectively Down）。 若是没有足够数量的sentinel同意该master下线，则该master客观下线的标识会被移除；若是master重新向哨兵的PING命令回复了客观下线的标识也会被移除。 选举算法当master被认为客观下线后，又是怎么进行故障恢复的呢？原来哨兵中首先选举出一个老大哨兵来进行故障恢复，选举老大哨兵的算法叫做Raft算法： 发现master下线的哨兵（sentinelA）会向其它的哨兵发送命令进行拉票，要求选择自己为哨兵大佬。 若是目标哨兵没有选择其它的哨兵，就会选择该哨兵（sentinelA）为大佬。 若是选择sentinelA的哨兵超过半数（半数原则），该大佬非sentinelA莫属。 如果有多个哨兵同时竞选，并且可能存在票数一致的情况，就会等待下次的一个随机时间再次发起竞选请求，进行新的一轮投票，直到大佬被选出来。 选出大佬哨兵后，大佬哨兵就会对故障进行自动回复，从slave中选出一名slave作为主数据库，选举的规则如下所示： 所有的slave中slave-priority优先级最高的会被选中。 若是优先级相同，会选择偏移量最大的，因为偏移量记录着数据的复制的增量，越大表示数据越完整。 若是以上两者都相同，选择ID最小的。 通过以上的层层筛选最终实现故障恢复，当选的slave晋升为master，其它的slave会向新的master复制数据，若是down掉的master重新上线，会被当作slave角色运行。 优点哨兵模式是主从模式的升级版，所以在系统层面提高了系统的可用性和性能、稳定性。当master宕机的时候，能够自动进行故障恢复，需不要人为的干预。 哨兵于哨兵之间、哨兵与master之间能够进行及时的监控，心跳检测，及时发现系统的问题，这都是弥补了主从的缺点。 缺点哨兵一主多从的模式同样也会遇到写的瓶颈，已经存储瓶颈，若是master宕机了，故障恢复的时间比较长，写的业务就会受到影响。 增加了哨兵也增加了系统的复杂度，需要同时维护哨兵模式。 实操搭建最后，我们进行一下哨兵模式的搭建，配置哨兵模式还是比较简单的，在上面配置的主从模式的基础上，同时创建一个文件夹用于存放三个哨兵的配置文件： 123mkdir /root/redis-4.0.6/sentinel.conf /root/redis/sentinel/sentinel1.conf mkdir /root/redis-4.0.6/sentinel.conf /root/redis/sentinel/sentinel2.conf mkdir /root/redis-4.0.6/sentinel.conf /root/redis/sentinel/sentinel3.conf 分别在这三个文件中添加如下配置： 1234567daemonize yes # 在后台运行sentinel monitor mymaster 127.0.0.1 6379 1 # 给master起一个名字mymaster，并且配置master的ip和端口sentinel auth-pass mymaster 123456 # master的密码port 26379 #另外两个配置36379,46379端口sentinel down-after-milliseconds mymaster 3000 # 3s未回复PING就认为master主观下线sentinel parallel-syncs mymaster 2 # 执行故障转移时，最多可以有2个slave实例在同步新的master实例sentinel failover-timeout mymaster 100000 # 如果在10s内未能完成故障转移操作认为故障转移失败 配置完后分别启动三台哨兵： 123./redis-server sentinel1.conf --sentinel./redis-server sentinel2.conf --sentinel./redis-server sentinel3.conf --sentinel 然后通过：ps -aux|grep redis进行查看：可以看到三台redis实例以及三个哨兵都已经正常启动，现登陆6379，通过INFO Repliaction查看master信息： 当前master为6379，然后我们来测试一下哨兵的自动故障恢复，直接kill掉6379进程，然后通过登陆6380再次查看master的信息： 可以看到当前的6380角色是master，并且6380可读可写，而不是只读模式，这说明我们的哨兵是起作用了，搭建成功，感兴趣的可以自行搭建，也有可能你会踩一堆的坑。 Cluster模式最后，Cluster是真正的集群模式了，哨兵解决和主从不能自动故障恢复的问题，但是同时也存在难以扩容以及单机存储、读写能力受限的问题，并且集群之前都是一台redis都是全量的数据，这样所有的redis都冗余一份，就会大大消耗内存空间。 集群模式实现了Redis数据的分布式存储，实现数据的分片，每个redis节点存储不同的内容，并且解决了在线的节点收缩（下线）和扩容（上线）问题。 集群模式真正意义上实现了系统的高可用和高性能，但是集群同时进一步使系统变得越来越复杂，接下来我们来详细的了解集群的运作原理。 数据分区原理集群的原理图还是很好理解的，在Redis集群中采用的使虚拟槽分区算法，会把redis集群分成16384 个槽（0 -16383）。 比如：下图所示三个master，会把0 -16383范围的槽可能分成三部分（0-5000）、（5001-11000）、（11001-16383）分别数据三个缓存节点的槽范围。 当客户端请求过来，会首先通过对key进行CRC16 校验并对 16384 取模（CRC16(key)%16383）计算出key所在的槽，然后再到对应的槽上进行取数据或者存数据，这样就实现了数据的访问更新。 之所以进行分槽存储，是将一整堆的数据进行分片，防止单台的redis数据量过大，影响性能的问题。 节点通信节点之间实现了将数据进行分片存储，那么节点之间又是怎么通信的呢？这个和前面哨兵模式讲的命令基本一样。 首先新上线的节点，会通过 Gossip 协议向老成员发送Meet消息，表示自己是新加入的成员。 老成员收到Meet消息后，在没有故障的情况下会恢复PONG消息，表示欢迎新结点的加入，除了第一次发送Meet消息后，之后都会发送定期PING消息，实现节点之间的通信。 通信的过程中会为每一个通信的节点开通一条tcp通道，之后就是定时任务，不断的向其它节点发送PING消息，这样做的目的就是为了了解节点之间的元数据存储情况，以及健康状况，以便即使发现问题。 数据请求上面说到了槽信息，在Redis的底层维护了unsigned char myslots[CLUSTER_SLOTS/8] 一个数组存放每个节点的槽信息。 因为他是一个二进制数组，只有存储0和1值，如下图所示： 这样数组只表示自己是否存储对应的槽数据，若是1表示存在该数据，0表示不存在该数据，这样查询的效率就会非常的高，类似于布隆过滤器，二进制存储。 比如：集群节点1负责存储0-5000的槽数据，但是此时只有0、1、2存储有数据，其它的槽还没有存数据，所以0、1、2对应的值为1。 并且，每个redis底层还维护了一个clusterNode数组，大小也是16384，用于储存负责对应槽的节点的ip、端口等信息，这样每一个节点就维护了其它节点的元数据信息，便于及时的找到对应的节点。 当新结点加入或者节点收缩，通过PING命令通信，及时的更新自己clusterNode数组中的元数据信息，这样有请求过来也就能及时的找到对应的节点。 有两种其它的情况就是，若是请求过来发现，数据发生了迁移，比如新节点加入，会使旧的缓存节点数据迁移到新结点。 请求过来发现旧节点已经发生了数据迁移并且数据被迁移到新结点，由于每个节点都有clusterNode信息，通过该信息的ip和端口。此时旧节点就会向客户端发一个MOVED 的重定向请求，表示数据已经迁移到新结点上，你要访问这个新结点的ip和端口就能拿到数据，这样就能重新获取到数据。 倘若正在发正数据迁移呢？旧节点就会向客户端发送一个ASK 重定向请求，并返回给客户端迁移的目标节点的ip和端口，这样也能获取到数据。 扩容和收缩扩容和收缩也就是节点的上线和下线，可能节点发生故障了，故障自动回复的过程（节点收缩）。 节点的收缩和扩容时，会重新计算每一个节点负责的槽范围，并发根据虚拟槽算法，将对应的数据更新到对应的节点。 还有前面的讲的新加入的节点会首先发送Meet消息，详细可以查看前面讲的内容，基本一样的模式。 以及发生故障后，哨兵老大节点的选举，master节点的重新选举，slave怎样晋升为master节点，可以查看前面哨兵模式选举过程。 优点集群模式时一个无中心的架构模式，将数据进行分片，分不到对应的槽中，每个节点存储不同的数据内容，通过路由能够找到对应的节点负责存储的槽，能够实现高效率的查询。 并且集群模式增加了横向和纵向的扩展能力，实现节点加入和收缩，集群模式时哨兵的升级版，哨兵的优点集群都有。 缺点缓存的最大问题就是带来数据一致性问题，在平衡数据一致性的问题时，兼顾性能与业务要求，大多数都是以最终一致性的方案进行解决，而不是强一致性。 并且集群模式带来节点数量的剧增，一个集群模式最少要6台机，因为要满足半数原则的选举方式，所以也带来了架构的复杂性。 slave只充当冷备，并不能缓解master的读的压力。 实操搭建集群模式的部署比较简单，只要在redis.conf加入下面的配置信息即可： 1234567port 6379# 本示例6个节点端口分别为6379、6380、6381、6382、6383、6384daemonize yes # r后台运行 pidfile /var/run/redis_6379.pid # 分别对应6379、6380、6381、6382、6383、6384cluster-enabled yes # 开启集群模式 masterauth 123456# 如果设置了密码，需要指定master密码cluster-config-file nodes_6379.conf # 集群的配置文件，同样对应6379、6380、6381、6382、6383、6384六个节点cluster-node-timeout 10000 # 请求超时时间 同时开启这六个实例，通过下面的命令将这六个实例以集群的方式运行 1./redis-cli --cluster create --cluster-replicas 1 127.0.0.1:6379 127.0.0.1:6380 127.0.0.1:6381 127.0.0.1:6382 127.0.0.1:6383 127.0.0.1:6384 -a 123456 这样就实现了集群的搭建，好了这一期就完成了，看了一下字数一共1.7W字，原创不易，看完点个在看和分享，不要白嫖我，传承中华民族的良好美德。 应用1.订阅与发布简介Redis发布与发布功能（Pub/Sub）是基于事件座位基本的通信机制，是目前应用比较普遍的通信模型，它的目的主要是解除消息的发布者与订阅者之间的耦合关系。 Redis作为消息发布和订阅之间的服务器，起到桥梁的作用，在Redis里面有一个channel的概念，也就是频道，发布者通过指定发布到某个频道，然后只要有订阅者订阅了该频道，该消息就会发送给订阅者，原理图如下所示：Redis同时也可以使用list类型实现消息队列（消息队列的实现以及应用场景会在下一篇文章继续讲解）。 Redis的发布与订阅的功能应用还是比较广泛的，它的应用场景有很多。比如：最常见的就是实现实时聊天的功能，还是有就是博客的粉丝文章的推送，当博主推送原创文章的时候，就会将文章实时推送给博主的粉丝。 简介完Redis的发布于订阅功能，下面就要来实操一下，包括linux命令的实操和java代码的实现。 命令实操这里就假设各位读者都已经安装好自己的虚拟机环境和Redis了，若是没有安装好的，可以参考这一篇博文：https://www.cnblogs.com/zuidongfeng/p/8032505.html 我这里是已经安装好了Redis了，直接启动我们的Redis，我已经设置好了开机启动，上面的那篇博文有讲解怎么设置开机启动。 发布消息Redis中发布消息的命令是publish，具体使用如下所示： PUBLISH test “haha”：test表示频道的名称，haha表示发布的内容，这样就完成了一个一个消息的发布，后面的返回（integer）0表示0人订阅。 订阅频道于此同时再启动一个窗口，这个窗口作为订阅者，订阅者的命令subscribe，使用SUBSCRIBE test就表示订阅了test这个频道订阅后返回的结果中由三条信息，第一个表示类型、第二个表示订阅的频道，第三个表示订阅的数量。接着在第一个窗口进行发布消息： 可以看到发布者发布的消息，订阅者都会实时的接收到，并发订阅者收到的信息中也会出现三条信息，分别表示：返回值的类型、频道名称、消息内容。 取消订阅若是想取消之前的订阅可以使用unsubscribe命令，格式为： 123unsubscribe 频道名称// 取消之前订阅的test频道unsubscribe test 输入命令后，返回以下结果： 12345[root@pinyoyougou-docker src]# ./redis-cli 127.0.0.1:6379&gt; UNSUBSCRIBE test1) &quot;unsubscribe&quot;2) &quot;test&quot;3) (integer) 0 它分别表示：返回值的类型、频道的名称、该频道订阅的数量。 按模式订阅除了直接以特定的名城进行订阅，还可以按照模式进行订阅，模式的方式进行订阅可以一次订阅多个频道，按照模式进行订阅的命令为psubscribe，具体格式如下： 123psubscribe 模式// 表示订阅名称以ldc开头的频道psubscribe ldc* 输入上面的命令后，返回如下结果： 12345127.0.0.1:6379&gt; PSUBSCRIBE ldc*Reading messages... (press Ctrl-C to quit)1) &quot;psubscribe&quot;2) &quot;ldc*&quot;3) (integer) 1 这个也是非常简单，分别表示：返回的类型（表示按模式订阅类型）、订阅的模式、订阅数。 取消按模式订阅假如你想取消之前的按模式订阅，可以使用punsubscribe来取消，具体格式： 123punsubscribe 模式// 取消频道名称按照ldc开头的频道punsubscribe ldc* 他的返回值，如下所示： 1234127.0.0.1:6379&gt; PUNSUBSCRIBE ldc*1) &quot;punsubscribe&quot;2) &quot;ldc*&quot;3) (integer) 0 这个就不多说了，表示的意思和上面的一样，可以看到上面的命令都是有规律的订阅SUBSCRIBE，取消就是UNSUBSCRIBE，前面加前缀UN，按模式订阅也是。 查看订阅消息（1）你想查看某一个模式下订阅数是大于零的频道，可以使用如下格式的命令进行操作： 123pubsub channels 模式// 查看频道名称以ldc模式开头的订阅数大于零的频道pubsub channels ldc* （2）假如你想查看某一个频道的订阅数，可以使用如下命令： 1pubsub numsub 频道名称 （3）查看按照模式的订阅数，可以使用如下命令进行操作： 1pubsub numpat 到这里以上的命令操作就基本结束了，下面就来代码实战。 代码实练（1）首先第一步想要操作Redis，再SpringBoot项目中引入jedis的依赖，毕竟jedis是官方推荐使用操作Redis的工具。 12345&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt; （2）然后创建发布者Publisher，用于消息的发布，具体代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.ldc.org.myproject.demo.redis;import java.io.BufferedReader;import java.io.IOException;import java.io.InputStreamReader;import redis.clients.jedis.Jedis;import redis.clients.jedis.JedisPool;/** * 发布者 * @author liduchang * */public class Publisher extends Thread{ // 连接池 private final JedisPool jedisPool; // 发布频道名称 private String name; public Publisher(JedisPool jedisPool, String name) { super(); this.jedisPool = jedisPool; this.name = name; } @Override public void run() { // 获取要发布的消息 BufferedReader reader = new BufferedReader(new InputStreamReader(System.in)); // 获取连接 Jedis resource = jedisPool.getResource(); while (true) { String message = null; try { message = reader.readLine(); if (!&quot;exit&quot;.equals(message)) { // 发布消息 resource.publish(name, &quot;发布者:&quot;+Thread.currentThread().getName()+&quot;发布消息：&quot;+message); } else { break; } } catch (IOException e) { e.printStackTrace(); } } }} （3）接着创建订阅类Subscriber，并且继承JedisPubSub 类，重写onMessage、onSubscribe、onUnsubscribe三个方法，这三个方法的调用时机在注释上都有说明，具体的实现代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243package com.ldc.org.myproject.demo.redis;import com.fasterxml.jackson.core.sym.Name;import redis.clients.jedis.JedisPubSub;/** * 订阅者 * @author liduchang */public class Subscriber extends JedisPubSub { //订阅频道名称 private String name; public Subscriber(String name) { this.name = name; } /** * 订阅者收到消息时会调用 */ @Override public void onMessage(String channel, String message) { // TODO Auto-generated method stub super.onMessage(channel, message); System.out.println(&quot;频道：&quot;+channel+&quot; 接受的消息为：&quot;+message); } /** * 订阅了频道会被调用 */ @Override public void onSubscribe(String channel, int subscribedChannels) { System.out.println(&quot;订阅了频道:&quot;+channel+&quot; 订阅数为：&quot;+subscribedChannels); } /** * 取消订阅频道会被调用 */ @Override public void onUnsubscribe(String channel, int subscribedChannels) { System.out.println(&quot;取消订阅的频道：&quot;+channel+&quot; 订阅的频道数量为：&quot;+subscribedChannels); }} （4）这次创建的才是真正的订阅者SubThread，上面的Subscriber是指为了测试实订阅的时候或者发布消息，能够有信息输出： 1234567891011121314151617181920212223242526272829303132333435363738394041424344package com.ldc.org.myproject.demo.redis;import redis.clients.jedis.Jedis;import redis.clients.jedis.JedisPool;/** * 订阅者线程 * @author liduchang * */public class SubThread extends Thread { private final JedisPool jedisPool; private final Subscriber subscriber; private String name; public SubThread(JedisPool jedisPool,Subscriber subscriber,String name) { super(); this.jedisPool = jedisPool; this.subscriber = subscriber; this.name = name; } @Override public void run() { Jedis jedis = null; try { jedis = jedisPool.getResource(); // 订阅频道为name jedis.subscribe(subscriber, name); } catch (Exception e) { System.err.println(&quot;订阅失败&quot;); e.printStackTrace(); } finally { if (jedis!=null) { // jedis.close(); //归还连接到redis池中 jedisPool.returnResource(jedis); } } }} （5）后面就是测试了，分别测试发布与订阅的测试，发布者为TestPublisher，订阅者为TestSubscriber： 12345678910111213141516package com.ldc.org.myproject.demo.redis;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.TimeUnit;import redis.clients.jedis.JedisPool;public class TestPublisher { public static void main(String[] args) throws InterruptedException { JedisPool jedisPool = new JedisPool(&quot;192.168.163.155&quot;); // 向ldc频道发布消息 Publisher publisher = new Publisher(jedisPool, &quot;ldc&quot;); publisher.start(); }} 订阅者 123456789101112131415161718192021package com.ldc.org.myproject.demo.redis;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.TimeUnit;import redis.clients.jedis.JedisPool;public class TestSubscriber1 { public static void main(String[] args) throws InterruptedException { JedisPool jedisPool = new JedisPool(&quot;192.168.163.155&quot;,6379); Subscriber subscriber = new Subscriber(&quot;黎杜&quot;); // 订阅ldc频道 SubThread thread= new SubThread(jedisPool, subscriber, &quot;ldc&quot;); thread.start(); Thread.sleep(600000); // 取消订阅 subscriber.unsubscribe(&quot;ldc&quot;); }} 这里为了测试方便就直接创建线程的方式，更好的话可以使用线程池的方式通过线程池的submit方法来执行线程，若是不用了可以使用shutdown方式关闭。 好了这一期的Redis的实现订阅与发布的讲解就说完了，我们下一期在讲解Redis的集群的知识，下期再见。 2.Redis实现的分布式锁之前讲了一片Redis事务的文章，很多读者Redis事务有啥用，主要是因为Redis的事务并没有Mysql的事务那么强大，所以一般的公司一般确实是用不到。 这里就来说一说Redis事务的一个实际用途，它可以用来实现一个简单的秒杀系统的库存扣减，下面我们就来进行代码的实现。 （1）首先使用线程池初始化5000个客户端。 12345678910111213public static void intitClients() { ExecutorService threadPool= Executors.newCachedThreadPool(); for (int i = 0; i &lt; 5000; i++) { threadPool.execute(new Client(i)); } threadPool.shutdown(); while(true){ if(threadPool.isTerminated()){ break; } } } （2）接着初始化商品的库存数为1000。 123456public static void initPrductNum() { Jedis jedis = RedisUtil.getInstance().getJedis(); jedisUtils.set(&quot;produce&quot;, &quot;1000&quot;);// 初始化商品库存数 RedisUtil.returnResource(jedis);// 返还数据库连接 }} （3）最后是库存扣减的每条线程的处理逻辑。 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * 顾客线程 * * @author linbingwen * */class client implements Runnable { Jedis jedis = null; String key = &quot;produce&quot;; // 商品数量的主键 String name; public ClientThread(int num) { name= &quot;编号=&quot; + num; } public void run() { while (true) { jedis = RedisUtil.getInstance().getJedis(); try { jedis.watch(key); int num= Integer.parseInt(jedis.get(key));// 当前商品个数 if (num&gt; 0) { Transaction ts= jedis.multi(); // 开始事务 ts.set(key, String.valueOf(num - 1)); // 库存扣减 List&lt;Object&gt; result = ts.exec(); // 执行事务 if (result == null || result.isEmpty()) { System.out.println(&quot;抱歉，您抢购失败，请再次重试&quot;); } else { System.out.println(&quot;恭喜您，抢购成功&quot;); break; } } else { System.out.println(&quot;抱歉，商品已经卖完&quot;); break; } } catch (Exception e) { e.printStackTrace(); } finally { jedis.unwatch(); // 解除被监视的key RedisUtil.returnResource(jedis); } } }} 在代码的实现中有一个重要的点就是商品的数据量被watch了，当前的客户端只要发现数量被改变就会抢购失败，然后不断的自旋进行抢购。 这个是基于Redis事务实现的简单的秒杀系统，Redis事务中的watch命令有点类似乐观锁的机制，只要发现商品数量被修改，就执行失败。 Redis实现分布式锁的第二种方式，可以使用setnx、getset、expire、del这四个命令来实现。 setnx：命令表示如果key不存在，就会执行set命令，若是key已经存在，不会执行任何操作。 getset：将key设置为给定的value值，并返回原来的旧value值，若是key不存在就会返回返回nil 。 expire：设置key生存时间，当当前时间超出了给定的时间，就会自动删除key。 del：删除key，它可以删除多个key，语法如下：DEL key [key …]，若是key不存在直接忽略。 下面通过一个代码案例是实现以下这个命令的操作方式： 12345678910111213public void redis(Produce produce) { long timeout= 10000L; // 超时时间 Long result= RedisUtil.setnx(produce.getId(), String.valueOf(System.currentTimeMillis() + timeout)); if (result!= null &amp;&amp; result.intValue() == 1) { // 返回1表示成功获取到锁 RedisUtil.expire(produce.getId(), 10);//有效期为5秒，防止死锁 //执行业务操作 ...... //执行完业务后，释放锁 RedisUtil.del(produce.getId()); } else { System.println.out(&quot;没有获取到锁&quot;) } } 在线程A通过setnx方法尝试去获取到produce对象的锁，若是获取成功旧会返回1，获取不成功，说明当前对象的锁已经被其它线程锁持有。 获取锁成功后并设置key的生存时间，能够有效的防止出现死锁，最后就是通过del来实现删除key，这样其它的线程就也可以获取到这个对象的锁。 执行的逻辑很简单，但是简单的同时也会出现问题，比如你在执行完setnx成功后设置生存时间不生效，此时服务器宕机，那么key就会一直存在Redis中。 当然解决的办法，你可以在服务器destroy函数里面再次执行： 1RedisUtil.del(produce.getId()); 或者通过定时任务检查是否有设置生存时间，没有的话都会统一进行设置生存时间。 还有比较好的解决方案就是，在上面的执行逻辑里面，若是没有获取到锁再次进行key的生存时间： 12345678910111213141516171819202122232425262728public void redis(Produce produce) { long timeout= 10000L; // 超时时间 Long result= RedisUtil.setnx(produce.getId(), String.valueOf(System.currentTimeMillis() + timeout)); if (result!= null &amp;&amp; result.intValue() == 1) { // 返回1表示成功获取到锁 RedisUtil.expire(produce.getId(), 10);//有效期为10秒，防止死锁 //执行业务操作 ...... //执行完业务后，释放锁 RedisUtil.del(produce.getId()); } else { String value= RedisUtil.get(produce.getId()); // 存在该key，并且已经超时 if (value!= null &amp;&amp; System.currentTimeMillis() &gt; Long.parseLong(value)) { String result = RedisUtil.getSet(produce.getId(), String.valueOf(System.currentTimeMillis() + timeout)); if (result == null || (result != null &amp;&amp; StringUtils.equals(value, result))) { RedisUtil.expire(produce.getId(), 10);//有效期为10秒，防止死锁 //执行业务操作 ...... //执行完业务后，释放锁 RedisUtil.del(produce.getId()); } else { System.println(&quot;没有获取到锁&quot;) } } else { System.println(&quot;没有获取到锁&quot;) } } } 这里对上面的代码进行了改进，在获取setnx失败的时候，再次重新判断该key的锁时间是否失效或者不存在，并重新设置生存的时间，避免出现死锁的情况。 第三种Redis实现分布式锁，可以使用Redisson来实现，它的实现简单，已经帮我们封装好了，屏蔽了底层复杂的实现逻辑。 先来一个Redisson的原理图，后面回对这个原理图进行详细的介绍： 我们在实际的项目中要使用它，只需要引入它的依赖，然后执行下面的代码： 123RLock lock = redisson.getLock(&quot;lockName&quot;);lock.locl();lock.unlock(); 并且它还支持Redis单实例、Redis哨兵、redis cluster、redis master-slave等各种部署架构，都给你完美的实现，不用自己再次拧螺丝。 但是，crud的同时还是要学习一下它的底层的实现原理，下面我们来了解下一下，对于一个分布式的锁的框架主要的学习分为下面的5个点： 加锁机制 解锁机制 生存时间延长机制 可重入加锁机制 锁释放机制 只要掌握一个框架的这五个大点，基本这个框架的核心思想就已经掌握了，若是要你去实现一个锁机制框架，就会有大体的一个思路。 Redisson中的加锁机制是通过lua脚本进行实现，Redisson首先会通过hash算法，选择redis cluster集群中的一个节点，接着会把一个lua脚本发送到Redis中。 它底层实现的lua脚本如下： 12345678910111213returncommandExecutor.evalWriteAsync(getName(), LongCodec.INSTANCE, command, &quot;if (redis.call('exists', KEYS[1]) == 0) then &quot; + &quot;redis.call('hset', KEYS[1], ARGV[2], 1); &quot; + &quot;redis.call('pexpire', KEYS[1], ARGV[1]); &quot; + &quot;return nil; &quot; + &quot;end; &quot; + &quot;if (redis.call('hexists', KEYS[1], ARGV[2]) == 1) then &quot; + &quot;redis.call('hincrby', KEYS[1], ARGV[2], 1); &quot; + &quot;redis.call('pexpire', KEYS[1], ARGV[1]); &quot; + &quot;return nil; &quot; + &quot;end; &quot; + &quot;return redis.call('pttl', KEYS[1]);&quot;, Collections.&lt;Object&gt;singletonList(getName()), internalLockLeaseTime, getLockName(threadId)); redis.call()的第一个参数表示要执行的命令，KEYS[1]表示要加锁的key值，ARGV[1]表示key的生存时间，默认时30秒，ARGV[2]表示加锁的客户端的ID。 比如第一行中redis.call('exists', KEYS[1]) == 0) 表示执行exists命令判断Redis中是否含有KEYS[1]，这个还是比较好理解的。 lua脚本中封装了要执行的业务逻辑代码，它能够保证执行业务代码的原子性，它通过hset lockName命令完成加锁。 若是第一个客户端已经通过hset命令成功加锁，当第二个客户端继续执行lua脚本时，会发现锁已经被占用，就会通过pttl myLock返回第一个客户端的持锁生存时间。 若是还有生存时间，表示第一个客户端会继续持有锁，那么第二个客户端就会不停的自旋尝试取获取锁。 假如第一个客户端持有锁的时间快到期了，想继续持有锁，可以给它启动一个watch dog看门狗，他是一个后台线程会每隔10秒检查一次，可以不断的延长持有锁的时间。 Redisson中可重入锁的实现是通过incrby lockName来实现，重入一个计数就会+1，释放一次锁计数就会-1。 最后，使用完锁后执行del lockName就可以直接释放锁，这样其它的客户端就可以争抢到该锁了。 这就是分布式锁的开源Redisson框架底层锁机制的实现原理，我们可以在生产中实现该框架实现分布式锁的高效使用。 下面通过一个多窗口抢票的例子代码来实现： 123456789101112131415161718192021222324252627public class SellTicket implements Runnable { private int ticketNum = 1000; RLock lock = getLock(); // 获取锁 private RLock getLock() { Config config = new Config(); config.useSingleServer().setAddress(&quot;redis://localhost:6379&quot;); Redisson redisson = (Redisson) Redisson.create(config); RLock lock = redisson.getLock(&quot;keyName&quot;); return lock; } @Override public void run() { while (ticketNum&gt;0) { // 获取锁,并设置超时时间 lock.lock(1, TimeUnit.MINUTES); try { if (ticketNum&gt; 0) { System.out.println(Thread.currentThread().getName() + &quot;出售第 &quot; + ticketNum-- + &quot; 张票&quot;); } } finally { lock.unlock(); // 释放锁 } } }} 测试的代码如下： 123456789public class Test { public static void main(String[] args) { SellTicket sellTick= new SellTicket(); // 开启5五条线程，模拟5个窗口 for (int i=1; i&lt;=5; i++) { new Thread(sellTick, &quot;窗口&quot; + i).start(); } }} 是不是感觉很简单，因为多线程竞争共享资源的复杂的过程它在底层都帮你实现了，屏蔽了这些复杂的过程，而你也就成为了优秀的API调用者。 上面就是Redis三种方式实现分布式锁的方式，基于Redis的实现方式基本都会选择Redisson的方式进行实现，因为简单命令，不用自己拧螺丝，开箱即用。","link":"/2020/10/14/%E5%AD%A6%E4%B9%A0Redis%E4%B8%80%E7%AF%87%E5%B0%B1%E5%A4%9F/"}],"tags":[],"categories":[]}